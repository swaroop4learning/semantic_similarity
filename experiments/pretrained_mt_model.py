# -*- coding: utf-8 -*-
"""pretrained-mt-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b-0QhJXemZ4_ooMVDAIqp4U2Yi8t9VVv
"""

pip install transformers

pip install datasets

# Commented out IPython magic to ensure Python compatibility.
#imprt libraries
from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import math
import re
import random
import string
import time
import pandas as pd
import numpy as np
import string
from string import digits
import matplotlib.pyplot as plt
# %matplotlib inline
import re
import random
from datasets import load_dataset
from scipy.stats import pearsonr
from sklearn.preprocessing import MinMaxScaler
from scipy.stats import pearsonr
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import pearsonr

train_es = load_dataset("stsb_multi_mt", name="es", split="train")
val_es = load_dataset("stsb_multi_mt", name="es", split="dev")
test_es = load_dataset("stsb_multi_mt", name="es", split="test")

train_en = load_dataset("stsb_multi_mt", name="en", split="train")
val_en = load_dataset("stsb_multi_mt", name="en", split="dev")
test_en = load_dataset("stsb_multi_mt", name="en", split="test")

trainEN_sen1 = []
trainES_sen2 = []
testEN_sen1 = []
testES_sen2 = []
valEN_sen1 = []
valES_sen2 = []
train_score = []
test_score = []
val_score = []

# train
for length in range (len(train_es)):
  trainEN_sen1.append(train_es[length]["sentence1"].lower())
  train_score.append(train_es[length]["similarity_score"])


for length in range (len(train_en)):
  trainES_sen2.append(train_en[length]["sentence2"].lower())

# val
for length in range (len(val_es)):
  valEN_sen1.append(val_es[length]["sentence1"].lower())
  val_score.append(val_es[length]["similarity_score"])


for length in range (len(val_en)):
  valES_sen2.append(val_en[length]["sentence2"].lower())

# test
for length in range (len(test_es)):
  testEN_sen1.append(test_es[length]["sentence1"].lower())
  test_score.append(test_es[length]["similarity_score"])


for length in range (len(test_en)):
  testES_sen2.append(test_en[length]["sentence2"].lower())

from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM
import numpy as np
from googletrans import Translator
import gensim.downloader as api

# Define the model repo
model_name = "Helsinki-NLP/opus-mt-en-es" 


# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Define the input sentence in Spanish
source_text = "hi how are you?"

# Tokenize the input sentence
inputs = tokenizer(source_text, return_tensors="pt")

# Generate translation from the input sentence
outputs = model.generate(**inputs)

# Decode the generated translation into text
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Print the generated translation
print(decoded_output)

print(len(testES_sen2))

outs = []
i=0
for sent in testES_sen2[:100]:
    # Tokenize the input sentence
    inputs = tokenizer(source_text, return_tensors="pt")

    # Generate translation from the input sentence
    outputs = model.generate(**inputs)

    # Decode the generated translation into text
    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    outs.append(decoded_output)
    i +=1
    if i%5 == 0:
      print(i)

print(len(testES_sen2[:100]))
print(len(outs))
print(len(testEN_sen1[:100]))

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/NLP/PreTrained_models

from gensim.models import Word2Vec
from gensim.models import KeyedVectors

emb_model = KeyedVectors.load_word2vec_format('SBW-vectors-300-min5.bin', binary=True)

sentence_embeddings1 = []

for sentence in valEN_sen1[:100]:
    sentence_embedding = []
    for word in sentence:
        if word in emb_model:
            sentence_embedding.append(emb_model[word])
    if sentence_embedding:
        sentence_embedding = np.mean(sentence_embedding, axis=0)
    else:
        sentence_embedding = np.zeros(emb_model.vector_size)
    sentence_embeddings1.append(sentence_embedding)
# print(sentence_embeddings[0])
print((sentence_embeddings1[0].shape))

sentence_embeddings2 = []

for sentence in outs:
    sentence_embedding = []
    for word in sentence:
        if word in emb_model:
            sentence_embedding.append(emb_model[word])
    if sentence_embedding:
        sentence_embedding = np.mean(sentence_embedding, axis=0)
    else:
        sentence_embedding = np.zeros(emb_model.vector_size)
    sentence_embeddings2.append(sentence_embedding)
# print(sentence_embeddings[0])
print((sentence_embeddings2[0].shape))

from sklearn.metrics.pairwise import cosine_similarity
# sentence_embeddings1 and sentence_embeddings2 are the two lists of sentence embeddings
# of the same length
assert len(sentence_embeddings1) == len(sentence_embeddings2)

# Compute the cosine similarity between corresponding indexed sentences
cos_sim = []
for emb1, emb2 in zip(sentence_embeddings1, sentence_embeddings2):
    similarity = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]
    cos_sim.append(similarity)

sim_scores = test_score[:100]
# Normalize sim_scores using Min-Max scaling
scaler = MinMaxScaler(feature_range=(0, 1))
sim_scores_norm = scaler.fit_transform(np.array(sim_scores).reshape(-1, 1)).flatten()


# cos_sim and similarity_score are the two lists of similarity scores
# of the same length
assert len(cos_sim) == len(sim_scores)

# Calculate the Pearson correlation coefficient and its p-value
corr, p_value = pearsonr(cos_sim, sim_scores_norm)

print(f"Pearson correlation coefficient: {corr:.3f}")
print(f"P-value: {p_value:.3f}")