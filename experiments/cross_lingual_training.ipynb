{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc9c968",
   "metadata": {},
   "source": [
    "# Cross Lingual Siamese BiLSTM Neural Network with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47bf9f",
   "metadata": {},
   "source": [
    "<p>In this following approaches are performed:\n",
    "    <li> The preprocessing and tokenization is performed specific to the language English and Spanish </li>\n",
    "    <li> Word2vec embeddings with respect to each language is generated </li>\n",
    "    <li> Word2vec STS models - Mean approach is performed. But results are not promising </li>\n",
    "    <li> Siamese BiLSTM neural network with attention is applied by initializing with two bilstms one with english word2vec embeddings and other with spanish word2vec embeddings </li>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741224c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7073ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c25b6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_df = pd.read_csv('../data/stsb-en-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe810374",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_en_df = pd.read_csv('../data/stsb-en-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f1ac216",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_df = pd.read_csv('../data/stsb-en-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a2521f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sent1  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "3                   Three men are playing chess.   \n",
       "4                    A man is playing the cello.   \n",
       "\n",
       "                                               sent2  score  \n",
       "0                        An air plane is taking off.   5.00  \n",
       "1                          A man is playing a flute.   3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...   3.80  \n",
       "3                         Two men are playing chess.   2.60  \n",
       "4                 A man seated is playing the cello.   4.25  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c6476a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_es_df = pd.read_csv('../data/stsb-es-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "453394c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_es_df = pd.read_csv('../data/stsb-es-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2ab1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_es_df = pd.read_csv('../data/stsb-es-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c89bfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un avión está despegando.</td>\n",
       "      <td>Un avión está despegando.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un hombre está tocando una gran flauta.</td>\n",
       "      <td>Un hombre está tocando una flauta.</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Un hombre está untando queso rallado en una pi...</td>\n",
       "      <td>Un hombre está untando queso rallado en una pi...</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tres hombres están jugando al ajedrez.</td>\n",
       "      <td>Dos hombres están jugando al ajedrez.</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un hombre está tocando el violonchelo.</td>\n",
       "      <td>Un hombre sentado está tocando el violonchelo.</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent1  \\\n",
       "0                          Un avión está despegando.   \n",
       "1            Un hombre está tocando una gran flauta.   \n",
       "2  Un hombre está untando queso rallado en una pi...   \n",
       "3             Tres hombres están jugando al ajedrez.   \n",
       "4             Un hombre está tocando el violonchelo.   \n",
       "\n",
       "                                               sent2  score  \n",
       "0                          Un avión está despegando.   5.00  \n",
       "1                 Un hombre está tocando una flauta.   3.80  \n",
       "2  Un hombre está untando queso rallado en una pi...   3.80  \n",
       "3              Dos hombres están jugando al ajedrez.   2.60  \n",
       "4     Un hombre sentado está tocando el violonchelo.   4.25  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_es_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4d013d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_es_df['sent1_trans'] = train_en_df['sent1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59dd0874",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_es_df['sent1_trans'] = val_en_df['sent1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe998941",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_es_df['sent1_trans'] = test_en_df['sent1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e260caab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>score</th>\n",
       "      <th>sent1_trans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un avión está despegando.</td>\n",
       "      <td>Un avión está despegando.</td>\n",
       "      <td>5.00</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un hombre está tocando una gran flauta.</td>\n",
       "      <td>Un hombre está tocando una flauta.</td>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Un hombre está untando queso rallado en una pi...</td>\n",
       "      <td>Un hombre está untando queso rallado en una pi...</td>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tres hombres están jugando al ajedrez.</td>\n",
       "      <td>Dos hombres están jugando al ajedrez.</td>\n",
       "      <td>2.60</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un hombre está tocando el violonchelo.</td>\n",
       "      <td>Un hombre sentado está tocando el violonchelo.</td>\n",
       "      <td>4.25</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent1  \\\n",
       "0                          Un avión está despegando.   \n",
       "1            Un hombre está tocando una gran flauta.   \n",
       "2  Un hombre está untando queso rallado en una pi...   \n",
       "3             Tres hombres están jugando al ajedrez.   \n",
       "4             Un hombre está tocando el violonchelo.   \n",
       "\n",
       "                                               sent2  score  \\\n",
       "0                          Un avión está despegando.   5.00   \n",
       "1                 Un hombre está tocando una flauta.   3.80   \n",
       "2  Un hombre está untando queso rallado en una pi...   3.80   \n",
       "3              Dos hombres están jugando al ajedrez.   2.60   \n",
       "4     Un hombre sentado está tocando el violonchelo.   4.25   \n",
       "\n",
       "                                     sent1_trans  \n",
       "0                         A plane is taking off.  \n",
       "1                A man is playing a large flute.  \n",
       "2  A man is spreading shreded cheese on a pizza.  \n",
       "3                   Three men are playing chess.  \n",
       "4                    A man is playing the cello.  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_es_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "2440ba4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>score</th>\n",
       "      <th>sent1_trans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>El hombre golpeó al otro hombre con un palo.</td>\n",
       "      <td>El hombre golpeó al otro hombre con un palo.</td>\n",
       "      <td>4.200</td>\n",
       "      <td>The man hit the other man with a stick.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Una mujer recoge y sostiene un bebé canguro.</td>\n",
       "      <td>Una mujer coge y sostiene un bebé canguro en s...</td>\n",
       "      <td>4.600</td>\n",
       "      <td>A woman picks up and holds a baby kangaroo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Un hombre está tocando una flauta.</td>\n",
       "      <td>Un hombre está tocando una flauta de bambú.</td>\n",
       "      <td>3.867</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Una persona está doblando un pedazo de papel.</td>\n",
       "      <td>Alguien está doblando un pedazo de papel.</td>\n",
       "      <td>4.667</td>\n",
       "      <td>A person is folding a piece of paper.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Un hombre está corriendo en la carretera.</td>\n",
       "      <td>Un perro panda está corriendo en la carretera.</td>\n",
       "      <td>1.667</td>\n",
       "      <td>A man is running on the road.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Un perro está tratando de quitarse el tocino d...</td>\n",
       "      <td>Un perro está tratando de comerse el tocino de...</td>\n",
       "      <td>3.750</td>\n",
       "      <td>A dog is trying to get bacon off his back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>El oso polar se está deslizando en la nieve.</td>\n",
       "      <td>Un oso polar se está deslizando por la nieve.</td>\n",
       "      <td>5.000</td>\n",
       "      <td>The polar bear is sliding on the snow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Una mujer está escribiendo.</td>\n",
       "      <td>Una mujer está nadando.</td>\n",
       "      <td>0.500</td>\n",
       "      <td>A woman is writing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Un gato se está frotando contra la cara del bebé.</td>\n",
       "      <td>Un gato se está frotando contra un bebé.</td>\n",
       "      <td>3.800</td>\n",
       "      <td>A cat is rubbing against baby's face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>El hombre está montando a caballo.</td>\n",
       "      <td>Un hombre está montando a caballo.</td>\n",
       "      <td>5.000</td>\n",
       "      <td>The man is riding a horse.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent1  \\\n",
       "10       El hombre golpeó al otro hombre con un palo.   \n",
       "11       Una mujer recoge y sostiene un bebé canguro.   \n",
       "12                 Un hombre está tocando una flauta.   \n",
       "13      Una persona está doblando un pedazo de papel.   \n",
       "14          Un hombre está corriendo en la carretera.   \n",
       "15  Un perro está tratando de quitarse el tocino d...   \n",
       "16       El oso polar se está deslizando en la nieve.   \n",
       "17                        Una mujer está escribiendo.   \n",
       "18  Un gato se está frotando contra la cara del bebé.   \n",
       "19                 El hombre está montando a caballo.   \n",
       "\n",
       "                                                sent2  score  \\\n",
       "10       El hombre golpeó al otro hombre con un palo.  4.200   \n",
       "11  Una mujer coge y sostiene un bebé canguro en s...  4.600   \n",
       "12        Un hombre está tocando una flauta de bambú.  3.867   \n",
       "13          Alguien está doblando un pedazo de papel.  4.667   \n",
       "14     Un perro panda está corriendo en la carretera.  1.667   \n",
       "15  Un perro está tratando de comerse el tocino de...  3.750   \n",
       "16      Un oso polar se está deslizando por la nieve.  5.000   \n",
       "17                            Una mujer está nadando.  0.500   \n",
       "18           Un gato se está frotando contra un bebé.  3.800   \n",
       "19                 Un hombre está montando a caballo.  5.000   \n",
       "\n",
       "                                    sent1_trans  \n",
       "10      The man hit the other man with a stick.  \n",
       "11  A woman picks up and holds a baby kangaroo.  \n",
       "12                    A man is playing a flute.  \n",
       "13        A person is folding a piece of paper.  \n",
       "14                A man is running on the road.  \n",
       "15   A dog is trying to get bacon off his back.  \n",
       "16       The polar bear is sliding on the snow.  \n",
       "17                          A woman is writing.  \n",
       "18        A cat is rubbing against baby's face.  \n",
       "19                   The man is riding a horse.  "
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_es_df[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31246959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/AH00434/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/AH00434/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c0fb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = nltk.word_tokenize(text, language='spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d301c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd0f9b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_es = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "567e2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a36e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_es(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Replace numbers with num\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Lower case\n",
    "    text= text.lower()\n",
    "    #sent_token = text.split()\n",
    "    sent_token = nltk.word_tokenize(text, language='spanish')\n",
    "    #stop words removal\n",
    "    sent_token = [word for word in sent_token if word.lower() not in stop_words_es]\n",
    "    # Lemmatize\n",
    "    sent_token = [lemmatizer.lemmatize(word) for word in sent_token]\n",
    "    # Stemming\n",
    "    #sent_token = [ps.stem(word) for word in sent_token]\n",
    "    return sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b81ca313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Replace numbers with num\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Lower case\n",
    "    text= text.lower()\n",
    "    #sent_token = text.split()\n",
    "    sent_token = nltk.word_tokenize(text, language='english')\n",
    "    #stop words removal\n",
    "    sent_token = [word for word in sent_token if word.lower() not in stop_words]\n",
    "    # Lemmatize\n",
    "    sent_token = [lemmatizer.lemmatize(word) for word in sent_token]\n",
    "    # Stemming\n",
    "    #sent_token = [ps.stem(word) for word in sent_token]\n",
    "    return sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0ba729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sent1'] = train_en_df['sent1'].apply(lambda x: preprocess_text(x))\n",
    "train_df['sent2'] = train_es_df['sent2'].apply(lambda x: preprocess_text_es(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1500c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[plane, taking]</td>\n",
       "      <td>[avión, despegando]</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[man, playing, large, flute]</td>\n",
       "      <td>[hombre, tocando, flauta]</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[man, spreading, shreded, cheese, pizza]</td>\n",
       "      <td>[hombre, untando, queso, rallado, pizza, cruda]</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[three, men, playing, chess]</td>\n",
       "      <td>[do, hombre, jugando, ajedrez]</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[man, playing, cello]</td>\n",
       "      <td>[hombre, sentado, tocando, violonchelo]</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sent1  \\\n",
       "0                           [plane, taking]   \n",
       "1              [man, playing, large, flute]   \n",
       "2  [man, spreading, shreded, cheese, pizza]   \n",
       "3              [three, men, playing, chess]   \n",
       "4                     [man, playing, cello]   \n",
       "\n",
       "                                             sent2  score  \n",
       "0                              [avión, despegando]   5.00  \n",
       "1                        [hombre, tocando, flauta]   3.80  \n",
       "2  [hombre, untando, queso, rallado, pizza, cruda]   3.80  \n",
       "3                   [do, hombre, jugando, ajedrez]   2.60  \n",
       "4          [hombre, sentado, tocando, violonchelo]   4.25  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7919bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = list(train_df['sent1'])\n",
    "es_sents = list(train_df['sent2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd0d81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_en_dict = {}\n",
    "for word_tokens in en_sents:\n",
    "    for word in word_tokens:\n",
    "        if word in word_en_dict:\n",
    "            word_en_dict[word] += 1\n",
    "        else:\n",
    "            word_en_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e41c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_es_dict = {}\n",
    "for word_tokens in es_sents:\n",
    "    for word in word_tokens:\n",
    "        if word in word_es_dict:\n",
    "            word_es_dict[word] += 1\n",
    "        else:\n",
    "            word_es_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "174eaaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9823"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_es_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8aa130f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8079"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_en_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50de5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = {}\n",
    "for sent in es_sents:\n",
    "    if len(sent) in sent_dict:\n",
    "        sent_dict[len(sent)] += 1\n",
    "    else:\n",
    "        sent_dict[len(sent)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48a0aa46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArcAAAIjCAYAAAAZajMiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABI/0lEQVR4nO3deVhV5f7//9cWmUQBQZkUAYdCFIcccS7IuaPpySxLNNNzTlgpaebJuRSlyTTT1NL6XHnqWFppOZJmGeGcY6aG6VGRFAHRQIX1+6Mf+9sWTTaCW5bPx3Xt62Lf6973eq+bdY6vbtZey2IYhiEAAADABCo4ugAAAACgtBBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAZiaxWLRpEmTHF3GDXXq1EkNGzYs8edDQ0M1aNCg0iuonOvUqZM6depUos8OGjRIoaGhpVrPzeL3CxQf4RaAjWnTpumzzz5zdBmmdPLkSU2aNEm7du1ydCkOt3//fk2aNElHjx51dCmmxjmHOxHhFoANwm3ZOXnypCZPnlwmQePgwYNasGBBqY9bVvbv36/JkyeXWbhdu3at1q5dW6LPLliwQAcPHizlihyjLM854HZV0dEFAABunqurq6NLKDOGYSg3N1fu7u7F/oyLi0uJ9+fs7FzizwJwPFZugdvU+fPnNWLECIWGhsrV1VV+fn66//77tWPHDpt+KSkp6tq1q7y8vFSpUiV17NhRmzdvtukzadIkWSwWHT58WIMGDZK3t7e8vLw0ePBgXbx40drPYrHowoULev/992WxWGSxWGyu8ztx4oSeeOIJ+fv7y9XVVQ0aNNB7771ns6+NGzfKYrHov//9r6ZOnaqaNWvKzc1N0dHROnz4cJHjTElJUffu3VW1alV5eHioUaNGevPNN236/PTTT/r73/8uHx8fubm5qXnz5vriiy9KOrVlchxz5sxR7dq15e7urpYtW+rbb7+1ue5z48aNatGihSRp8ODB1vldvHixzTj79+/Xvffeq0qVKqlGjRpKTEws1jFdfU3m4sWLZbFYtHnzZsXHx6t69ery8PDQgw8+qN9++61YYxZn3i9fvqzJkyerXr16cnNzk6+vr9q1a6d169Zdd9zFixfroYcekiTde++91rnYuHGj9Vh69uypNWvWqHnz5nJ3d9c777wjSVq0aJHuu+8++fn5ydXVVREREZo7d26RfVx9za09v8+rr7k9evSoLBaLXn31Vc2fP1916tSRq6urWrRooa1btxbZ99KlSxURESE3Nzc1bNhQy5cvL/Z1vIZh6OWXX1bNmjVVqVIl3Xvvvdq3b1+RfhkZGRo1apQiIyNVuXJleXp6qlu3bvrxxx9tjvmvzrlvv/1WDz30kGrVqiVXV1cFBwdr5MiR+v33329YJ3A7Y+UWuE3985//1CeffKLhw4crIiJCZ8+e1XfffacDBw7onnvukSR9/fXX6tatm5o1a6aJEyeqQoUK1n/8v/32W7Vs2dJmzH79+iksLEwJCQnasWOHFi5cKD8/P82YMUOS9H//93968skn1bJlSw0bNkySVKdOHUnS6dOn1bp1a1ksFg0fPlzVq1fXqlWrNGTIEGVnZ2vEiBE2+5o+fboqVKigUaNGKSsrS4mJiRowYIBSUlKsfdatW6eePXsqMDBQzz77rAICAnTgwAGtXLlSzz77rCRp3759atu2rWrUqKEXXnhBHh4e+u9//6vevXvr008/1YMPPmjXvJbFccydO1fDhw9X+/btNXLkSB09elS9e/dW1apVVbNmTUlS/fr1NWXKFE2YMEHDhg1T+/btJUlt2rSxjnPu3Dl17dpVffr0Ub9+/fTJJ59ozJgxioyMVLdu3ew6zkJPP/20qlatqokTJ+ro0aOaOXOmhg8fro8//vgvP1fceZ80aZISEhKs5012dra2bdumHTt26P7777/m2B06dNAzzzyjWbNm6d///rfq169vnaNCBw8e1COPPKJ//OMfGjp0qO6++27rXDdo0EB/+9vfVLFiRa1YsUJPPfWUCgoKFBcXd8P5KM7v83qWLFmi8+fP6x//+IcsFosSExPVp08f/fLLL9bV3i+//FIPP/ywIiMjlZCQoHPnzmnIkCGqUaPGDceXpAkTJujll19W9+7d1b17d+3YsUOdO3fWpUuXbPr98ssv+uyzz/TQQw8pLCxMp0+f1jvvvKOOHTtq//79CgoKuuE5t3TpUl28eFH/+te/5Ovrqy1btmj27Nn63//+p6VLlxarXuC2ZAC4LXl5eRlxcXHX3V5QUGDUq1fP6NKli1FQUGBtv3jxohEWFmbcf//91raJEycakownnnjCZowHH3zQ8PX1tWnz8PAwYmNji+xvyJAhRmBgoHHmzBmb9v79+xteXl7GxYsXDcMwjA0bNhiSjPr16xt5eXnWfm+++aYhydizZ49hGIZx5coVIywszAgJCTHOnTtX5NgKRUdHG5GRkUZubq7N9jZt2hj16tW77vwUkmRMnDixzI4jLy/P8PX1NVq0aGFcvnzZ2m/x4sWGJKNjx47Wtq1btxqSjEWLFhWps2PHjoYk44MPPrC25eXlGQEBAUbfvn1veJwhISE2v7dFixYZkoyYmBib+Rw5cqTh5ORkZGZm/uV4xZ33xo0bGz169LhhfVdbunSpIcnYsGHDNY9FkrF69eoi2wp/P3/WpUsXo3bt2jZtHTt2tJn74v4+DcMwYmNjjZCQEOv71NRUQ5Lh6+trZGRkWNs///xzQ5KxYsUKa1tkZKRRs2ZN4/z589a2jRs3GpJsxryW9PR0w8XFxejRo4fN7+zf//63Icnm95ubm2vk5+fbfD41NdVwdXU1pkyZYm37q3PuWnOZkJBgWCwW49dff/3LWoHbGZclALcpb29vpaSk6OTJk9fcvmvXLh06dEiPPvqozp49qzNnzujMmTO6cOGCoqOjtWnTJhUUFNh85p///KfN+/bt2+vs2bPKzs7+y1oMw9Cnn36qBx54QIZhWPd15swZdenSRVlZWUUulxg8eLDNdY+Fq0a//PKLJGnnzp1KTU3ViBEj5O3tbfNZi8Ui6Y8/vX799dfq16+fzp8/b93n2bNn1aVLFx06dEgnTpz4y9rL+ji2bdums2fPaujQoapY8f/9MWzAgAGqWrVqsWuTpMqVK+uxxx6zvndxcVHLli2t+yqJYcOGWeezsP78/Hz9+uuv1/2MPfPu7e2tffv26dChQyWu8VrCwsLUpUuXIu1/vu42KytLZ86cUceOHfXLL78oKyvrhuPe6Pf5Vx5++GGb3+nVnz158qT27NmjgQMHqnLlytZ+HTt2VGRk5A3HX79+vS5duqSnn37a5nd29V8TpD+usa5Q4Y9/wvPz83X27FlVrlxZd999d5Fz+Hr+PJcXLlzQmTNn1KZNGxmGoZ07dxZrDOB2xGUJwG0qMTFRsbGxCg4OVrNmzdS9e3cNHDhQtWvXliRrmIiNjb3uGFlZWTb/GNeqVctme+G2c+fOydPT87rj/Pbbb8rMzNT8+fM1f/78a/ZJT0+3ef9X+5KkI0eOSNJf3tv18OHDMgxD48eP1/jx46+73+L+ybcsjqMwJNatW9emX8WKFe2+V2rNmjVtQk3h/nbv3m3XOH92o/qvxZ55nzJlinr16qW77rpLDRs2VNeuXfX444+rUaNGJa5Z+iPcXsvmzZs1ceJEJScn21wvLv1xvnt5ef3luCWZj+J+9nrnQmHbjUJn4efr1atn0169evUi/6FUUFCgN998U2+//bZSU1OVn59v3ebr63vDY5GkY8eOacKECfriiy+KHH9x/kMBuF0RboHbVL9+/dS+fXstX75ca9eu1SuvvKIZM2Zo2bJl6tatm3VV9pVXXlGTJk2uOcafV48kycnJ6Zr9DMP4y1oK9/XYY49dN0xfHWZKuq9r7XfUqFHXXMWTrh0kbjTerT6O4iqLfZVkTHvmvUOHDjpy5Ig+//xzrV27VgsXLtQbb7yhefPm6cknnyxx3de6M8KRI0cUHR2t8PBwvf766woODpaLi4u++uorvfHGG0X+UnEtNzPHt/JcuJFp06Zp/PjxeuKJJ/TSSy/Jx8dHFSpU0IgRI4o1D/n5+br//vuVkZGhMWPGKDw8XB4eHjpx4oQGDRpUrDGA2xXhFriNBQYG6qmnntJTTz2l9PR03XPPPZo6daq6detm/aKXp6enYmJiSm2fV68cSn+sHFWpUkX5+fmltq/C+vfu3XvdMQtXqZ2dnUtlv2VxHCEhIZL+WO289957re1XrlzR0aNHbcLyteb2dmTvvPv4+Gjw4MEaPHiwcnJy1KFDB02aNOkvw21J5mLFihXKy8vTF198YbOKumHDBrvHKgt/Pheudq22633+0KFD1t+B9MdfHK5eWf3kk09077336t1337Vpz8zMVLVq1azvrzfPe/bs0c8//6z3339fAwcOtLb/1V0ugPKCa26B21B+fn6RPwv6+fkpKChIeXl5kqRmzZqpTp06evXVV5WTk1NkjOLe7ulqHh4eyszMtGlzcnJS37599emnn2rv3r2lsq977rlHYWFhmjlzZpH9Fa6E+fn5qVOnTnrnnXd06tSpm95vWRxH8+bN5evrqwULFujKlSvW9g8//LBIIPHw8JCkIsd7u7Fn3s+ePWuzrXLlyqpbt671PL2eksxF4crpn1dKs7KytGjRomKPUZaCgoLUsGFDffDBBzb/m/zmm2+0Z8+eG34+JiZGzs7Omj17ts0xzpw5s0hfJyenIivGS5cuLXIN+vXm+VpzaRhGkdvwAeURK7fAbej8+fOqWbOm/v73v6tx48aqXLmy1q9fr61bt+q1116TJFWoUEELFy5Ut27d1KBBAw0ePFg1atTQiRMntGHDBnl6emrFihV277tZs2Zav369Xn/9dQUFBSksLEytWrXS9OnTtWHDBrVq1UpDhw5VRESEMjIytGPHDq1fv14ZGRl27adChQqaO3euHnjgATVp0kSDBw9WYGCgfvrpJ+3bt09r1qyR9Mf9Y9u1a6fIyEgNHTpUtWvX1unTp5WcnKz//e9/Nvf1LI7SPg4XFxdNmjRJTz/9tO677z7169dPR48e1eLFi1WnTh2blbM6derI29tb8+bNU5UqVeTh4aFWrVpd9/pSRyruvEdERKhTp05q1qyZfHx8tG3bNust7P5KkyZN5OTkpBkzZigrK0uurq7W+9deT+fOneXi4qIHHnhA//jHP5STk6MFCxbIz8/vmiHcEaZNm6ZevXqpbdu2Gjx4sM6dO6e33npLDRs2vOZ/hP5Z9erVNWrUKCUkJKhnz57q3r27du7cqVWrVtmsxkpSz549NWXKFA0ePFht2rTRnj179OGHH9qs+ErXP+fCw8NVp04djRo1SidOnJCnp6c+/fTTYl17DNz2bu3NGQAUR15enjF69GijcePGRpUqVQwPDw+jcePGxttvv12k786dO40+ffoYvr6+hqurqxESEmL069fPSEpKsvYpvBXYb7/9ZvPZwttFpaamWtt++ukno0OHDoa7u3uR2w+dPn3aiIuLM4KDgw1nZ2cjICDAiI6ONubPn2/tU3jLpaVLl9rsq/B2Slffkui7774z7r//futxNmrUyJg9e7ZNnyNHjhgDBw40AgICDGdnZ6NGjRpGz549jU8++eSGc6mrbgVWVscxa9YsIyQkxHB1dTVatmxpbN682WjWrJnRtWtXm36ff/65ERERYVSsWNFmnI4dOxoNGjQoUv/Vt6W6nuvdCmzr1q02/QqP61q34Lpaceb95ZdfNlq2bGl4e3sb7u7uRnh4uDF16lTj0qVLNxx/wYIFRu3atQ0nJyebmkJCQq57e7EvvvjCaNSokeHm5maEhoYaM2bMMN57770i5/H1bgVWnN/n9W4F9sorrxSp51rn10cffWSEh4cbrq6uRsOGDY0vvvjC6Nu3rxEeHn7DOcnPzzcmT55sBAYGGu7u7kanTp2MvXv3Fvn95ubmGs8995y1X9u2bY3k5OQix20Y1z/n9u/fb8TExBiVK1c2qlWrZgwdOtT48ccfr3vrMKC8sBiGA66EBwCTKygoUPXq1dWnTx8tWLDA0eXAwZo0aaLq1atzTStwC3DNLQDcpNzc3CLXP37wwQfKyMiweQQszO/y5cs2115LfzwG98cff+RcAG4RVm4B4CZt3LhRI0eO1EMPPSRfX1/t2LFD7777rurXr6/t27fbPDQA5nb06FHFxMToscceU1BQkH766SfNmzdPXl5e2rt3b7HvQQug5PhCGQDcpNDQUAUHB2vWrFnKyMiQj4+PBg4cqOnTpxNs7zBVq1ZVs2bNtHDhQv3222/y8PBQjx49NH36dIItcIuwcgsAAADT4JpbAAAAmAbhFgAAAKbBNbf645Y9J0+eVJUqVcrN4zEBAADuJIZh6Pz58woKClKFCtdfnyXcSjp58qSCg4MdXQYAAABu4Pjx46pZs+Z1txNuJVWpUkXSH5Pl6enp4GoAAABwtezsbAUHB1tz2/UQbiXrpQienp6EWwAAgNvYjS4h5QtlAAAAMA3CLQAAAEyDcAsAAADTINwCAADANAi3AAAAMA3CLQAAAEyDcAsAAADTINwCAADANAi3AAAAMA3CLQAAAEyDcAsAAADTINwCAADANAi3AAAAMA3CLQAAAEyDcAsAAADTINwCAADANAi3AAAAMA3CLQAAAEyDcAsAAADTqOjoAoCyEvrClzfsc3R6j1tQCQAAuFVYuQUAAIBpEG4BAABgGlyWANwAlzcAAFB+sHILAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADCNio4uALha6Atf3rDP0ek9bkElAACgvGHlFgAAAKZBuAUAAIBpODTc5ufna/z48QoLC5O7u7vq1Kmjl156SYZhWPsYhqEJEyYoMDBQ7u7uiomJ0aFDh2zGycjI0IABA+Tp6Slvb28NGTJEOTk5t/pwAAAA4GAODbczZszQ3Llz9dZbb+nAgQOaMWOGEhMTNXv2bGufxMREzZo1S/PmzVNKSoo8PDzUpUsX5ebmWvsMGDBA+/bt07p167Ry5Upt2rRJw4YNc8QhAQAAwIEc+oWy77//Xr169VKPHn98OSg0NFT/+c9/tGXLFkl/rNrOnDlT48aNU69evSRJH3zwgfz9/fXZZ5+pf//+OnDggFavXq2tW7eqefPmkqTZs2ere/fuevXVVxUUFOSYgwMAAMAt59CV2zZt2igpKUk///yzJOnHH3/Ud999p27dukmSUlNTlZaWppiYGOtnvLy81KpVKyUnJ0uSkpOT5e3tbQ22khQTE6MKFSooJSXlmvvNy8tTdna2zQsAAADln0NXbl944QVlZ2crPDxcTk5Oys/P19SpUzVgwABJUlpamiTJ39/f5nP+/v7WbWlpafLz87PZXrFiRfn4+Fj7XC0hIUGTJ08u7cMBAACAgzk03P73v//Vhx9+qCVLlqhBgwbatWuXRowYoaCgIMXGxpbZfseOHav4+Hjr++zsbAUHB5fZ/u4kN7pHLfenBQAAZcmh4Xb06NF64YUX1L9/f0lSZGSkfv31VyUkJCg2NlYBAQGSpNOnTyswMND6udOnT6tJkyaSpICAAKWnp9uMe+XKFWVkZFg/fzVXV1e5urqWwREBAADAkRx6ze3FixdVoYJtCU5OTiooKJAkhYWFKSAgQElJSdbt2dnZSklJUVRUlCQpKipKmZmZ2r59u7XP119/rYKCArVq1eoWHAUAAABuFw5duX3ggQc0depU1apVSw0aNNDOnTv1+uuv64knnpAkWSwWjRgxQi+//LLq1aunsLAwjR8/XkFBQerdu7ckqX79+uratauGDh2qefPm6fLlyxo+fLj69+/PnRIAAADuMA4Nt7Nnz9b48eP11FNPKT09XUFBQfrHP/6hCRMmWPs8//zzunDhgoYNG6bMzEy1a9dOq1evlpubm7XPhx9+qOHDhys6OloVKlRQ3759NWvWLEccEgAAABzIoeG2SpUqmjlzpmbOnHndPhaLRVOmTNGUKVOu28fHx0dLliwpgwoBAABQnjj0mlsAAACgNBFuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmUbE0BsnMzJS3t3dpDAWYVugLX96wz9HpPW5BJQAAmJfdK7czZszQxx9/bH3fr18/+fr6qkaNGvrxxx9LtTgAAADAHnaH23nz5ik4OFiStG7dOq1bt06rVq1St27dNHr06FIvEAAAACguuy9LSEtLs4bblStXql+/furcubNCQ0PVqlWrUi8QAAAAKC67V26rVq2q48ePS5JWr16tmJgYSZJhGMrPzy/d6gAAAAA72L1y26dPHz366KOqV6+ezp49q27dukmSdu7cqbp165Z6gQAAAEBx2R1u33jjDYWGhur48eNKTExU5cqVJUmnTp3SU089VeoFAgAAAMVld7h1dnbWqFGjirSPHDmyVAoCAAAASqpED3H4v//7P7Vr105BQUH69ddfJUkzZ87U559/XqrFAQAAAPawO9zOnTtX8fHx6tatmzIzM61fIvP29tbMmTNLuz4AAACg2OwOt7Nnz9aCBQv04osvysnJydrevHlz7dmzp1SLAwAAAOxhd7hNTU1V06ZNi7S7urrqwoULpVIUAAAAUBJ2h9uwsDDt2rWrSPvq1atVv3790qgJAAAAKBG775YQHx+vuLg45ebmyjAMbdmyRf/5z3+UkJCghQsXlkWNAAAAQLHYHW6ffPJJubu7a9y4cbp48aIeffRRBQUF6c0331T//v3LokYAAACgWOwOt5I0YMAADRgwQBcvXlROTo78/PxKuy4AAADAbnaH29TUVF25ckX16tVTpUqVVKlSJUnSoUOH5OzsrNDQ0NKuEQAAACgWu79QNmjQIH3//fdF2lNSUjRo0KDSqAkAAAAoEbvD7c6dO9W2bdsi7a1bt77mXRQAAACAW8XucGuxWHT+/Pki7VlZWdanldnjxIkTeuyxx+Tr6yt3d3dFRkZq27Zt1u2GYWjChAkKDAyUu7u7YmJidOjQIZsxMjIyNGDAAHl6esrb21tDhgxRTk6O3bUAAACgfLM73Hbo0EEJCQk2QTY/P18JCQlq166dXWOdO3dObdu2lbOzs1atWqX9+/frtddeU9WqVa19EhMTNWvWLM2bN08pKSny8PBQly5dlJuba+0zYMAA7du3T+vWrdPKlSu1adMmDRs2zN5DAwAAQDln9xfKZsyYoQ4dOujuu+9W+/btJUnffvutsrOz9fXXX9s9VnBwsBYtWmRtCwsLs/5sGIZmzpypcePGqVevXpKkDz74QP7+/vrss8/Uv39/HThwQKtXr9bWrVvVvHlzSX88Irh79+569dVXFRQUVGS/eXl5ysvLs77Pzs62q24AAADcnuxeuY2IiNDu3bvVr18/paen6/z58xo4cKB++uknNWzY0K6xvvjiCzVv3lwPPfSQ/Pz81LRpUy1YsMC6PTU1VWlpaYqJibG2eXl5qVWrVkpOTpYkJScny9vb2xpsJSkmJkYVKlRQSkrKNfebkJAgLy8v6ys4ONiuugEAAHB7KtF9boOCgjRt2rSb3vkvv/yiuXPnKj4+Xv/+97+1detWPfPMM3JxcVFsbKzS0tIkSf7+/jaf8/f3t25LS0srcp/dihUrysfHx9rnamPHjlV8fLz1fXZ2NgEXAADABEoUbjMzM7Vlyxalp6eroKDAZtvAgQOLPU5BQYGaN29uDcpNmzbV3r17NW/ePMXGxpaktGJxdXWVq6trmY0PAAAAx7A73K5YsUIDBgxQTk6OPD09ZbFYrNssFotd4TYwMFARERE2bfXr19enn34qSQoICJAknT59WoGBgdY+p0+fVpMmTax90tPTbca4cuWKMjIyrJ8HAADAncHua26fe+45PfHEE8rJyVFmZqbOnTtnfWVkZNg1Vtu2bXXw4EGbtp9//lkhISGS/vhyWUBAgJKSkqzbs7OzlZKSoqioKElSVFSUMjMztX37dmufr7/+WgUFBWrVqpW9hwcAAIByzO6V2xMnTuiZZ56xPnb3ZowcOVJt2rTRtGnT1K9fP23ZskXz58/X/PnzJf2xEjxixAi9/PLLqlevnsLCwjR+/HgFBQWpd+/ekv5Y6e3atauGDh2qefPm6fLlyxo+fLj69+9/zTslAAAAwLzsDrddunTRtm3bVLt27ZveeYsWLbR8+XKNHTtWU6ZMUVhYmGbOnKkBAwZY+zz//PO6cOGChg0bpszMTLVr106rV6+Wm5ubtc+HH36o4cOHKzo6WhUqVFDfvn01a9asm64PAAAA5Yvd4bZHjx4aPXq09u/fr8jISDk7O9ts/9vf/mbXeD179lTPnj2vu91isWjKlCmaMmXKdfv4+PhoyZIldu0XAAAA5mN3uB06dKgkXTNsWiyWEj2CFwAAACgNdofbq2/9BQAAANwu7L5bwp/l5uaWVh0AAADATbM73Obn5+ull15SjRo1VLlyZf3yyy+SpPHjx+vdd98t9QIBAACA4rI73E6dOlWLFy9WYmKiXFxcrO0NGzbUwoULS7U4AAAAwB52h9sPPvhA8+fP14ABA+Tk5GRtb9y4sX766adSLQ4AAACwh93h9sSJE6pbt26R9oKCAl2+fLlUigIAAABKwu5wGxERoW+//bZI+yeffKKmTZuWSlEAAABASdh9K7AJEyYoNjZWJ06cUEFBgZYtW6aDBw/qgw8+0MqVK8uiRgAAAKBY7F657dWrl1asWKH169fLw8NDEyZM0IEDB7RixQrdf//9ZVEjAAAAUCx2r9xKUvv27bVu3brSrgUAAAC4KXav3NauXVtnz54t0p6ZmanatWuXSlEAAABASdgdbo8ePar8/Pwi7Xl5eTpx4kSpFAUAAACURLEvS/jiiy+sP69Zs0ZeXl7W9/n5+UpKSlJoaGipFgcAAADYo9jhtnfv3pIki8Wi2NhYm23Ozs4KDQ3Va6+9VqrFAQAAAPYodrgtKCiQJIWFhWnr1q2qVq1amRUFAAAAlITdd0tITU0tizoAAACAm1aiW4ElJSUpKSlJ6enp1hXdQu+9916pFAYAAADYy+5wO3nyZE2ZMkXNmzdXYGCgLBZLWdQFAAAA2M3ucDtv3jwtXrxYjz/+eFnUAwAAAJSY3fe5vXTpktq0aVMWtQAAAAA3xe5w++STT2rJkiVlUQsAAABwU+y+LCE3N1fz58/X+vXr1ahRIzk7O9tsf/3110utOAAAAMAedofb3bt3q0mTJpKkvXv32mzjy2UAAABwJLvD7YYNG8qiDgAAAOCm2X3NbaHDhw9rzZo1+v333yVJhmGUWlEAAABASdgdbs+ePavo6Gjddddd6t69u06dOiVJGjJkiJ577rlSLxAAAAAoLrvD7ciRI+Xs7Kxjx46pUqVK1vaHH35Yq1evLtXiAAAAAHvYfc3t2rVrtWbNGtWsWdOmvV69evr1119LrTAAAADAXnav3F64cMFmxbZQRkaGXF1dS6UoAAAAoCTsDrft27fXBx98YH1vsVhUUFCgxMRE3XvvvaVaHAAAAGAPuy9LSExMVHR0tLZt26ZLly7p+eef1759+5SRkaHNmzeXRY0AAABAsdi9ctuwYUP9/PPPateunXr16qULFy6oT58+2rlzp+rUqVMWNQIAAADFYvfKrSR5eXnpxRdfLO1aAAAAgJti98rt6tWr9d1331nfz5kzR02aNNGjjz6qc+fOlWpxAAAAgD3sDrejR49Wdna2JGnPnj2Kj49X9+7dlZqaqvj4+FIvEAAAACguuy9LSE1NVUREhCTp008/1QMPPKBp06Zpx44d6t69e6kXCAAAABSX3Su3Li4uunjxoiRp/fr16ty5syTJx8fHuqILAAAAOILdK7ft2rVTfHy82rZtqy1btujjjz+WJP38889FnloGAAAA3Ep2r9y+9dZbqlixoj755BPNnTtXNWrUkCStWrVKXbt2LfUCAQAAgOKye+W2Vq1aWrlyZZH2N954o1QKAgAAAErK7pVbAAAA4HZFuAUAAIBpEG4BAABgGsUKt7t371ZBQUFZ1wIAAADclGKF26ZNm+rMmTOSpNq1a+vs2bNlWhQAAABQEsUKt97e3kpNTZUkHT16lFVcAAAA3JaKdSuwvn37qmPHjgoMDJTFYlHz5s3l5OR0zb6//PJLqRYIAAAAFFexwu38+fPVp08fHT58WM8884yGDh2qKlWqlHVtAAAAgF2K/RCHwqePbd++Xc8++yzhFgAAALcdu59QtmjRIuvP//vf/yRJNWvWLL2KAAAAgBKy+z63BQUFmjJliry8vBQSEqKQkBB5e3vrpZde4otmAAAAcCi7V25ffPFFvfvuu5o+fbratm0rSfruu+80adIk5ebmaurUqaVeJAAAAFAcdofb999/XwsXLtTf/vY3a1ujRo1Uo0YNPfXUU4RbAAAAOIzdlyVkZGQoPDy8SHt4eLgyMjJKpSgAAACgJOwOt40bN9Zbb71VpP2tt95S48aNS6UoAAAAoCTsviwhMTFRPXr00Pr16xUVFSVJSk5O1vHjx/XVV1+VeoEAAABAcdm9ctuxY0f9/PPPevDBB5WZmanMzEz16dNHBw8eVPv27cuiRgAAAKBY7F65laSgoCC+OAYAAIDbjt0rtwAAAMDtinALAAAA0yDcAgAAwDTsCreGYejYsWPKzc0tq3oAAACAErM73NatW1fHjx8vq3oAAACAErMr3FaoUEH16tXT2bNny6oeAAAAoMTsvuZ2+vTpGj16tPbu3VsW9QAAAAAlZvd9bgcOHKiLFy+qcePGcnFxkbu7u832jIyMUisOAAAAsIfd4XbmzJllUAYAAABw8+wOt7GxsWVRBwAAAHDTSnSf2yNHjmjcuHF65JFHlJ6eLklatWqV9u3bV6rFAQAAAPawO9x+8803ioyMVEpKipYtW6acnBxJ0o8//qiJEyeWeoEAAABAcdkdbl944QW9/PLLWrdunVxcXKzt9913n3744YdSLQ4AAACwh93hds+ePXrwwQeLtPv5+enMmTOlUhQAAABQEnaHW29vb506dapI+86dO1WjRo1SKQoAAAAoCbvDbf/+/TVmzBilpaXJYrGooKBAmzdv1qhRozRw4MCyqBEAAAAoFrvD7bRp0xQeHq7g4GDl5OQoIiJCHTp0UJs2bTRu3LiyqBEAAAAoFrvvc+vi4qIFCxZo/Pjx2rt3r3JyctS0aVPVq1evLOoDAAAAis3ucFuoVq1aCg4OliRZLJZSKwgAAAAoqRI9xOHdd99Vw4YN5ebmJjc3NzVs2FALFy4s7doAAAAAu9i9cjthwgS9/vrrevrppxUVFSVJSk5O1siRI3Xs2DFNmTKl1IsEAAAAisPucDt37lwtWLBAjzzyiLXtb3/7mxo1aqSnn36acAsAAACHsfuyhMuXL6t58+ZF2ps1a6YrV66USlEAAABASdgdbh9//HHNnTu3SPv8+fM1YMCAEhcyffp0WSwWjRgxwtqWm5uruLg4+fr6qnLlyurbt69Onz5t87ljx46pR48eqlSpkvz8/DR69GhCNgAAwB2qWJclxMfHW3+2WCxauHCh1q5dq9atW0uSUlJSdOzYsRI/xGHr1q1655131KhRI5v2kSNH6ssvv9TSpUvl5eWl4cOHq0+fPtq8ebMkKT8/Xz169FBAQIC+//57nTp1SgMHDpSzs7OmTZtWoloAAABQfhUr3O7cudPmfbNmzSRJR44ckSRVq1ZN1apV0759++wuICcnRwMGDNCCBQv08ssvW9uzsrL07rvvasmSJbrvvvskSYsWLVL9+vX1ww8/qHXr1lq7dq3279+v9evXy9/fX02aNNFLL72kMWPGaNKkSXJxcbG7HuB2F/rClzfsc3R6j1tQCQAAt59ihdsNGzaUWQFxcXHq0aOHYmJibMLt9u3bdfnyZcXExFjbwsPDVatWLSUnJ6t169ZKTk5WZGSk/P39rX26dOmif/3rX9q3b5+aNm16zX3m5eUpLy/P+j47O7sMjgwAAAC3Wokf4lAaPvroI+3YsUNbt24tsi0tLU0uLi7y9va2aff391daWpq1z5+DbeH2wm3Xk5CQoMmTJ99k9QAAALjd2B1uc3NzNXv2bG3YsEHp6ekqKCiw2b5jx45ijXP8+HE9++yzWrdundzc3Owt46aMHTvW5jri7Oxs69PWAAAAUH7ZHW6HDBmitWvX6u9//7tatmxZ4kfvbt++Xenp6brnnnusbfn5+dq0aZPeeustrVmzRpcuXVJmZqbN6u3p06cVEBAgSQoICNCWLVtsxi28m0Jhn2txdXWVq6trieoGAADA7cvucLty5Up99dVXatu27U3tODo6Wnv27LFpGzx4sMLDwzVmzBgFBwfL2dlZSUlJ6tu3ryTp4MGDOnbsmPXJaFFRUZo6darS09Pl5+cnSVq3bp08PT0VERFxU/UBAACg/LE73NaoUUNVqlS56R1XqVJFDRs2tGnz8PCQr6+vtX3IkCGKj4+Xj4+PPD09rY/8LbwFWefOnRUREaHHH39ciYmJSktL07hx4xQXF8fKLAAAwB3I7oc4vPbaaxozZox+/fXXsqjHxhtvvKGePXuqb9++6tChgwICArRs2TLrdicnJ61cuVJOTk6KiorSY489poEDB/IIYAAAgDuU3Su3zZs3V25urmrXrq1KlSrJ2dnZZntGRkaJi9m4caPNezc3N82ZM0dz5sy57mdCQkL01VdflXifAAAAMA+7w+0jjzyiEydOaNq0afL39y/xF8oAAACA0mZ3uP3++++VnJysxo0bl0U9AAAAQInZfc1teHi4fv/997KoBQAAALgpdofb6dOn67nnntPGjRt19uxZZWdn27wAAAAAR7H7soSuXbtK+uM+tX9mGIYsFovy8/NLpzIAAADATnaH2w0bNpRFHQAAAMBNszvcduzYsSzqAAAAAG6a3eF206ZNf7m9Q4cOJS4GAAAAuBl2h9tOnToVafvzvW655hYAAACOYvfdEs6dO2fzSk9P1+rVq9WiRQutXbu2LGoEAAAAisXulVsvL68ibffff79cXFwUHx+v7du3l0phAAAAgL3sXrm9Hn9/fx08eLC0hgMAAADsZvfK7e7du23eG4ahU6dOafr06WrSpElp1QUAAADYze5w26RJE1ksFhmGYdPeunVrvffee6VWGAAAAGAvu8NtamqqzfsKFSqoevXqcnNzK7WiAAAAgJKwO9yGhISURR0AAADATbM73EpSUlKSkpKSlJ6eroKCApttXJoAAAAAR7E73E6ePFlTpkxR8+bNFRgYaPMABwAAAMCR7A638+bN0+LFi/X444+XRT0AAABAidl9n9tLly6pTZs2ZVELAAAAcFPsDrdPPvmklixZUha1AAAAADfF7ssScnNzNX/+fK1fv16NGjWSs7OzzfbXX3+91IoDAAAA7FGiJ5QVPols7969Ntv4chkAAAAcye5wu2HDhrKoAwAAALhpdl9zCwAAANyuCLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwDcItAAAATINwCwAAANMg3AIAAMA0CLcAAAAwjYqOLgCAY4S+8OUN+xyd3uMWVAIAQOlh5RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBoODbcJCQlq0aKFqlSpIj8/P/Xu3VsHDx606ZObm6u4uDj5+vqqcuXK6tu3r06fPm3T59ixY+rRo4cqVaokPz8/jR49WleuXLmVhwIAAIDbgEPD7TfffKO4uDj98MMPWrdunS5fvqzOnTvrwoUL1j4jR47UihUrtHTpUn3zzTc6efKk+vTpY92en5+vHj166NKlS/r+++/1/vvva/HixZowYYIjDgkAAAAOVNGRO1+9erXN+8WLF8vPz0/bt29Xhw4dlJWVpXfffVdLlizRfffdJ0latGiR6tevrx9++EGtW7fW2rVrtX//fq1fv17+/v5q0qSJXnrpJY0ZM0aTJk2Si4uLIw4NAAAADnBbXXOblZUlSfLx8ZEkbd++XZcvX1ZMTIy1T3h4uGrVqqXk5GRJUnJysiIjI+Xv72/t06VLF2VnZ2vfvn3X3E9eXp6ys7NtXgAAACj/bptwW1BQoBEjRqht27Zq2LChJCktLU0uLi7y9va26evv76+0tDRrnz8H28LthduuJSEhQV5eXtZXcHBwKR8NAAAAHOG2CbdxcXHau3evPvroozLf19ixY5WVlWV9HT9+vMz3CQAAgLLn0GtuCw0fPlwrV67Upk2bVLNmTWt7QECALl26pMzMTJvV29OnTysgIMDaZ8uWLTbjFd5NobDP1VxdXeXq6lrKRwEAAABHc+jKrWEYGj58uJYvX66vv/5aYWFhNtubNWsmZ2dnJSUlWdsOHjyoY8eOKSoqSpIUFRWlPXv2KD093dpn3bp18vT0VERExK05EAAAANwWHLpyGxcXpyVLlujzzz9XlSpVrNfIenl5yd3dXV5eXhoyZIji4+Pl4+MjT09PPf3004qKilLr1q0lSZ07d1ZERIQef/xxJSYmKi0tTePGjVNcXByrswAAAHcYh4bbuXPnSpI6depk075o0SINGjRIkvTGG2+oQoUK6tu3r/Ly8tSlSxe9/fbb1r5OTk5auXKl/vWvfykqKkoeHh6KjY3VlClTbtVhAAAA4Dbh0HBrGMYN+7i5uWnOnDmaM2fOdfuEhIToq6++Ks3SAAAAUA7dNndLAAAAAG7WbXG3BADlV+gLX96wz9HpPW5BJQAAsHILAAAAEyHcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDR4/C4k8QhVAABgDqzcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADANwi0AAABMg3ALAAAA0yDcAgAAwDQItwAAADCNio4uAAAkKfSFL2/Y5+j0HregEgBAecbKLQAAAEyDlVsApnKjFWBWfwHA3Fi5BQAAgGkQbgEAAGAahFsAAACYBuEWAAAApkG4BQAAgGkQbgEAAGAahFsAAACYBuEWAAAApkG4BQAAgGkQbgEAAGAahFsAAACYRkVHFwAAt5vQF768YZ+j03vcgkoAAPZi5RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmQbgFAACAaRBuAQAAYBqEWwAAAJgG4RYAAACmwRPKAKCMlNaTznhiGgAUHyu3AAAAMA3CLQAAAEyDcAsAAADTINwCAADANAi3AAAAMA3CLQAAAEyDcAsAAADTINwCAADANHiIAwDcIXgYBIA7ASu3AAAAMA3CLQAAAEyDcAsAAADTINwCAADANAi3AAAAMA3CLQAAAEyDcAsAAADT4D63AAC7cL9cALczVm4BAABgGqzcAgAcghVgAGWBlVsAAACYBuEWAAAApkG4BQAAgGkQbgEAAGAafKEMAFCu8cU0AH/Gyi0AAABMg5VbAADECjBgFqzcAgAAwDRMs3I7Z84cvfLKK0pLS1Pjxo01e/ZstWzZ0tFllTlWGgDg9lIa/7/M/7cDJWeKlduPP/5Y8fHxmjhxonbs2KHGjRurS5cuSk9Pd3RpAAAAuIVMEW5ff/11DR06VIMHD1ZERITmzZunSpUq6b333nN0aQAAALiFyv1lCZcuXdL27ds1duxYa1uFChUUExOj5OTka34mLy9PeXl51vdZWVmSpOzs7LIt9k8aTlxzwz57J3e5YZ+CvIs37FOc47pV49xOtTDOnTnO7VQL4zBOWddSWv/WmHUc/LUbzfOtnuPCc94wjL/uaJRzJ06cMCQZ33//vU376NGjjZYtW17zMxMnTjQk8eLFixcvXrx48Spnr+PHj/9lNiz3K7clMXbsWMXHx1vfFxQUKCMjQ76+vrJYLLe8nuzsbAUHB+v48ePy9PS85fu/UzDPZY85vjWY57LHHN8azHPZM9McG4ah8+fPKygo6C/7lftwW61aNTk5Oen06dM27adPn1ZAQMA1P+Pq6ipXV1ebNm9v77Iqsdg8PT3L/YlXHjDPZY85vjWY57LHHN8azHPZM8sce3l53bBPuf9CmYuLi5o1a6akpCRrW0FBgZKSkhQVFeXAygAAAHCrlfuVW0mKj49XbGysmjdvrpYtW2rmzJm6cOGCBg8e7OjSAAAAcAuZItw+/PDD+u233zRhwgSlpaWpSZMmWr16tfz9/R1dWrG4urpq4sSJRS6VQOlinssec3xrMM9ljzm+NZjnsncnzrHFMG50PwUAAACgfCj319wCAAAAhQi3AAAAMA3CLQAAAEyDcAsAAADTINzeBubMmaPQ0FC5ubmpVatW2rJli6NLMo1JkybJYrHYvMLDwx1dVrm3adMmPfDAAwoKCpLFYtFnn31ms90wDE2YMEGBgYFyd3dXTEyMDh065Jhiy6kbzfGgQYOKnNtdu3Z1TLHlVEJCglq0aKEqVarIz89PvXv31sGDB2365ObmKi4uTr6+vqpcubL69u1b5KFB+GvFmedOnToVOZ//+c9/Oqji8mfu3Llq1KiR9UENUVFRWrVqlXX7nXYeE24d7OOPP1Z8fLwmTpyoHTt2qHHjxurSpYvS09MdXZppNGjQQKdOnbK+vvvuO0eXVO5duHBBjRs31pw5c665PTExUbNmzdK8efOUkpIiDw8PdenSRbm5ube40vLrRnMsSV27drU5t//zn//cwgrLv2+++UZxcXH64YcftG7dOl2+fFmdO3fWhQsXrH1GjhypFStWaOnSpfrmm2908uRJ9enTx4FVlz/FmWdJGjp0qM35nJiY6KCKy5+aNWtq+vTp2r59u7Zt26b77rtPvXr10r59+yTdgeexAYdq2bKlERcXZ32fn59vBAUFGQkJCQ6syjwmTpxoNG7c2NFlmJokY/ny5db3BQUFRkBAgPHKK69Y2zIzMw1XV1fjP//5jwMqLP+unmPDMIzY2FijV69eDqnHrNLT0w1JxjfffGMYxh/nrbOzs7F06VJrnwMHDhiSjOTkZEeVWe5dPc+GYRgdO3Y0nn32WccVZUJVq1Y1Fi5ceEeex6zcOtClS5e0fft2xcTEWNsqVKigmJgYJScnO7Ayczl06JCCgoJUu3ZtDRgwQMeOHXN0SaaWmpqqtLQ0m/Pay8tLrVq14rwuZRs3bpSfn5/uvvtu/etf/9LZs2cdXVK5lpWVJUny8fGRJG3fvl2XL1+2OZfDw8NVq1YtzuWbcPU8F/rwww9VrVo1NWzYUGPHjtXFixcdUV65l5+fr48++kgXLlxQVFTUHXkem+IJZeXVmTNnlJ+fX+RJav7+/vrpp58cVJW5tGrVSosXL9bdd9+tU6dOafLkyWrfvr327t2rKlWqOLo8U0pLS5Oka57Xhdtw87p27ao+ffooLCxMR44c0b///W9169ZNycnJcnJycnR55U5BQYFGjBihtm3bqmHDhpL+OJddXFzk7e1t05dzueSuNc+S9OijjyokJERBQUHavXu3xowZo4MHD2rZsmUOrLZ82bNnj6KiopSbm6vKlStr+fLlioiI0K5du+6485hwC1Pr1q2b9edGjRqpVatWCgkJ0X//+18NGTLEgZUBN6d///7WnyMjI9WoUSPVqVNHGzduVHR0tAMrK5/i4uK0d+9erskvY9eb52HDhll/joyMVGBgoKKjo3XkyBHVqVPnVpdZLt19993atWuXsrKy9Mknnyg2NlbffPONo8tyCC5LcKBq1arJycmpyDcWT58+rYCAAAdVZW7e3t666667dPjwYUeXYlqF5y7n9a1Vu3ZtVatWjXO7BIYPH66VK1dqw4YNqlmzprU9ICBAly5dUmZmpk1/zuWSud48X0urVq0kifPZDi4uLqpbt66aNWumhIQENW7cWG+++eYdeR4Tbh3IxcVFzZo1U1JSkrWtoKBASUlJioqKcmBl5pWTk6MjR44oMDDQ0aWYVlhYmAICAmzO6+zsbKWkpHBel6H//e9/Onv2LOe2HQzD0PDhw7V8+XJ9/fXXCgsLs9nerFkzOTs725zLBw8e1LFjxziX7XCjeb6WXbt2SRLn800oKChQXl7eHXkec1mCg8XHxys2NlbNmzdXy5YtNXPmTF24cEGDBw92dGmmMGrUKD3wwAMKCQnRyZMnNXHiRDk5OemRRx5xdGnlWk5Ojs2KSmpqqnbt2iUfHx/VqlVLI0aM0Msvv6x69eopLCxM48ePV1BQkHr37u24osuZv5pjHx8fTZ48WX379lVAQICOHDmi559/XnXr1lWXLl0cWHX5EhcXpyVLlujzzz9XlSpVrNcfenl5yd3dXV5eXhoyZIji4+Pl4+MjT09PPf3004qKilLr1q0dXH35caN5PnLkiJYsWaLu3bvL19dXu3fv1siRI9WhQwc1atTIwdWXD2PHjlW3bt1Uq1YtnT9/XkuWLNHGjRu1Zs2aO/M8dvTtGmAYs2fPNmrVqmW4uLgYLVu2NH744QdHl2QaDz/8sBEYGGi4uLgYNWrUMB5++GHj8OHDji6r3NuwYYMhqcgrNjbWMIw/bgc2fvx4w9/f33B1dTWio6ONgwcPOrbocuav5vjixYtG586djerVqxvOzs5GSEiIMXToUCMtLc3RZZcr15pfScaiRYusfX7//XfjqaeeMqpWrWpUqlTJePDBB41Tp045ruhy6EbzfOzYMaNDhw6Gj4+P4erqatStW9cYPXq0kZWV5djCy5EnnnjCCAkJMVxcXIzq1asb0dHRxtq1a63b77Tz2GIYhnErwzQAAABQVrjmFgAAAKZBuAUAAIBpEG4BAABgGoRbAAAAmAbhFgAAAKZBuAUAAIBpEG4BAABgGoRbAAAAmAbhFgAgSTp69KgsFot27drl6FIkSYMGDeKRzQDsRrgFgDK0ePFieXt7O7qM29rtFqoBlG+EWwAAAJgG4RbAHe2TTz5RZGSk3N3d5evrq5iYGF24cMG6feHChapfv77c3NwUHh6ut99+27qtcMVx2bJluvfee1WpUiU1btxYycnJkqSNGzdq8ODBysrKksVikcVi0aRJkyRJeXl5GjVqlGrUqCEPDw+1atVKGzdutI5duOK7Zs0a1a9fX5UrV1bXrl116tQpm/rfe+89NWjQQK6urgoMDNTw4cOt2zIzM/Xkk0+qevXq8vT01H333acff/zRrvnZu3evunXrpsqVK8vf31+PP/64zpw5Y93eqVMnPfPMM3r++efl4+OjgIAA6zEW+umnn9SuXTu5ubkpIiJC69evl8Vi0WeffSZJCgsLkyQ1bdpUFotFnTp1svn8q6++qsDAQPn6+iouLk6XL1+26xgA3FkItwDuWKdOndIjjzyiJ554QgcOHNDGjRvVp08fGYYhSfrwww81YcIETZ06VQcOHNC0adM0fvx4vf/++zbjvPjiixo1apR27dqlu+66S4888oiuXLmiNm3aaObMmfL09NSpU6d06tQpjRo1SpI0fPhwJScn66OPPtLu3bv10EMPqWvXrjp06JB13IsXL+rVV1/V//3f/2nTpk06duyY9fOSNHfuXMXFxWnYsGHas2ePvvjiC9WtW9e6/aGHHlJ6erpWrVql7du365577lF0dLQyMjKKNT+ZmZm677771LRpU23btk2rV6/W6dOn1a9fP5t+77//vjw8PJSSkqLExERNmTJF69atkyTl5+erd+/eqlSpklJSUjR//ny9+OKLNp/fsmWLJGn9+vU6deqUli1bZt22YcMGHTlyRBs2bND777+vxYsXa/HixcWqH8AdygCAO9T27dsNScbRo0evub1OnTrGkiVLbNpeeuklIyoqyjAMw0hNTTUkGQsXLrRu37dvnyHJOHDggGEYhrFo0SLDy8vLZoxff/3VcHJyMk6cOGHTHh0dbYwdO9b6OUnG4cOHrdvnzJlj+Pv7W98HBQUZL7744jVr//bbbw1PT08jNze3yDG988471/xM4fHs3LnTeqydO3e26XP8+HFDknHw4EHDMAyjY8eORrt27Wz6tGjRwhgzZoxhGIaxatUqo2LFisapU6es29etW2dIMpYvX37N/RaKjY01QkJCjCtXrljbHnroIePhhx++Zv0AYBiGUdFxsRoAHKtx48aKjo5WZGSkunTpos6dO+vvf/+7qlatqgsXLujIkSMaMmSIhg4dav3MlStX5OXlZTNOo0aNrD8HBgZKktLT0xUeHn7N/e7Zs0f5+fm66667bNrz8vLk6+trfV+pUiXVqVPHZuz09HTr+CdPnlR0dPQ19/Hjjz8qJyfHZjxJ+v3333XkyJHrzsnVY2zYsEGVK1cusu3IkSPW+v98/FfXefDgQQUHBysgIMC6vWXLlsXavyQ1aNBATk5ONmPv2bOn2J8HcOch3AK4Yzk5OWndunX6/vvvtXbtWs2ePVsvvviiUlJSVKlSJUnSggUL1KpVqyKf+zNnZ2frzxaLRZJUUFBw3f3m5OTIyclJ27dvLzLWn4Pkn8ctHNv4/y+ZcHd3/8tjy8nJUWBgoM11vIWKe/eGnJwcPfDAA5oxY0aRbYUh/np1/tXx26MsxwZgToRbAHc0i8Witm3bqm3btpowYYJCQkK0fPlyxcfHKygoSL/88osGDBhQ4vFdXFyUn59v09a0aVPl5+crPT1d7du3L9G4VapUUWhoqJKSknTvvfcW2X7PPfcoLS1NFStWVGhoaIn2cc899+jTTz9VaGioKlYs2T8Xd999t44fP67Tp0/L399fkrR161abPi4uLpJUZJ4AoCT4QhmAO1ZKSoqmTZumbdu26dixY1q2bJl+++031a9fX5I0efJkJSQkaNasWfr555+1Z88eLVq0SK+//nqx9xEaGqqcnBwlJSXpzJkzunjxou666y4NGDBAAwcO1LJly5SamqotW7YoISFBX375ZbHHnjRpkl577TXNmjVLhw4d0o4dOzR79mxJUkxMjKKiotS7d2+tXbtWR48e1ffff68XX3xR27ZtK9b4cXFxysjI0COPPKKtW7fqyJEjWrNmjQYPHlzsIHr//ferTp06io2N1e7du7V582aNGzdO0v9b5fbz85O7u7v1C2tZWVnFngMAuBrhFsAdy9PTU5s2bVL37t111113ady4cXrttdfUrVs3SdKTTz6phQsXatGiRYqMjFTHjh21ePFi662riqNNmzb65z//qYcffljVq1dXYmKiJGnRokUaOHCgnnvuOd19993q3bu3tm7dqlq1ahV77NjYWM2cOVNvv/22GjRooJ49e1rvtmCxWPTVV1+pQ4cOGjx4sO666y71799fv/76q3UF9UaCgoK0efNm5efnq3PnzoqMjNSIESPk7e2tChWK98+Hk5OTPvvsM+Xk5KhFixZ68sknrXdLcHNzkyRVrFhRs2bN0jvvvKOgoCD16tWr2HMAAFezGIUXcAEAcAts3rxZ7dq10+HDh22+MAcApYFwCwAoU8uXL1flypVVr149HT58WM8++6yqVq2q7777ztGlATAhvlAGAChT58+f15gxY3Ts2DFVq1ZNMTExeu211xxdFgCTYuUWAAAApsEXygAAAGAahFsAAACYBuEWAAAApkG4BQAAgGkQbgEAAGAahFsAAACYBuEWAAAApkG4BQAAgGn8f3+DqyrGVSVpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.bar(list(sent_dict.keys()), list(sent_dict.values()), width=0.5)\n",
    "plt.title(\"sentence length in es training data\")\n",
    "plt.xlabel(\"sentence length\")\n",
    "plt.ylabel(\"number of sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d55fc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_en_df.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0483dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['sent1'] = val_en_df['sent1'].apply(lambda x: preprocess_text(x))\n",
    "val_df['sent2'] = val_es_df['sent2'].apply(lambda x: preprocess_text_es(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54ea6d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[man, hard, hat, dancing]</td>\n",
       "      <td>[hombre, casco, bailando]</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[young, child, riding, horse]</td>\n",
       "      <td>[niño, montando, caballo]</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[man, feeding, mouse, snake]</td>\n",
       "      <td>[hombre, alimentando, serpiente, ratón]</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[woman, playing, guitar]</td>\n",
       "      <td>[hombre, tocando, guitarra]</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[woman, playing, flute]</td>\n",
       "      <td>[hombre, tocando, flauta]</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           sent1                                    sent2  \\\n",
       "0      [man, hard, hat, dancing]                [hombre, casco, bailando]   \n",
       "1  [young, child, riding, horse]                [niño, montando, caballo]   \n",
       "2   [man, feeding, mouse, snake]  [hombre, alimentando, serpiente, ratón]   \n",
       "3       [woman, playing, guitar]              [hombre, tocando, guitarra]   \n",
       "4        [woman, playing, flute]                [hombre, tocando, flauta]   \n",
       "\n",
       "   score  \n",
       "0   5.00  \n",
       "1   4.75  \n",
       "2   5.00  \n",
       "3   2.40  \n",
       "4   2.75  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14a4df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_en_df.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8601b047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sent1  \\\n",
       "0                    A girl is styling her hair.   \n",
       "1       A group of men play soccer on the beach.   \n",
       "2  One woman is measuring another woman's ankle.   \n",
       "3                A man is cutting up a cucumber.   \n",
       "4                       A man is playing a harp.   \n",
       "\n",
       "                                              sent2  score  \n",
       "0                      A girl is brushing her hair.    2.5  \n",
       "1  A group of boys are playing soccer on the beach.    3.6  \n",
       "2           A woman measures another woman's ankle.    5.0  \n",
       "3                      A man is slicing a cucumber.    4.2  \n",
       "4                      A man is playing a keyboard.    1.5  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0586905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['sent1'] = test_en_df['sent1'].apply(lambda x: preprocess_text(x))\n",
    "test_df['sent2'] = test_es_df['sent2'].apply(lambda x: preprocess_text_es(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2243c936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[girl, styling, hair]</td>\n",
       "      <td>[chica, cepillando, pelo]</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[group, men, play, soccer, beach]</td>\n",
       "      <td>[grupo, chico, jugando, fútbol, playa]</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[one, woman, measuring, another, woman, ankle]</td>\n",
       "      <td>[mujer, mide, tobillo, mujer]</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[man, cutting, cucumber]</td>\n",
       "      <td>[hombre, cortando, pepino]</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[man, playing, harp]</td>\n",
       "      <td>[hombre, tocando, teclado]</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sent1  \\\n",
       "0                           [girl, styling, hair]   \n",
       "1               [group, men, play, soccer, beach]   \n",
       "2  [one, woman, measuring, another, woman, ankle]   \n",
       "3                        [man, cutting, cucumber]   \n",
       "4                            [man, playing, harp]   \n",
       "\n",
       "                                    sent2  score  \n",
       "0               [chica, cepillando, pelo]    2.5  \n",
       "1  [grupo, chico, jugando, fútbol, playa]    3.6  \n",
       "2           [mujer, mide, tobillo, mujer]    5.0  \n",
       "3              [hombre, cortando, pepino]    4.2  \n",
       "4              [hombre, tocando, teclado]    1.5  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2c31c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../data/cleaned_cross_train_df1.csv\", index=False)  \n",
    "val_df.to_csv(\"../data/cleaned_cross_val_df1.csv\", index=False)  \n",
    "test_df.to_csv(\"../data/cleaned_cross_test_df1.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5099da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/word_dict_cross_en_v1.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_en_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6eb8017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_dict_cross_es_v1.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_es_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faffc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06f8bb0",
   "metadata": {},
   "source": [
    "## Word2vec-STS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f534adb0",
   "metadata": {},
   "source": [
    "<p> Pretrained word2vec embeddings for english and spanish are used to generated vector embeddings for each word. These embeddings are averaged and concatenated fed to BiLSTM Regresssion model for similarity score generation <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "872e4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import KeyedVectors\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f19d05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_model = KeyedVectors.load_word2vec_format(\"../data/SBW-vectors-300-min5.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9fed3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "91afe030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate sentence embeddings\n",
    "def get_sentence_embedding_es(sentence, max_length=30):\n",
    "    # split sentence into words\n",
    "    #words = sentence.split()\n",
    "    words = sentence\n",
    "    unk_token = \"unk\"\n",
    "    # filter out words that are not present in the model's vocabulary\n",
    "    words = [word if word in es_model.key_to_index else unk_token for word in words ]\n",
    "    if len(words)==0:\n",
    "        words = [\"unk\"]\n",
    "    # generate word embeddings for each word\n",
    "    embeddings = [es_model[word] for word in words]\n",
    "    # pad embeddings with zeros if the sentence is shorter than max_length\n",
    "    embedding = np.mean(embeddings, axis=0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8a937e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate sentence embeddings\n",
    "def get_sentence_embedding_en(sentence, max_length=30):\n",
    "    # split sentence into words\n",
    "    #words = sentence.split()\n",
    "    words = sentence\n",
    "    unk_token = \"unk\"\n",
    "    # filter out words that are not present in the model's vocabulary\n",
    "    words = [word if word in en_model.key_to_index else unk_token for word in words ]\n",
    "    if len(words)==0:\n",
    "        words = [\"unk\"]\n",
    "    # generate word embeddings for each word\n",
    "    embeddings = [en_model[word] for word in words]\n",
    "    # pad embeddings with zeros if the sentence is shorter than max_length\n",
    "    embedding = np.mean(embeddings, axis=0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "93b6c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences1 = list(train_df['sent1'])\n",
    "train_sentences2 = list(train_df['sent2'])\n",
    "train_similarity_scores = list(train_df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b3a6f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in train_sentences1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f708e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in train_sentences2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "14752c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentences1 = list(val_df['sent1'])\n",
    "val_sentences2 = list(val_df['sent2'])\n",
    "val_similarity_scores = list(val_df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "327dc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in val_sentences1])\n",
    "val_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in val_sentences2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f76f2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "# Define a custom dataset class\n",
    "class SentenceSimilarityDataset(data.Dataset):\n",
    "    def __init__(self, embeddings1, embeddings2, scores):\n",
    "        self.embeddings1 = embeddings1\n",
    "        self.embeddings2 = embeddings2\n",
    "        self.scores = scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.embeddings1),len(self.embeddings2))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.embeddings1[index]), torch.tensor(self.embeddings2[index]), torch.tensor(self.scores[index], dtype=torch.float)\n",
    "        #return torch.tensor(self.embeddings1[index]), torch.tensor(self.scores[index], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "fa8b4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentenceSimilarityDataset(train_X1, train_X2, train_similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ac776779",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8aaebca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SentenceSimilarityDataset(val_X1, val_X2, val_similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "bf2ecbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = data.DataLoader(val_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "131d0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiLSTMRegression(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.num_layers = num_layers\n",
    "#         self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         x = torch.cat((x1, x2), dim=1)\n",
    "#         x = x.view(len(x), 1, -1)\n",
    "#         h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "#         c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "#         out, _ = self.bilstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "    \n",
    "class BiLSTMRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out = self.dropout(out) # Apply dropout\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class GRURegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, attention_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_dim = attention_dim\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = SelfAttention(hidden_dim*2, attention_dim, 1)\n",
    "        #self.attention = nn.Linear(hidden_dim*2, attention_dim)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        #att_weights = self.softmax(self.attention(out))\n",
    "        att_weights = self.attention(out)\n",
    "        out = torch.sum(out * att_weights, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the attention block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # TODO implement\n",
    "\n",
    "        # constructing linear layers with weights analogous to Ws1 and Ws2\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    ## the forward function would receive lstm's all hidden states as input\n",
    "    def forward(self, attention_input):\n",
    "        # TODO implement\n",
    "\n",
    "        # implementing the attention mechanism\n",
    "        output = self.layer1(attention_input)\n",
    "        output = torch.tanh(output)\n",
    "        output = self.layer2(output)\n",
    "        #output = F.softmax(output.transpose(1,2), dim=2)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "4a92936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_dim = 300 # The dimension of the sentence embeddings\n",
    "hidden_dim = 150\n",
    "lr = 0.001\n",
    "num_epochs = 15\n",
    "#batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "65033a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GRURegression(input_dim*2, hidden_dim, num_layers=2)\n",
    "model = BiLSTMRegression(input_dim*2, hidden_dim, num_layers=2, dropout_prob = 0.3)\n",
    "#model = BiLSTMAttention(input_dim*2, hidden_dim, num_layers=2, attention_dim=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "e25f7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "class PearsonLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PearsonLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        pearson_r, _ = pearsonr(pred.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "        loss = 1 - pearson_r\n",
    "        return torch.tensor(loss, requires_grad=True, device=pred.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "533f9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# Define the loss function\n",
    "#loss_fn = nn.MSELoss()\n",
    "loss_fn = PearsonLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "03e6a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs, train_dataloader):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for embeddings1_batch, embeddings2_batch, scores_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(embeddings1_batch, embeddings2_batch)\n",
    "            loss = loss_fn(output.squeeze(), scores_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(embeddings1_batch)\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_embeddings1_batch, val_embeddings2_batch, val_scores_batch in val_dataloader:\n",
    "                val_output = model(val_embeddings1_batch, val_embeddings2_batch)\n",
    "                val_loss += loss_fn(val_output.squeeze(), val_scores_batch).item() * len(val_embeddings1_batch)\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        print('Epoch {} - Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, train_loss, val_loss))\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "15dd6d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 9.7208, Validation Loss: 9.6175\n",
      "Epoch 2 - Training Loss: 9.7487, Validation Loss: 9.6562\n",
      "Epoch 3 - Training Loss: 9.8712, Validation Loss: 9.8164\n",
      "Epoch 4 - Training Loss: 9.8837, Validation Loss: 9.4729\n",
      "Epoch 5 - Training Loss: 9.6155, Validation Loss: 9.6676\n",
      "Epoch 6 - Training Loss: 9.6144, Validation Loss: 9.7780\n",
      "Epoch 7 - Training Loss: 9.8278, Validation Loss: 9.9708\n",
      "Epoch 8 - Training Loss: 9.7361, Validation Loss: 9.6792\n",
      "Epoch 9 - Training Loss: 9.7499, Validation Loss: 9.6764\n",
      "Epoch 10 - Training Loss: 10.0480, Validation Loss: 9.6591\n",
      "Epoch 11 - Training Loss: 9.6885, Validation Loss: 9.5048\n",
      "Epoch 12 - Training Loss: 9.7946, Validation Loss: 9.8069\n",
      "Epoch 13 - Training Loss: 9.9145, Validation Loss: 9.5931\n",
      "Epoch 14 - Training Loss: 9.8349, Validation Loss: 9.6208\n",
      "Epoch 15 - Training Loss: 9.7345, Validation Loss: 9.8524\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(model, optimizer, num_epochs, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "7ca28a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    y_pred_test = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for emb1, emb2, scores in data_loader:\n",
    "            test_output = model(emb1, emb2)\n",
    "            y_pred_test.extend(test_output.squeeze().tolist())\n",
    "    return y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "01984f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = evaluate(model, train_dataloader)\n",
    "#corr = pearson_corr(train_similarity_scores, y_pred)\n",
    "#print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9e99350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def pearson_corr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Pearson correlation coefficient between two arrays.\n",
    "    \"\"\"\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ae90e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: -0.00\n"
     ]
    }
   ],
   "source": [
    "corr = pearson_corr(train_similarity_scores, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "3b74887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = evaluate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9aa23618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.03\n"
     ]
    }
   ],
   "source": [
    "corr = pearson_corr(val_similarity_scores, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "04a0790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pearson_corr(test_similarity_scores, y_pred)\n",
    "print(\"Pearson correlation coefficient: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220bac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c32c9aad",
   "metadata": {},
   "source": [
    "# Siamese BiLstm with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "3bb3284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_en = en_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "1ec98a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_es = es_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "875a6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_en = {word: i for i, word in enumerate(en_model.index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "098cd43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_es = {word: i for i, word in enumerate(es_model.index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "abd08a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_dict_cross_en_v1.pickle', 'rb') as f:\n",
    "    vocab_en = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c17089f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_dict_cross_es_v1.pickle', 'rb') as f:\n",
    "    vocab_es = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1e566816",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list_en = list(vocab_en.keys())\n",
    "vocab_list_es = list(vocab_es.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "27a666fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list_en.append(\"unk\")\n",
    "vocab_list_es.append(\"unk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "b5cc37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list_en = [i for i in vocab_list_en if i in word2idx_en]\n",
    "vocab_list_es = [i for i in vocab_list_es if i in word2idx_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "305c9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict_en = {k:i for i,k in enumerate(vocab_list_en)}\n",
    "vocab_dict_es = {k:i for i,k in enumerate(vocab_list_es)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "0d58bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_trunc_en = {}\n",
    "for i in list(vocab_dict_en.keys()):\n",
    "  word2idx_trunc_en[vocab_dict_en[i]] = word2idx_en[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4e59844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_trunc_es = {}\n",
    "for i in list(vocab_dict_es.keys()):\n",
    "  word2idx_trunc_es[vocab_dict_es[i]] = word2idx_es[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "fa61b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes_en = list(word2idx_trunc_en.values())\n",
    "word_indexes_es = list(word2idx_trunc_es.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "41a82a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_embedding_matrix_en = en_model.vectors[word_indexes_en]\n",
    "subset_embedding_matrix_es = es_model.vectors[word_indexes_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "9a45e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sentences1, sentences2, scores, word_to_ix_en, word_to_ix_es ):\n",
    "        self.sentences1 = sentences1\n",
    "        self.sentences2 = sentences2\n",
    "        self.scores = scores\n",
    "        self.word_to_ix_en = word_to_ix_en\n",
    "        self.word_to_ix_es = word_to_ix_es\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.sentences1),len(self.sentences2))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        unk_token1 = self.word_to_ix_en['unk']\n",
    "        unk_token2 = self.word_to_ix_es['unk']\n",
    "        sentence1 = self.sentences1[idx]\n",
    "        sentence2 = self.sentences2[idx]\n",
    "        score = self.scores[idx]\n",
    "        seq1 = [self.word_to_ix_en[word] if word in self.word_to_ix_en else unk_token1 for word in sentence1]\n",
    "        seq2 = [self.word_to_ix_es[word] if word in self.word_to_ix_es else unk_token2 for word in sentence2]\n",
    "        #seq1 = [self.word_to_ix[word] for word in sentence1 if word in self.word_to_ix]\n",
    "        #seq2 = [self.word_to_ix[word] for word in sentence2 if word in self.word_to_ix]\n",
    "        return seq1, seq2, score\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        sequences1, sequences2, scores = zip(*batch)\n",
    "        padded_seqs1 = pad_sequence([torch.LongTensor(seq) for seq in sequences1], batch_first=True, padding_value=0)\n",
    "        padded_seqs2 = pad_sequence([torch.LongTensor(seq) for seq in sequences2], batch_first=True, padding_value=0)\n",
    "        #return padded_seqs1, padded_seqs2, torch.tensor(scores, dtype=torch.float)\n",
    "        return padded_seqs1, padded_seqs2, torch.LongTensor(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "8fe09012",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_tokens = list(train_df['sent1'])\n",
    "sent2_tokens = list(train_df['sent2'])\n",
    "scores = list(train_df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "987d30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix_en = vocab_dict_en\n",
    "word_to_ix_es = vocab_dict_es\n",
    "train_dataset = MyDataset(sent1_tokens, sent2_tokens, scores, word_to_ix_en, word_to_ix_es )\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "62b7b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sent1_tokens = list(val_df['sent1'])\n",
    "val_sent2_tokens = list(val_df['sent2'])\n",
    "val_scores = list(val_df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "aaaedb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MyDataset(val_sent1_tokens, val_sent2_tokens, val_scores, word_to_ix_en, word_to_ix_es)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "0f865591",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent1_tokens = list(test_df['sent1'])\n",
    "test_sent2_tokens = list(test_df['sent2'])\n",
    "test_scores = list(test_df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "22d7b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(test_sent1_tokens, test_sent2_tokens, test_scores, word_to_ix_en, word_to_ix_es)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "aeb086b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6967, 8786)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_embedding_matrix_en), len(subset_embedding_matrix_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "dfee19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SiameseBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, embedding_dim, embd_matrix1, embd_matrix2, dropout=0.2):\n",
    "        super(SiameseBiLSTM, self).__init__()\n",
    "\n",
    "        # LSTM parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embd_matrix1 = embd_matrix1\n",
    "        self.embd_matrix2 = embd_matrix2\n",
    "\n",
    "        # Word embeddings\n",
    "        self.word_embeddings1 = nn.Embedding(len(self.embd_matrix1), embedding_dim)\n",
    "        self.word_embeddings1.weight = nn.Parameter(torch.from_numpy(self.embd_matrix1))\n",
    "        self.word_embeddings1.weight.requires_grad = False\n",
    "        \n",
    "        self.word_embeddings2 = nn.Embedding(len(self.embd_matrix2), embedding_dim)\n",
    "        self.word_embeddings2.weight = nn.Parameter(torch.from_numpy(self.embd_matrix2))\n",
    "        self.word_embeddings2.weight.requires_grad = False\n",
    "\n",
    "        # BiLSTM layers\n",
    "        self.bilstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Attention layers\n",
    "        self.attention_fc = nn.Linear(hidden_size * 2, 1)\n",
    "        self.attention_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Similarity scoring layer\n",
    "        self.fc = nn.Linear(hidden_size * 4, 1)  # 4 because we concatenate forward and backward hidden states of both LSTMs\n",
    "\n",
    "    def forward_once_en(self, sentence):\n",
    "        # Word embeddings\n",
    "        embeds = self.word_embeddings1(sentence)\n",
    "\n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.bilstm(embeds)\n",
    "\n",
    "        # Apply dropout to hidden layers\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention_softmax(self.attention_fc(lstm_out))\n",
    "        lstm_out = lstm_out * attention_weights\n",
    "        lstm_out = lstm_out.sum(dim=1)\n",
    "\n",
    "        return lstm_out\n",
    "    \n",
    "    def forward_once_es(self, sentence):\n",
    "        # Word embeddings\n",
    "        embeds = self.word_embeddings2(sentence)\n",
    "\n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.bilstm(embeds)\n",
    "\n",
    "        # Apply dropout to hidden layers\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention_softmax(self.attention_fc(lstm_out))\n",
    "        lstm_out = lstm_out * attention_weights\n",
    "        lstm_out = lstm_out.sum(dim=1)\n",
    "\n",
    "        return lstm_out\n",
    "\n",
    "    def forward(self, sentence1, sentence2):\n",
    "        # Process sentence 1\n",
    "        output1 = self.forward_once_en(sentence1)\n",
    "\n",
    "        # Process sentence 2\n",
    "        output2 = self.forward_once_es(sentence2)\n",
    "\n",
    "        # Concatenate outputs of both LSTMs\n",
    "        concatenated = torch.cat((output1, output2), dim=1)\n",
    "\n",
    "        # Pass through similarity scoring layer\n",
    "        similarity_score = torch.sigmoid(self.fc(concatenated))\n",
    "\n",
    "        return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "6576a8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/8lc9v2sx5mq19kgysgv67bzw0000gn/T/ipykernel_66582/2886958499.py:33: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return padded_seqs1, padded_seqs2, torch.LongTensor(scores)\n",
      "/var/folders/gb/8lc9v2sx5mq19kgysgv67bzw0000gn/T/ipykernel_66582/1260678673.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score_tensor = torch.tensor(score, dtype=torch.float)/5.0\n",
      "/var/folders/gb/8lc9v2sx5mq19kgysgv67bzw0000gn/T/ipykernel_66582/1260678673.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_score_tensor = torch.tensor(val_score, dtype=torch.float)/5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.1450, Val Loss: 1.0735\n",
      "Epoch [2/10], Train Loss: 1.1797, Val Loss: 1.0836\n",
      "Epoch [3/10], Train Loss: 1.1853, Val Loss: 1.0822\n",
      "Epoch [4/10], Train Loss: 1.1765, Val Loss: 1.0781\n",
      "Epoch [5/10], Train Loss: 1.1836, Val Loss: 1.0598\n",
      "Epoch [6/10], Train Loss: 1.1705, Val Loss: 1.0681\n",
      "Epoch [7/10], Train Loss: 1.1847, Val Loss: 1.0850\n",
      "Epoch [8/10], Train Loss: 1.1824, Val Loss: 1.0770\n",
      "Epoch [9/10], Train Loss: 1.1747, Val Loss: 1.0719\n",
      "Epoch [10/10], Train Loss: 1.1743, Val Loss: 1.0704\n"
     ]
    }
   ],
   "source": [
    "# Define model and optimizer\n",
    "model = SiameseBiLSTM(hidden_size=150, num_layers=2, embedding_dim=300, embd_matrix1 = subset_embedding_matrix_en, embd_matrix2 = subset_embedding_matrix_es)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = PearsonLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i, (sentence1, sentence2, score) in enumerate(train_dataloader):\n",
    "        # Convert inputs and output to PyTorch tensors\n",
    "        sentence1_tensor = sentence1\n",
    "        sentence2_tensor = sentence2\n",
    "        score_tensor = torch.tensor(score, dtype=torch.float)/5.0\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(sentence1_tensor, sentence2_tensor)\n",
    "        \n",
    "        #print(score_tensor.squeeze().shape)\n",
    "        # Compute loss\n",
    "        #loss = criterion(outputs, score_tensor.unsqueeze(-1))\n",
    "        loss = criterion(output.squeeze(), score_tensor.squeeze())\n",
    "        #loss = model.loss(output, score_tensor.unsqueeze(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # add batch loss to total epoch loss\n",
    "        \n",
    "     # Validation loop\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for j, (val_sentence1, val_sentence2, val_score) in enumerate(val_dataloader):\n",
    "            val_sentence1_tensor = val_sentence1\n",
    "            val_sentence2_tensor = val_sentence2\n",
    "            val_score_tensor = torch.tensor(val_score, dtype=torch.float)/5.0\n",
    "            outputs = model(val_sentence1_tensor, val_sentence2_tensor)\n",
    "            #val_loss = criterion(outputs, val_score_tensor.unsqueeze(-1))\n",
    "            val_loss = criterion(outputs.squeeze(), val_score_tensor.squeeze())\n",
    "            #val_loss = model.loss(outputs, val_score_tensor.unsqueeze(-1))\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "    avg_train_loss = epoch_loss / len(train_dataloader) \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}'.format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8fe9f",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning using Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "c6e1f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    predictions_lt = []\n",
    "    labels_lt = []\n",
    "    for i, (sentence1, sentence2, score) in enumerate(iterator):\n",
    "        sentence1_tensor = sentence1\n",
    "        sentence2_tensor = sentence2\n",
    "        score_tensor = torch.tensor(score, dtype=torch.float)/5.0\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(sentence1_tensor, sentence2_tensor)\n",
    "        y_pred = output.detach().tolist()\n",
    "        loss = criterion(output.squeeze(), score_tensor.squeeze())\n",
    "        y_true = score.tolist()\n",
    "        #print(len(y_pred), len(y_true))\n",
    "        predictions_lt.extend(y_pred)\n",
    "        labels_lt.extend(y_true)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    labels = np.array(labels_lt)\n",
    "    predictions = np.array(predictions_lt)\n",
    "    corr = pearson_corr(labels, predictions.ravel())\n",
    "    #print(\"corr is \", corr)\n",
    "    \n",
    "    return epoch_loss / len(iterator), corr\n",
    "                         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "fec7c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    predictions_lt = []\n",
    "    labels_lt = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (sentence1, sentence2, score) in enumerate(iterator):\n",
    "            sentence1_tensor = sentence1\n",
    "            sentence2_tensor = sentence2\n",
    "            score_tensor = torch.tensor(score, dtype=torch.float)/5.0\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(sentence1_tensor, sentence2_tensor)\n",
    "            y_pred = output.detach().tolist()\n",
    "            y_true = score.tolist()\n",
    "            predictions_lt.extend(y_pred)\n",
    "            labels_lt.extend(y_true)\n",
    "            \n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        labels = np.array(labels_lt)\n",
    "        predictions = np.array(predictions_lt)\n",
    "        corr = pearson_corr(labels, predictions.ravel())\n",
    "    \n",
    "    return epoch_loss / len(iterator), corr\n",
    "                         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "8918543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_iterators(BATCH_SIZE=16):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=train_dataset.collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=val_dataset.collate_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=test_dataset.collate_fn)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "ada86786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(N_LAYERS, HIDDEN_DIM, DROPOUT, BATCH_SIZE, optim_type, N_EPOCHS, lr):\n",
    "    #print(\"params are\", HIDDEN_DIM, DROPOUT, BATCH_SIZE, optim_type, N_EPOCHS, lr)\n",
    "    #N_LAYERS = 2\n",
    "    HIDDEN_DIM = HIDDEN_DIM\n",
    "    N_LAYERS = int(N_LAYERS)\n",
    "    BATCH_SIZE = int(BATCH_SIZE)\n",
    "    N_EPOCHS = int(N_EPOCHS)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_iterator, valid_iterator, test_iterator = get_data_iterators(BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    model = SiameseBiLSTM(hidden_size=150, num_layers=2, embedding_dim=300, embd_matrix1 = subset_embedding_matrix_en, embd_matrix2 = subset_embedding_matrix_es, dropout = DROPOUT)\n",
    "    \n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax']\n",
    "    optimizerD= {'Adam':optim.Adam(model.parameters(), lr=lr), 'SGD':optim.SGD(model.parameters(), lr=lr),\n",
    "                 'RMSprop':optim.RMSprop(model.parameters(), lr=lr), 'Adadelta':optim.Adadelta(model.parameters(), lr=lr),\n",
    "                 'Adagrad':optim.Adagrad(model.parameters(), lr=lr), 'Adamax':optim.Adamax(model.parameters(), lr=lr)\n",
    "                 }\n",
    "    \n",
    "    optimizer = optimizerD[optimizerL[round(optim_type)]]\n",
    "    #optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    #model = model.to(device)\n",
    "    #criterion = criterion.to(device)\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "    \n",
    "        train_loss, train_corr = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, val_corr = evaluate(model, valid_iterator, criterion)\n",
    "        test_loss, test_corr = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "        #print(\"the valid acc is \",valid_acc)\n",
    "    \n",
    "\n",
    "        best_valid_loss = float('inf')\n",
    "        best_valid_corr = 0\n",
    "    \n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "        #     best_valid_acc = valid_acc\n",
    "        if best_valid_corr < val_corr:\n",
    "            best_valid_corr = val_corr\n",
    "#             #torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "#             torch.save(model, \"cross_sts_model_v1.pt\")\n",
    "    \n",
    "\n",
    "#         print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "#         print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "#         print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')\n",
    "        \n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}| Train Corr:{train_corr:.3f}| Val. Loss: {valid_loss:.3f}| Val Corr:{val_corr:.3f} | Test. Loss: {test_loss:.3f}| Test Corr:{test_corr:.3f}')\n",
    "#         print(f'\\t Val. Loss: {valid_loss:.3f}| Val Corr:{val_corr:.3f}')\n",
    "#         print(f'\\t Test. Loss: {test_loss:.3f}| Test Corr:{test_corr:.3f}')\n",
    "    \n",
    "    \n",
    "#     print(\"the best valid corr is \", best_valid_corr)\n",
    "    return best_valid_corr\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "8515a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "ffd04a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn = {\n",
    "    'N_LAYERS': (1,3),\n",
    "    'HIDDEN_DIM':(50, 500),\n",
    "    'DROPOUT':(0,1),\n",
    "    'BATCH_SIZE':(10, 32),\n",
    "    'optim_type':(0,5),\n",
    "    'N_EPOCHS':(1, 10),\n",
    "    'lr':(0.001, 1)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "d7482216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "47ca290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_bo = BayesianOptimization(tune_model, params_nn, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "72900411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | BATCH_... |  DROPOUT  | HIDDEN... | N_EPOCHS  | N_LAYERS  |    lr     | optim_... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "\tTrain Loss: 0.084| Train Corr:0.151| Val. Loss: 1.622| Val Corr:0.019 | Test. Loss: 1.622| Test Corr:0.018\n",
      "\tTrain Loss: 0.083| Train Corr:0.168| Val. Loss: 1.622| Val Corr:0.027 | Test. Loss: 1.622| Test Corr:0.028\n",
      "\tTrain Loss: 0.083| Train Corr:0.175| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:0.033\n",
      "\tTrain Loss: 0.083| Train Corr:0.183| Val. Loss: 1.622| Val Corr:0.033 | Test. Loss: 1.622| Test Corr:0.037\n",
      "\tTrain Loss: 0.082| Train Corr:0.190| Val. Loss: 1.622| Val Corr:0.035 | Test. Loss: 1.622| Test Corr:0.040\n",
      "\tTrain Loss: 0.082| Train Corr:0.196| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.043\n",
      "\tTrain Loss: 0.082| Train Corr:0.202| Val. Loss: 1.622| Val Corr:0.037 | Test. Loss: 1.622| Test Corr:0.047\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.03747  \u001b[0m | \u001b[0m23.47    \u001b[0m | \u001b[0m0.1691   \u001b[0m | \u001b[0m246.2    \u001b[0m | \u001b[0m7.923    \u001b[0m | \u001b[0m1.591    \u001b[0m | \u001b[0m0.15     \u001b[0m | \u001b[0m0.1124   \u001b[0m |\n",
      "\tTrain Loss: 0.086| Train Corr:0.041| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:-0.003\n",
      "\tTrain Loss: 0.085| Train Corr:0.082| Val. Loss: 1.622| Val Corr:0.035 | Test. Loss: 1.622| Test Corr:0.017\n",
      "\tTrain Loss: 0.085| Train Corr:0.093| Val. Loss: 1.622| Val Corr:0.039 | Test. Loss: 1.622| Test Corr:0.030\n",
      "\tTrain Loss: 0.085| Train Corr:0.103| Val. Loss: 1.622| Val Corr:0.041 | Test. Loss: 1.622| Test Corr:0.039\n",
      "\tTrain Loss: 0.085| Train Corr:0.112| Val. Loss: 1.622| Val Corr:0.042 | Test. Loss: 1.622| Test Corr:0.045\n",
      "\tTrain Loss: 0.084| Train Corr:0.121| Val. Loss: 1.622| Val Corr:0.044 | Test. Loss: 1.622| Test Corr:0.050\n",
      "\tTrain Loss: 0.084| Train Corr:0.129| Val. Loss: 1.622| Val Corr:0.045 | Test. Loss: 1.622| Test Corr:0.054\n",
      "\tTrain Loss: 0.084| Train Corr:0.137| Val. Loss: 1.622| Val Corr:0.046 | Test. Loss: 1.622| Test Corr:0.058\n",
      "\tTrain Loss: 0.084| Train Corr:0.144| Val. Loss: 1.622| Val Corr:0.047 | Test. Loss: 1.622| Test Corr:0.061\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.04662  \u001b[0m | \u001b[95m19.24    \u001b[0m | \u001b[95m0.2387   \u001b[0m | \u001b[95m201.9    \u001b[0m | \u001b[95m9.916    \u001b[0m | \u001b[95m1.475    \u001b[0m | \u001b[95m0.08211  \u001b[0m | \u001b[95m3.348    \u001b[0m |\n",
      "\tTrain Loss: 0.368| Train Corr:-0.029| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m17.6     \u001b[0m | \u001b[0m0.4273   \u001b[0m | \u001b[0m201.7    \u001b[0m | \u001b[0m8.755    \u001b[0m | \u001b[0m1.311    \u001b[0m | \u001b[0m0.6763   \u001b[0m | \u001b[0m1.98     \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.202| Val. Loss: 1.622| Val Corr:0.021 | Test. Loss: 1.622| Test Corr:-0.010\n",
      "\tTrain Loss: 0.081| Train Corr:0.224| Val. Loss: 1.622| Val Corr:0.026 | Test. Loss: 1.622| Test Corr:0.002\n",
      "\tTrain Loss: 0.081| Train Corr:0.236| Val. Loss: 1.622| Val Corr:0.029 | Test. Loss: 1.622| Test Corr:0.009\n",
      "\tTrain Loss: 0.081| Train Corr:0.244| Val. Loss: 1.622| Val Corr:0.030 | Test. Loss: 1.622| Test Corr:0.015\n",
      "\tTrain Loss: 0.080| Train Corr:0.251| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:0.023\n",
      "\tTrain Loss: 0.080| Train Corr:0.259| Val. Loss: 1.622| Val Corr:0.033 | Test. Loss: 1.622| Test Corr:0.031\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.03254  \u001b[0m | \u001b[0m24.26    \u001b[0m | \u001b[0m0.03065  \u001b[0m | \u001b[0m212.2    \u001b[0m | \u001b[0m6.231    \u001b[0m | \u001b[0m1.787    \u001b[0m | \u001b[0m0.7036   \u001b[0m | \u001b[0m2.53     \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.023| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m18.29    \u001b[0m | \u001b[0m0.7106   \u001b[0m | \u001b[0m406.1    \u001b[0m | \u001b[0m3.796    \u001b[0m | \u001b[0m2.489    \u001b[0m | \u001b[0m0.8691   \u001b[0m | \u001b[0m0.8098   \u001b[0m |\n",
      "\tTrain Loss: 0.368| Train Corr:-0.049| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m31.53    \u001b[0m | \u001b[0m0.5873   \u001b[0m | \u001b[0m264.1    \u001b[0m | \u001b[0m5.38     \u001b[0m | \u001b[0m2.065    \u001b[0m | \u001b[0m0.1762   \u001b[0m | \u001b[0m4.654    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.027| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m14.23    \u001b[0m | \u001b[0m0.1717   \u001b[0m | \u001b[0m322.5    \u001b[0m | \u001b[0m9.13     \u001b[0m | \u001b[0m1.751    \u001b[0m | \u001b[0m0.6978   \u001b[0m | \u001b[0m0.9652   \u001b[0m |\n",
      "\tTrain Loss: 0.087| Train Corr:-0.006| Val. Loss: 1.622| Val Corr:0.009 | Test. Loss: 1.622| Test Corr:-0.052\n",
      "\tTrain Loss: 0.085| Train Corr:0.072| Val. Loss: 1.622| Val Corr:0.021 | Test. Loss: 1.622| Test Corr:-0.027\n",
      "\tTrain Loss: 0.085| Train Corr:0.091| Val. Loss: 1.622| Val Corr:0.030 | Test. Loss: 1.622| Test Corr:-0.008\n",
      "\tTrain Loss: 0.085| Train Corr:0.102| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.006\n",
      "\tTrain Loss: 0.085| Train Corr:0.112| Val. Loss: 1.622| Val Corr:0.040 | Test. Loss: 1.622| Test Corr:0.016\n",
      "\tTrain Loss: 0.085| Train Corr:0.121| Val. Loss: 1.622| Val Corr:0.043 | Test. Loss: 1.622| Test Corr:0.024\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.04309  \u001b[0m | \u001b[0m24.63    \u001b[0m | \u001b[0m0.9934   \u001b[0m | \u001b[0m209.1    \u001b[0m | \u001b[0m6.399    \u001b[0m | \u001b[0m1.97     \u001b[0m | \u001b[0m0.1112   \u001b[0m | \u001b[0m3.422    \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.198| Val. Loss: 1.622| Val Corr:0.018 | Test. Loss: 1.622| Test Corr:-0.018\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.01824  \u001b[0m | \u001b[0m15.34    \u001b[0m | \u001b[0m0.3355   \u001b[0m | \u001b[0m254.0    \u001b[0m | \u001b[0m1.583    \u001b[0m | \u001b[0m2.868    \u001b[0m | \u001b[0m0.2182   \u001b[0m | \u001b[0m0.06621  \u001b[0m |\n",
      "\tTrain Loss: 0.085| Train Corr:0.122| Val. Loss: 1.622| Val Corr:-0.008 | Test. Loss: 1.622| Test Corr:-0.019\n",
      "\tTrain Loss: 0.084| Train Corr:0.144| Val. Loss: 1.622| Val Corr:0.012 | Test. Loss: 1.622| Test Corr:0.016\n",
      "\tTrain Loss: 0.084| Train Corr:0.157| Val. Loss: 1.622| Val Corr:0.020 | Test. Loss: 1.622| Test Corr:0.030\n",
      "\tTrain Loss: 0.083| Train Corr:0.170| Val. Loss: 1.622| Val Corr:0.024 | Test. Loss: 1.622| Test Corr:0.039\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.0239   \u001b[0m | \u001b[0m17.5     \u001b[0m | \u001b[0m0.6499   \u001b[0m | \u001b[0m335.9    \u001b[0m | \u001b[0m4.726    \u001b[0m | \u001b[0m2.255    \u001b[0m | \u001b[0m0.1753   \u001b[0m | \u001b[0m2.583    \u001b[0m |\n",
      "\tTrain Loss: 0.368| Train Corr:-0.024| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m16.12    \u001b[0m | \u001b[0m0.7036   \u001b[0m | \u001b[0m163.9    \u001b[0m | \u001b[0m8.654    \u001b[0m | \u001b[0m1.73     \u001b[0m | \u001b[0m0.9495   \u001b[0m | \u001b[0m1.463    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.028| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m22.9     \u001b[0m | \u001b[0m0.1157   \u001b[0m | \u001b[0m84.33    \u001b[0m | \u001b[0m7.199    \u001b[0m | \u001b[0m2.771    \u001b[0m | \u001b[0m0.7996   \u001b[0m | \u001b[0m4.418    \u001b[0m |\n",
      "\tTrain Loss: 0.368| Train Corr:-0.043| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m28.76    \u001b[0m | \u001b[0m0.178    \u001b[0m | \u001b[0m426.0    \u001b[0m | \u001b[0m6.713    \u001b[0m | \u001b[0m2.786    \u001b[0m | \u001b[0m0.1056   \u001b[0m | \u001b[0m2.349    \u001b[0m |\n",
      "\tTrain Loss: 0.368| Train Corr:-0.049| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m31.99    \u001b[0m | \u001b[0m0.6742   \u001b[0m | \u001b[0m476.2    \u001b[0m | \u001b[0m1.097    \u001b[0m | \u001b[0m2.593    \u001b[0m | \u001b[0m0.7139   \u001b[0m | \u001b[0m1.511    \u001b[0m |\n",
      "\tTrain Loss: 0.368| Train Corr:-0.024| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m16.3     \u001b[0m | \u001b[0m0.1286   \u001b[0m | \u001b[0m436.2    \u001b[0m | \u001b[0m5.09     \u001b[0m | \u001b[0m1.447    \u001b[0m | \u001b[0m0.2003   \u001b[0m | \u001b[0m4.429    \u001b[0m |\n",
      "\tTrain Loss: 0.302| Train Corr:0.022| Val. Loss: 1.622| Val Corr:-0.029 | Test. Loss: 1.622| Test Corr:-0.180\n",
      "\tTrain Loss: 0.306| Train Corr:-0.131| Val. Loss: 1.622| Val Corr:-0.030 | Test. Loss: 1.622| Test Corr:-0.176\n",
      "\tTrain Loss: 0.306| Train Corr:-0.113| Val. Loss: 1.622| Val Corr:-0.010 | Test. Loss: 1.622| Test Corr:-0.098\n",
      "\tTrain Loss: 0.305| Train Corr:0.011| Val. Loss: 1.622| Val Corr:0.007 | Test. Loss: 1.622| Test Corr:-0.090\n",
      "\tTrain Loss: 0.306| Train Corr:0.015| Val. Loss: 1.622| Val Corr:0.007 | Test. Loss: 1.622| Test Corr:-0.090\n",
      "\tTrain Loss: 0.306| Train Corr:0.015| Val. Loss: 1.622| Val Corr:0.007 | Test. Loss: 1.622| Test Corr:-0.090\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.006884 \u001b[0m | \u001b[0m13.19    \u001b[0m | \u001b[0m0.9503   \u001b[0m | \u001b[0m385.7    \u001b[0m | \u001b[0m6.015    \u001b[0m | \u001b[0m1.943    \u001b[0m | \u001b[0m0.01933  \u001b[0m | \u001b[0m2.263    \u001b[0m |\n",
      "\tTrain Loss: 0.083| Train Corr:0.164| Val. Loss: 1.622| Val Corr:0.015 | Test. Loss: 1.622| Test Corr:0.014\n",
      "\tTrain Loss: 0.082| Train Corr:0.193| Val. Loss: 1.622| Val Corr:0.024 | Test. Loss: 1.622| Test Corr:0.027\n",
      "\tTrain Loss: 0.082| Train Corr:0.209| Val. Loss: 1.622| Val Corr:0.028 | Test. Loss: 1.622| Test Corr:0.034\n",
      "\tTrain Loss: 0.081| Train Corr:0.221| Val. Loss: 1.622| Val Corr:0.030 | Test. Loss: 1.622| Test Corr:0.041\n",
      "\tTrain Loss: 0.081| Train Corr:0.230| Val. Loss: 1.622| Val Corr:0.032 | Test. Loss: 1.622| Test Corr:0.047\n",
      "\tTrain Loss: 0.081| Train Corr:0.237| Val. Loss: 1.622| Val Corr:0.034 | Test. Loss: 1.622| Test Corr:0.054\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.03445  \u001b[0m | \u001b[0m23.83    \u001b[0m | \u001b[0m0.3219   \u001b[0m | \u001b[0m212.7    \u001b[0m | \u001b[0m6.328    \u001b[0m | \u001b[0m1.356    \u001b[0m | \u001b[0m0.409    \u001b[0m | \u001b[0m3.068    \u001b[0m |\n",
      "\tTrain Loss: 0.086| Train Corr:0.076| Val. Loss: 1.622| Val Corr:0.030 | Test. Loss: 1.622| Test Corr:-0.004\n",
      "\tTrain Loss: 0.085| Train Corr:0.117| Val. Loss: 1.622| Val Corr:0.038 | Test. Loss: 1.622| Test Corr:0.015\n",
      "\tTrain Loss: 0.085| Train Corr:0.127| Val. Loss: 1.622| Val Corr:0.043 | Test. Loss: 1.622| Test Corr:0.027\n",
      "\tTrain Loss: 0.085| Train Corr:0.137| Val. Loss: 1.622| Val Corr:0.046 | Test. Loss: 1.622| Test Corr:0.035\n",
      "\tTrain Loss: 0.084| Train Corr:0.147| Val. Loss: 1.622| Val Corr:0.048 | Test. Loss: 1.622| Test Corr:0.041\n",
      "\tTrain Loss: 0.084| Train Corr:0.156| Val. Loss: 1.622| Val Corr:0.050 | Test. Loss: 1.622| Test Corr:0.045\n",
      "| \u001b[95m18       \u001b[0m | \u001b[95m0.0499   \u001b[0m | \u001b[95m26.54    \u001b[0m | \u001b[95m0.3037   \u001b[0m | \u001b[95m209.6    \u001b[0m | \u001b[95m6.998    \u001b[0m | \u001b[95m2.315    \u001b[0m | \u001b[95m0.1768   \u001b[0m | \u001b[95m3.337    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.029| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m21.51    \u001b[0m | \u001b[0m0.04305  \u001b[0m | \u001b[0m202.4    \u001b[0m | \u001b[0m9.361    \u001b[0m | \u001b[0m1.283    \u001b[0m | \u001b[0m0.2911   \u001b[0m | \u001b[0m4.533    \u001b[0m |\n",
      "\tTrain Loss: 0.366| Train Corr:-0.035| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m25.48    \u001b[0m | \u001b[0m0.5187   \u001b[0m | \u001b[0m210.5    \u001b[0m | \u001b[0m6.811    \u001b[0m | \u001b[0m1.856    \u001b[0m | \u001b[0m0.09345  \u001b[0m | \u001b[0m3.522    \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.215| Val. Loss: 1.622| Val Corr:0.022 | Test. Loss: 1.622| Test Corr:-0.012\n",
      "\tTrain Loss: 0.081| Train Corr:0.238| Val. Loss: 1.622| Val Corr:0.027 | Test. Loss: 1.622| Test Corr:0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.080| Train Corr:0.251| Val. Loss: 1.622| Val Corr:0.029 | Test. Loss: 1.622| Test Corr:0.009\n",
      "\tTrain Loss: 0.080| Train Corr:0.260| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:0.018\n",
      "\tTrain Loss: 0.079| Train Corr:0.270| Val. Loss: 1.622| Val Corr:0.032 | Test. Loss: 1.622| Test Corr:0.028\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.03217  \u001b[0m | \u001b[0m24.45    \u001b[0m | \u001b[0m0.1604   \u001b[0m | \u001b[0m211.7    \u001b[0m | \u001b[0m5.973    \u001b[0m | \u001b[0m1.785    \u001b[0m | \u001b[0m0.9761   \u001b[0m | \u001b[0m2.974    \u001b[0m |\n",
      "\tTrain Loss: 0.329| Train Corr:0.001| Val. Loss: 1.622| Val Corr:-0.042 | Test. Loss: 1.622| Test Corr:0.030\n",
      "\tTrain Loss: 0.304| Train Corr:0.035| Val. Loss: 1.622| Val Corr:0.040 | Test. Loss: 1.622| Test Corr:-0.043\n",
      "\tTrain Loss: 0.306| Train Corr:0.022| Val. Loss: 1.622| Val Corr:0.040 | Test. Loss: 1.622| Test Corr:-0.043\n",
      "\tTrain Loss: 0.306| Train Corr:0.022| Val. Loss: 1.622| Val Corr:0.040 | Test. Loss: 1.622| Test Corr:-0.043\n",
      "\tTrain Loss: 0.306| Train Corr:0.022| Val. Loss: 1.622| Val Corr:0.040 | Test. Loss: 1.622| Test Corr:-0.043\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.04035  \u001b[0m | \u001b[0m24.81    \u001b[0m | \u001b[0m0.9851   \u001b[0m | \u001b[0m208.7    \u001b[0m | \u001b[0m5.925    \u001b[0m | \u001b[0m1.789    \u001b[0m | \u001b[0m0.4916   \u001b[0m | \u001b[0m4.248    \u001b[0m |\n",
      "\tTrain Loss: 0.084| Train Corr:0.130| Val. Loss: 1.622| Val Corr:0.023 | Test. Loss: 1.622| Test Corr:-0.021\n",
      "\tTrain Loss: 0.084| Train Corr:0.156| Val. Loss: 1.622| Val Corr:0.032 | Test. Loss: 1.622| Test Corr:-0.000\n",
      "\tTrain Loss: 0.083| Train Corr:0.169| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.009\n",
      "\tTrain Loss: 0.083| Train Corr:0.182| Val. Loss: 1.622| Val Corr:0.038 | Test. Loss: 1.622| Test Corr:0.015\n",
      "\tTrain Loss: 0.083| Train Corr:0.194| Val. Loss: 1.622| Val Corr:0.040 | Test. Loss: 1.622| Test Corr:0.020\n",
      "\tTrain Loss: 0.082| Train Corr:0.205| Val. Loss: 1.622| Val Corr:0.041 | Test. Loss: 1.622| Test Corr:0.024\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.04142  \u001b[0m | \u001b[0m24.49    \u001b[0m | \u001b[0m0.1507   \u001b[0m | \u001b[0m209.0    \u001b[0m | \u001b[0m6.668    \u001b[0m | \u001b[0m1.265    \u001b[0m | \u001b[0m0.2767   \u001b[0m | \u001b[0m2.736    \u001b[0m |\n",
      "\tTrain Loss: 0.083| Train Corr:0.161| Val. Loss: 1.622| Val Corr:0.019 | Test. Loss: 1.622| Test Corr:-0.003\n",
      "\tTrain Loss: 0.082| Train Corr:0.191| Val. Loss: 1.622| Val Corr:0.025 | Test. Loss: 1.622| Test Corr:0.016\n",
      "\tTrain Loss: 0.082| Train Corr:0.205| Val. Loss: 1.622| Val Corr:0.028 | Test. Loss: 1.622| Test Corr:0.025\n",
      "\tTrain Loss: 0.082| Train Corr:0.217| Val. Loss: 1.622| Val Corr:0.029 | Test. Loss: 1.622| Test Corr:0.033\n",
      "\tTrain Loss: 0.081| Train Corr:0.226| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:0.039\n",
      "\tTrain Loss: 0.081| Train Corr:0.234| Val. Loss: 1.622| Val Corr:0.032 | Test. Loss: 1.622| Test Corr:0.045\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.03196  \u001b[0m | \u001b[0m23.95    \u001b[0m | \u001b[0m0.6879   \u001b[0m | \u001b[0m208.2    \u001b[0m | \u001b[0m6.302    \u001b[0m | \u001b[0m2.237    \u001b[0m | \u001b[0m0.3845   \u001b[0m | \u001b[0m3.127    \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.190| Val. Loss: 1.622| Val Corr:0.010 | Test. Loss: 1.622| Test Corr:0.004\n",
      "\tTrain Loss: 0.081| Train Corr:0.227| Val. Loss: 1.622| Val Corr:0.016 | Test. Loss: 1.622| Test Corr:0.018\n",
      "\tTrain Loss: 0.081| Train Corr:0.241| Val. Loss: 1.622| Val Corr:0.018 | Test. Loss: 1.622| Test Corr:0.028\n",
      "\tTrain Loss: 0.080| Train Corr:0.250| Val. Loss: 1.622| Val Corr:0.020 | Test. Loss: 1.622| Test Corr:0.037\n",
      "\tTrain Loss: 0.080| Train Corr:0.257| Val. Loss: 1.622| Val Corr:0.022 | Test. Loss: 1.622| Test Corr:0.047\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.02248  \u001b[0m | \u001b[0m23.53    \u001b[0m | \u001b[0m0.9521   \u001b[0m | \u001b[0m210.1    \u001b[0m | \u001b[0m5.654    \u001b[0m | \u001b[0m1.649    \u001b[0m | \u001b[0m0.7199   \u001b[0m | \u001b[0m2.803    \u001b[0m |\n",
      "\tTrain Loss: 0.369| Train Corr:-0.037| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m26.29    \u001b[0m | \u001b[0m0.6899   \u001b[0m | \u001b[0m208.6    \u001b[0m | \u001b[0m6.258    \u001b[0m | \u001b[0m1.617    \u001b[0m | \u001b[0m0.8757   \u001b[0m | \u001b[0m3.93     \u001b[0m |\n",
      "\tTrain Loss: 0.366| Train Corr:-0.035| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m25.12    \u001b[0m | \u001b[0m0.8987   \u001b[0m | \u001b[0m209.1    \u001b[0m | \u001b[0m6.601    \u001b[0m | \u001b[0m1.093    \u001b[0m | \u001b[0m0.1026   \u001b[0m | \u001b[0m4.949    \u001b[0m |\n",
      "\tTrain Loss: 0.366| Train Corr:-0.035| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m25.12    \u001b[0m | \u001b[0m0.05416  \u001b[0m | \u001b[0m209.0    \u001b[0m | \u001b[0m6.339    \u001b[0m | \u001b[0m2.699    \u001b[0m | \u001b[0m0.3401   \u001b[0m | \u001b[0m4.98     \u001b[0m |\n",
      "\tTrain Loss: 0.366| Train Corr:-0.035| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m25.03    \u001b[0m | \u001b[0m0.445    \u001b[0m | \u001b[0m208.4    \u001b[0m | \u001b[0m4.987    \u001b[0m | \u001b[0m1.661    \u001b[0m | \u001b[0m0.128    \u001b[0m | \u001b[0m2.37     \u001b[0m |\n",
      "\tTrain Loss: 0.369| Train Corr:-0.037| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.369| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m26.21    \u001b[0m | \u001b[0m0.03095  \u001b[0m | \u001b[0m209.6    \u001b[0m | \u001b[0m5.964    \u001b[0m | \u001b[0m2.925    \u001b[0m | \u001b[0m0.4326   \u001b[0m | \u001b[0m2.218    \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.219| Val. Loss: 1.622| Val Corr:0.021 | Test. Loss: 1.622| Test Corr:-0.011\n",
      "\tTrain Loss: 0.081| Train Corr:0.241| Val. Loss: 1.622| Val Corr:0.026 | Test. Loss: 1.622| Test Corr:0.000\n",
      "\tTrain Loss: 0.080| Train Corr:0.252| Val. Loss: 1.622| Val Corr:0.029 | Test. Loss: 1.622| Test Corr:0.009\n",
      "\tTrain Loss: 0.080| Train Corr:0.260| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:0.018\n",
      "\tTrain Loss: 0.079| Train Corr:0.270| Val. Loss: 1.622| Val Corr:0.033 | Test. Loss: 1.622| Test Corr:0.029\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.03289  \u001b[0m | \u001b[0m24.71    \u001b[0m | \u001b[0m0.8086   \u001b[0m | \u001b[0m208.8    \u001b[0m | \u001b[0m5.696    \u001b[0m | \u001b[0m2.624    \u001b[0m | \u001b[0m0.9908   \u001b[0m | \u001b[0m3.075    \u001b[0m |\n",
      "\tTrain Loss: 0.366| Train Corr:-0.041| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m27.03    \u001b[0m | \u001b[0m0.3931   \u001b[0m | \u001b[0m209.9    \u001b[0m | \u001b[0m6.925    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m0.6059   \u001b[0m | \u001b[0m3.651    \u001b[0m |\n",
      "\tTrain Loss: 0.357| Train Corr:-0.006| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m22.89    \u001b[0m | \u001b[0m0.9281   \u001b[0m | \u001b[0m287.3    \u001b[0m | \u001b[0m7.374    \u001b[0m | \u001b[0m1.467    \u001b[0m | \u001b[0m0.7038   \u001b[0m | \u001b[0m4.482    \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.211| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:-0.006\n",
      "\tTrain Loss: 0.081| Train Corr:0.236| Val. Loss: 1.622| Val Corr:0.035 | Test. Loss: 1.622| Test Corr:0.005\n",
      "\tTrain Loss: 0.080| Train Corr:0.249| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.012\n",
      "\tTrain Loss: 0.080| Train Corr:0.257| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.020\n",
      "\tTrain Loss: 0.080| Train Corr:0.267| Val. Loss: 1.622| Val Corr:0.037 | Test. Loss: 1.622| Test Corr:0.028\n",
      "\tTrain Loss: 0.079| Train Corr:0.280| Val. Loss: 1.622| Val Corr:0.038 | Test. Loss: 1.622| Test Corr:0.039\n",
      "\tTrain Loss: 0.078| Train Corr:0.295| Val. Loss: 1.622| Val Corr:0.041 | Test. Loss: 1.622| Test Corr:0.051\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.04052  \u001b[0m | \u001b[0m24.68    \u001b[0m | \u001b[0m0.7575   \u001b[0m | \u001b[0m209.5    \u001b[0m | \u001b[0m7.212    \u001b[0m | \u001b[0m2.952    \u001b[0m | \u001b[0m0.8983   \u001b[0m | \u001b[0m3.338    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.029| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m21.39    \u001b[0m | \u001b[0m0.1367   \u001b[0m | \u001b[0m370.1    \u001b[0m | \u001b[0m2.562    \u001b[0m | \u001b[0m1.524    \u001b[0m | \u001b[0m0.8533   \u001b[0m | \u001b[0m1.418    \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.218| Val. Loss: 1.622| Val Corr:0.018 | Test. Loss: 1.622| Test Corr:-0.017\n",
      "\tTrain Loss: 0.081| Train Corr:0.230| Val. Loss: 1.622| Val Corr:0.023 | Test. Loss: 1.622| Test Corr:-0.005\n",
      "\tTrain Loss: 0.081| Train Corr:0.240| Val. Loss: 1.622| Val Corr:0.025 | Test. Loss: 1.622| Test Corr:0.001\n",
      "\tTrain Loss: 0.080| Train Corr:0.247| Val. Loss: 1.622| Val Corr:0.027 | Test. Loss: 1.622| Test Corr:0.007\n",
      "\tTrain Loss: 0.080| Train Corr:0.253| Val. Loss: 1.622| Val Corr:0.028 | Test. Loss: 1.622| Test Corr:0.012\n",
      "\tTrain Loss: 0.080| Train Corr:0.259| Val. Loss: 1.622| Val Corr:0.029 | Test. Loss: 1.622| Test Corr:0.018\n",
      "\tTrain Loss: 0.080| Train Corr:0.266| Val. Loss: 1.622| Val Corr:0.030 | Test. Loss: 1.622| Test Corr:0.025\n",
      "\tTrain Loss: 0.079| Train Corr:0.275| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:0.033\n",
      "\tTrain Loss: 0.079| Train Corr:0.285| Val. Loss: 1.622| Val Corr:0.033 | Test. Loss: 1.622| Test Corr:0.043\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.03319  \u001b[0m | \u001b[0m15.4     \u001b[0m | \u001b[0m0.0497   \u001b[0m | \u001b[0m89.63    \u001b[0m | \u001b[0m9.549    \u001b[0m | \u001b[0m2.686    \u001b[0m | \u001b[0m0.326    \u001b[0m | \u001b[0m0.4854   \u001b[0m |\n",
      "\tTrain Loss: 0.084| Train Corr:0.126| Val. Loss: 1.622| Val Corr:0.019 | Test. Loss: 1.622| Test Corr:-0.022\n",
      "\tTrain Loss: 0.084| Train Corr:0.160| Val. Loss: 1.622| Val Corr:0.029 | Test. Loss: 1.622| Test Corr:-0.002\n",
      "\tTrain Loss: 0.083| Train Corr:0.172| Val. Loss: 1.622| Val Corr:0.033 | Test. Loss: 1.622| Test Corr:0.008\n",
      "\tTrain Loss: 0.083| Train Corr:0.184| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.015\n",
      "\tTrain Loss: 0.082| Train Corr:0.195| Val. Loss: 1.622| Val Corr:0.038 | Test. Loss: 1.622| Test Corr:0.020\n",
      "\tTrain Loss: 0.082| Train Corr:0.206| Val. Loss: 1.622| Val Corr:0.040 | Test. Loss: 1.622| Test Corr:0.025\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.04033  \u001b[0m | \u001b[0m24.39    \u001b[0m | \u001b[0m0.1249   \u001b[0m | \u001b[0m208.4    \u001b[0m | \u001b[0m6.228    \u001b[0m | \u001b[0m2.473    \u001b[0m | \u001b[0m0.2778   \u001b[0m | \u001b[0m2.814    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.031| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m24.08    \u001b[0m | \u001b[0m0.776    \u001b[0m | \u001b[0m209.5    \u001b[0m | \u001b[0m7.681    \u001b[0m | \u001b[0m2.262    \u001b[0m | \u001b[0m0.5739   \u001b[0m | \u001b[0m2.186    \u001b[0m |\n",
      "\tTrain Loss: 0.368| Train Corr:-0.043| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m28.99    \u001b[0m | \u001b[0m0.3882   \u001b[0m | \u001b[0m68.14    \u001b[0m | \u001b[0m6.313    \u001b[0m | \u001b[0m1.791    \u001b[0m | \u001b[0m0.4141   \u001b[0m | \u001b[0m4.688    \u001b[0m |\n",
      "\tTrain Loss: 0.083| Train Corr:0.175| Val. Loss: 1.622| Val Corr:0.015 | Test. Loss: 1.622| Test Corr:-0.005\n",
      "\tTrain Loss: 0.082| Train Corr:0.203| Val. Loss: 1.622| Val Corr:0.024 | Test. Loss: 1.622| Test Corr:0.010\n",
      "\tTrain Loss: 0.082| Train Corr:0.218| Val. Loss: 1.622| Val Corr:0.028 | Test. Loss: 1.622| Test Corr:0.017\n",
      "\tTrain Loss: 0.081| Train Corr:0.230| Val. Loss: 1.622| Val Corr:0.030 | Test. Loss: 1.622| Test Corr:0.024\n",
      "\tTrain Loss: 0.081| Train Corr:0.238| Val. Loss: 1.622| Val Corr:0.032 | Test. Loss: 1.622| Test Corr:0.030\n",
      "\tTrain Loss: 0.080| Train Corr:0.245| Val. Loss: 1.622| Val Corr:0.034 | Test. Loss: 1.622| Test Corr:0.037\n",
      "\tTrain Loss: 0.080| Train Corr:0.252| Val. Loss: 1.622| Val Corr:0.035 | Test. Loss: 1.622| Test Corr:0.044\n",
      "\tTrain Loss: 0.080| Train Corr:0.260| Val. Loss: 1.622| Val Corr:0.037 | Test. Loss: 1.622| Test Corr:0.052\n",
      "\tTrain Loss: 0.079| Train Corr:0.268| Val. Loss: 1.622| Val Corr:0.039 | Test. Loss: 1.622| Test Corr:0.061\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.03939  \u001b[0m | \u001b[0m19.42    \u001b[0m | \u001b[0m0.3326   \u001b[0m | \u001b[0m202.0    \u001b[0m | \u001b[0m9.7      \u001b[0m | \u001b[0m1.623    \u001b[0m | \u001b[0m0.3933   \u001b[0m | \u001b[0m3.083    \u001b[0m |\n",
      "\tTrain Loss: 0.081| Train Corr:0.220| Val. Loss: 1.622| Val Corr:0.014 | Test. Loss: 1.622| Test Corr:0.011\n",
      "\tTrain Loss: 0.081| Train Corr:0.234| Val. Loss: 1.622| Val Corr:0.017 | Test. Loss: 1.622| Test Corr:0.018\n",
      "\tTrain Loss: 0.080| Train Corr:0.245| Val. Loss: 1.622| Val Corr:0.018 | Test. Loss: 1.622| Test Corr:0.024\n",
      "\tTrain Loss: 0.080| Train Corr:0.252| Val. Loss: 1.622| Val Corr:0.019 | Test. Loss: 1.622| Test Corr:0.030\n",
      "\tTrain Loss: 0.080| Train Corr:0.257| Val. Loss: 1.622| Val Corr:0.020 | Test. Loss: 1.622| Test Corr:0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.080| Train Corr:0.263| Val. Loss: 1.622| Val Corr:0.021 | Test. Loss: 1.622| Test Corr:0.043\n",
      "\tTrain Loss: 0.079| Train Corr:0.270| Val. Loss: 1.622| Val Corr:0.022 | Test. Loss: 1.622| Test Corr:0.051\n",
      "\tTrain Loss: 0.079| Train Corr:0.278| Val. Loss: 1.622| Val Corr:0.023 | Test. Loss: 1.622| Test Corr:0.061\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.02332  \u001b[0m | \u001b[0m23.77    \u001b[0m | \u001b[0m0.6532   \u001b[0m | \u001b[0m247.3    \u001b[0m | \u001b[0m8.49     \u001b[0m | \u001b[0m1.232    \u001b[0m | \u001b[0m0.5837   \u001b[0m | \u001b[0m0.05161  \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.031| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m24.1     \u001b[0m | \u001b[0m0.1189   \u001b[0m | \u001b[0m208.5    \u001b[0m | \u001b[0m6.752    \u001b[0m | \u001b[0m2.149    \u001b[0m | \u001b[0m0.6931   \u001b[0m | \u001b[0m3.982    \u001b[0m |\n",
      "\tTrain Loss: 0.368| Train Corr:-0.043| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m28.48    \u001b[0m | \u001b[0m0.8798   \u001b[0m | \u001b[0m101.4    \u001b[0m | \u001b[0m5.002    \u001b[0m | \u001b[0m2.556    \u001b[0m | \u001b[0m0.8566   \u001b[0m | \u001b[0m4.894    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.016| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.34    \u001b[0m | \u001b[0m0.04433  \u001b[0m | \u001b[0m284.0    \u001b[0m | \u001b[0m2.913    \u001b[0m | \u001b[0m2.5      \u001b[0m | \u001b[0m0.2806   \u001b[0m | \u001b[0m0.728    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.022| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.367| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m14.01    \u001b[0m | \u001b[0m0.8943   \u001b[0m | \u001b[0m358.2    \u001b[0m | \u001b[0m9.987    \u001b[0m | \u001b[0m2.533    \u001b[0m | \u001b[0m0.6257   \u001b[0m | \u001b[0m2.344    \u001b[0m |\n",
      "\tTrain Loss: 0.083| Train Corr:0.175| Val. Loss: 1.622| Val Corr:0.023 | Test. Loss: 1.622| Test Corr:-0.019\n",
      "\tTrain Loss: 0.082| Train Corr:0.203| Val. Loss: 1.622| Val Corr:0.030 | Test. Loss: 1.622| Test Corr:-0.000\n",
      "\tTrain Loss: 0.082| Train Corr:0.219| Val. Loss: 1.622| Val Corr:0.033 | Test. Loss: 1.622| Test Corr:0.009\n",
      "\tTrain Loss: 0.081| Train Corr:0.231| Val. Loss: 1.622| Val Corr:0.034 | Test. Loss: 1.622| Test Corr:0.016\n",
      "\tTrain Loss: 0.081| Train Corr:0.240| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.022\n",
      "\tTrain Loss: 0.080| Train Corr:0.247| Val. Loss: 1.622| Val Corr:0.037 | Test. Loss: 1.622| Test Corr:0.029\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.03695  \u001b[0m | \u001b[0m24.58    \u001b[0m | \u001b[0m0.08745  \u001b[0m | \u001b[0m214.0    \u001b[0m | \u001b[0m6.573    \u001b[0m | \u001b[0m2.394    \u001b[0m | \u001b[0m0.5256   \u001b[0m | \u001b[0m2.724    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.031| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m24.3     \u001b[0m | \u001b[0m0.1276   \u001b[0m | \u001b[0m210.4    \u001b[0m | \u001b[0m6.213    \u001b[0m | \u001b[0m1.695    \u001b[0m | \u001b[0m0.5743   \u001b[0m | \u001b[0m2.242    \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.192| Val. Loss: 1.622| Val Corr:0.020 | Test. Loss: 1.622| Test Corr:0.015\n",
      "\tTrain Loss: 0.082| Train Corr:0.215| Val. Loss: 1.622| Val Corr:0.025 | Test. Loss: 1.622| Test Corr:0.026\n",
      "\tTrain Loss: 0.081| Train Corr:0.230| Val. Loss: 1.622| Val Corr:0.027 | Test. Loss: 1.622| Test Corr:0.035\n",
      "\tTrain Loss: 0.081| Train Corr:0.241| Val. Loss: 1.622| Val Corr:0.028 | Test. Loss: 1.622| Test Corr:0.044\n",
      "\tTrain Loss: 0.080| Train Corr:0.250| Val. Loss: 1.622| Val Corr:0.030 | Test. Loss: 1.622| Test Corr:0.055\n",
      "\tTrain Loss: 0.080| Train Corr:0.259| Val. Loss: 1.622| Val Corr:0.032 | Test. Loss: 1.622| Test Corr:0.067\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.03173  \u001b[0m | \u001b[0m23.83    \u001b[0m | \u001b[0m0.1282   \u001b[0m | \u001b[0m207.7    \u001b[0m | \u001b[0m6.858    \u001b[0m | \u001b[0m2.698    \u001b[0m | \u001b[0m0.584    \u001b[0m | \u001b[0m3.022    \u001b[0m |\n",
      "\tTrain Loss: 0.082| Train Corr:0.202| Val. Loss: 1.622| Val Corr:0.028 | Test. Loss: 1.622| Test Corr:0.004\n",
      "\tTrain Loss: 0.081| Train Corr:0.225| Val. Loss: 1.622| Val Corr:0.032 | Test. Loss: 1.622| Test Corr:0.018\n",
      "\tTrain Loss: 0.081| Train Corr:0.237| Val. Loss: 1.622| Val Corr:0.034 | Test. Loss: 1.622| Test Corr:0.027\n",
      "\tTrain Loss: 0.080| Train Corr:0.245| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.037\n",
      "\tTrain Loss: 0.080| Train Corr:0.253| Val. Loss: 1.622| Val Corr:0.038 | Test. Loss: 1.622| Test Corr:0.048\n",
      "\tTrain Loss: 0.080| Train Corr:0.262| Val. Loss: 1.622| Val Corr:0.041 | Test. Loss: 1.622| Test Corr:0.060\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.04135  \u001b[0m | \u001b[0m25.11    \u001b[0m | \u001b[0m0.06106  \u001b[0m | \u001b[0m208.9    \u001b[0m | \u001b[0m6.112    \u001b[0m | \u001b[0m2.287    \u001b[0m | \u001b[0m0.7911   \u001b[0m | \u001b[0m2.582    \u001b[0m |\n",
      "\tTrain Loss: 0.083| Train Corr:0.174| Val. Loss: 1.622| Val Corr:0.021 | Test. Loss: 1.622| Test Corr:-0.010\n",
      "\tTrain Loss: 0.082| Train Corr:0.202| Val. Loss: 1.622| Val Corr:0.027 | Test. Loss: 1.622| Test Corr:0.005\n",
      "\tTrain Loss: 0.082| Train Corr:0.217| Val. Loss: 1.622| Val Corr:0.031 | Test. Loss: 1.622| Test Corr:0.013\n",
      "\tTrain Loss: 0.081| Train Corr:0.227| Val. Loss: 1.622| Val Corr:0.033 | Test. Loss: 1.622| Test Corr:0.018\n",
      "\tTrain Loss: 0.081| Train Corr:0.235| Val. Loss: 1.622| Val Corr:0.035 | Test. Loss: 1.622| Test Corr:0.023\n",
      "\tTrain Loss: 0.081| Train Corr:0.241| Val. Loss: 1.622| Val Corr:0.036 | Test. Loss: 1.622| Test Corr:0.029\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.03628  \u001b[0m | \u001b[0m24.76    \u001b[0m | \u001b[0m0.5253   \u001b[0m | \u001b[0m208.9    \u001b[0m | \u001b[0m6.428    \u001b[0m | \u001b[0m2.061    \u001b[0m | \u001b[0m0.4757   \u001b[0m | \u001b[0m2.925    \u001b[0m |\n",
      "\tTrain Loss: 0.084| Train Corr:0.144| Val. Loss: 1.622| Val Corr:0.023 | Test. Loss: 1.622| Test Corr:-0.010\n",
      "\tTrain Loss: 0.083| Train Corr:0.179| Val. Loss: 1.622| Val Corr:0.029 | Test. Loss: 1.622| Test Corr:0.004\n",
      "\tTrain Loss: 0.083| Train Corr:0.194| Val. Loss: 1.622| Val Corr:0.032 | Test. Loss: 1.622| Test Corr:0.012\n",
      "\tTrain Loss: 0.082| Train Corr:0.206| Val. Loss: 1.622| Val Corr:0.035 | Test. Loss: 1.622| Test Corr:0.017\n",
      "\tTrain Loss: 0.082| Train Corr:0.216| Val. Loss: 1.622| Val Corr:0.037 | Test. Loss: 1.622| Test Corr:0.022\n",
      "\tTrain Loss: 0.081| Train Corr:0.224| Val. Loss: 1.622| Val Corr:0.038 | Test. Loss: 1.622| Test Corr:0.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.081| Train Corr:0.231| Val. Loss: 1.622| Val Corr:0.040 | Test. Loss: 1.622| Test Corr:0.032\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.03975  \u001b[0m | \u001b[0m24.06    \u001b[0m | \u001b[0m0.7264   \u001b[0m | \u001b[0m208.1    \u001b[0m | \u001b[0m7.422    \u001b[0m | \u001b[0m2.793    \u001b[0m | \u001b[0m0.3391   \u001b[0m | \u001b[0m3.154    \u001b[0m |\n",
      "\tTrain Loss: 0.367| Train Corr:-0.031| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "\tTrain Loss: 0.368| Train Corr:nan| Val. Loss: 1.622| Val Corr:nan | Test. Loss: 1.622| Test Corr:nan\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m24.74    \u001b[0m | \u001b[0m0.7854   \u001b[0m | \u001b[0m208.1    \u001b[0m | \u001b[0m7.835    \u001b[0m | \u001b[0m2.867    \u001b[0m | \u001b[0m0.2259   \u001b[0m | \u001b[0m2.209    \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "nn_bo.maximize(init_points=2, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "5bf58eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 5.5184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "model.eval()  # set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for k, (test_sentence1, test_sentence2, test_score) in enumerate(train_dataloader):\n",
    "        test_sentence1_tensor = test_sentence1\n",
    "        test_sentence2_tensor = test_sentence2\n",
    "        test_score_tensor = torch.tensor(test_score, dtype=torch.float)/5.0\n",
    "        test_output = model(test_sentence1_tensor, test_sentence2_tensor)\n",
    "        test_predictions.extend(test_output.tolist())\n",
    "        test_labels.extend(test_score)\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "test_mse = mean_squared_error(test_labels, test_predictions)\n",
    "print('Test MSE: {:.4f}'.format(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "5c412819",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pearson_corr(test_labels, test_predictions.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "d6076e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Define the data\n",
    "epochs = range(1, 11)\n",
    "#train_losses = train_loss\n",
    "#val_losses = val_loss\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Validation Loss for cross lingual en-es')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19459604",
   "metadata": {},
   "source": [
    "<p> it appears that the training loss and validation loss are fluctuating and not decreasing consistently over the 10 epochs. This suggests that the model may not have converged to the optimal solution yet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "71c18934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/word_dict_en_v1.pickle', 'wb') as handle:\n",
    "    pickle.dump(vocab_dict_en, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "ae440428",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_dict_es_v1.pickle', 'wb') as handle:\n",
    "    pickle.dump(vocab_dict_es, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "32a6afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../data/cross_siamese_model_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182952df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
