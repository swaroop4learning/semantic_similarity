{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adityaraghuvanshi999/project1-bilstm?scriptVersionId=126970685\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Figure Settings\nimport ipywidgets as widgets\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Helper functions\ndef cosine_similarity(vec_a, vec_b):\n  \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n  return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n\n\ndef getSimilarity(word1, word2):\n  v1 = ft_en_vectors.get_word_vector(word1)\n  v2 = ft_en_vectors.get_word_vector(word2)\n  return cosine_similarity(v1, v2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# For DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  \"\"\"\n  Function that controls randomness.\n  NumPy and random modules must be imported.\n\n  Args:\n    seed : Integer\n      A non-negative integer that defines the random state. Default is `None`.\n    seed_torch : Boolean\n      If `True` sets the random seed for pytorch tensors, so pytorch module\n      must be imported. Default is `True`.\n\n  Returns:\n    Nothing.\n  \"\"\"\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  \"\"\"\n  DataLoader will reseed workers following randomness in\n  multi-process data loading algorithm.\n\n  Args:\n    worker_id: integer\n      ID of subprocess to seed. 0 means that\n      the data will be loaded in the main process\n      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n\n  Returns:\n    Nothing\n  \"\"\"\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Set device (GPU or CPU). Execute `set_device()`\n\n# Inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  \"\"\"\n  Set the device. CUDA if available, CPU otherwise\n\n  Args:\n    None\n\n  Returns:\n    Nothing\n  \"\"\"\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = set_device()\nSEED = 2021\nset_seed(seed=SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # @markdown ### Download FastText English Embeddings of dimension 100\n# # @markdown This will take 1-2 minutes to run\n\n# import os, zipfile, requests\n\n# url = \"https://osf.io/2frqg/download\"\n# fname = \"cc.en.100.bin.gz\"\n\n# print('Downloading Started...')\n# # Downloading the file by sending the request to the URL\n# r = requests.get(url, stream=True)\n\n# # Writing the file to the local file system\n# with open(fname, 'wb') as f:\n#   f.write(r.content)\n# print('Downloading Completed.')\n\n# # opening the zip file in READ mode\n# with zipfile.ZipFile(fname, 'r') as zipObj:\n#   # extracting all the files\n#   print('Extracting all the files now...')\n#   zipObj.extractall()\n#   print('Done!')\n#   os.remove(fname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load 100 dimension FastText Vectors using FastText library\n# ft_en_vectors = fasttext.load_model('cc.en.100.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # @markdown ### Download FastText French Embeddings of dimension 100\n\n# # @markdown **Note:** This cell might take 2-4 minutes to run\n\n# import os, zipfile, requests\n\n# url = \"https://osf.io/rqadk/download\"\n# fname = \"cc.es.100.bin.gz\"\n\n# print('Downloading Started...')\n# # Downloading the file by sending the request to the URL\n# r = requests.get(url, stream=True)\n\n# # Writing the file to the local file system\n# with open(fname, 'wb') as f:\n#   f.write(r.content)\n# print('Downloading Completed.')\n\n# # opening the zip file in READ mode\n# with zipfile.ZipFile(fname, 'r') as zipObj:\n#   # extracting all the files\n#   print('Extracting all the files now...')\n#   zipObj.extractall()\n#   print('Done!')\n#   os.remove(fname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load 100 dimension FastText Vectors using FastText library\n# french = fasttext.load_model('cc.es.100.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import fasttext.util\n# # fasttext.util.download_model('en', if_exists='ignore')  # English\n# # ft_en_vectors = fasttext.load_model('/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')\n# from gensim.models import KeyedVectors\n\n# # Load the pre-trained word embeddings using gensim\n# ft_en_vectors = KeyedVectors.load_word2vec_format('/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')\n\n# # # Get the word vector for a specific word\n# word_vector = word_vectors['example']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext.util\n# fasttext.util.download_model('es', if_exists='ignore')  # English\nft_en_vectors = fasttext.load_model('/kaggle/input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext.util\n# fasttext.util.download_model('es', if_exists='ignore')  # English\nft_es_vectors = fasttext.load_model('/kaggle/input/fasttext-spanish-300/cc.es.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see similarity without making bilingual embedding","metadata":{}},{"cell_type":"code","source":"hello = ft_en_vectors.get_word_vector('hello')\nhi = ft_en_vectors.get_word_vector('hi')\nhola = ft_es_vectors.get_word_vector('hola')\n\nprint(f\"Cosine Similarity between HI and HELLO: {cosine_similarity(hello, hi)}\")\nprint(f\"Cosine Similarity between HOLA and HELLO: {cosine_similarity(hello, hola)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = ft_en_vectors.get_word_vector('cat')\nchatte = ft_es_vectors.get_word_vector('chatte')\nchat = ft_es_vectors.get_word_vector('chat')\n\nprint(f\"Cosine Similarity between cat and chatte: {cosine_similarity(cat, chatte)}\")\nprint(f\"Cosine Similarity between cat and chat: {cosine_similarity(cat, chat)}\")\nprint(f\"Cosine Similarity between chatte and chat: {cosine_similarity(chatte, chat)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_words = set(ft_en_vectors.words)\nes_words = set(ft_es_vectors.words)\noverlap = list(en_words & es_words)\nbilingual_dictionary = [(entry, entry) for entry in overlap]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(bilingual_dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_training_matrices(source_dictionary, target_dictionary,\n                           bilingual_dictionary):\n  source_matrix = []\n  target_matrix = []\n  for (source, target) in tqdm(bilingual_dictionary):\n    # if source in source_dictionary.words and target in target_dictionary.words:\n    source_matrix.append(source_dictionary.get_word_vector(source))\n    target_matrix.append(target_dictionary.get_word_vector(target))\n  # return training matrices\n  return np.array(source_matrix), np.array(target_matrix)\n\n\n# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\ndef normalized(a, axis=-1, order=2):\n  \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n  l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n  l2[l2==0] = 1\n  return a / np.expand_dims(l2, axis)\n\n\ndef learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n  \"\"\"\n  Source and target matrices are numpy arrays, shape\n  (dictionary_length, embedding_dimension). These contain paired\n  word vectors from the bilingual dictionary.\n  \"\"\"\n  # optionally normalize the training vectors\n  if normalize_vectors:\n    source_matrix = normalized(source_matrix)\n    target_matrix = normalized(target_matrix)\n  # perform the SVD\n  product = np.matmul(source_matrix.transpose(), target_matrix)\n  U, s, V = np.linalg.svd(product)\n  # return orthogonal transformation which aligns source language to the target\n  return np.matmul(U, V)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source_training_matrix, target_training_matrix = make_training_matrices(ft_en_vectors, ft_es_vectors, bilingual_dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = learn_transformation(source_training_matrix, target_training_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s run the same examples as above, but this time, whenever we use French words, the matrix multiplies the embedding by the transpose of the transform matrix. That works a lot better!","metadata":{}},{"cell_type":"code","source":"hello = ft_en_vectors.get_word_vector('hello')\nhi = ft_en_vectors.get_word_vector('hi')\nhola = np.matmul(ft_es_vectors.get_word_vector('hola'), transform.T)\n\nprint(f\"Cosine Similarity between HI and HELLO: {cosine_similarity(hello, hi)}\")\nprint(f\"Cosine Similarity between HOLA and HELLO: {cosine_similarity(hello, hola)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = ft_en_vectors.get_word_vector('cat')\nchatte = np.matmul(ft_es_vectors.get_word_vector('chatte'), transform.T)\nchat = np.matmul(ft_es_vectors.get_word_vector('chat'), transform.T)\n\nprint(f\"Cosine Similarity between cat and chatte: {cosine_similarity(cat, chatte)}\")\nprint(f\"Cosine Similarity between cat and chat: {cosine_similarity(cat, chat)}\")\nprint(f\"Cosine Similarity between chatte and chat: {cosine_similarity(chatte, chat)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(ft_es_vectors.words)\n\n# for word in ft_es_vectors.words:\n#     ft_es_vectors.get_word_vector(word)\n# print(len(transform.T))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(ft_en_vectors.words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sentence_embedding_en(sentence, max_length=30):\n        words = sentence\n        # filter out words that are not present in the model's vocabulary\n        words = [word for word in words if word in ft_en_vectors.words ]\n        # generate word embeddings for each word\n        embeddings = [ft_en_vectors.get_word_vector(word) for word in words]\n        #mean strategy\n        embedding = np.mean(embeddings, axis=0)\n        return embedding\n    \ndef get_sentence_embedding_es(sentence, max_length=30):\n        words = sentence\n        # filter out words that are not present in the model's vocabulary\n        words = [word for word in words if word in ft_es_vectors.words ]\n        # generate word embeddings for each word\n        embeddings = [np.matmul(ft_es_vectors.get_word_vector(word), transform.T) for word in words]\n        #mean strategy\n        embedding = np.mean(embeddings, axis=0)\n        return embedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eng = get_sentence_embedding_en ( ['hey', 'friends'])\nspa = get_sentence_embedding_es ( [ 'hola', 'amigos'])\ndef sts_score(sim_score):\n    sts_score = (sim_score+1) * 2.5\n    return sts_score\nfrom scipy import spatial\ndef get_sts_scores(emb1, emb2):\n    sim_score = 1 - spatial.distance.cosine(emb1, emb2)\n    return sim_score\n\nnormalized_cos_scores = sts_score(get_sts_scores(eng, spa))\nprint(normalized_cos_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom gensim.models import KeyedVectors\nimport nltk\n# nltk.download()\nimport nltk\n\n# from sts_utils import load_file, sts_score, get_sts_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import nltk\n# from nltk.stem import WordNetLemmatizer\n# from nltk.stem import PorterStemmer\n# lemmatizer = WordNetLemmatizer()\n# ps = PorterStemmer()\n# from scipy import spatial\n\n# def preprocess_text(text):\n#     # Remove punctuation\n#     text = re.sub(r'[^\\w\\s]', '', text)\n#     # Replace numbers with num\n#     text = re.sub(r'\\d+', '', text)\n#     # Lower case\n#     text = text.lower()\n#     sent_token = text.split()\n#     # Lemmatize\n#     sent_token = [lemmatizer.lemmatize(word) for word in sent_token]\n#     # Stemming\n#     sent_token = [ps.stem(word) for word in sent_token]\n#     return sent_token\n    \n# def sts_score(sim_score):\n#     sts_score = (sim_score+1) * 2.5\n#     return sts_score\n# def get_sts_scores(emb1, emb2):\n#     sim_score = 1 - spatial.distance.cosine(emb1, emb2)\n#     return sim_score\n\n# def generate_similarity_score(sent1, sent2):\n#         sent1_token = preprocess_text(sent1)\n#         sent2_token = preprocess_text(sent2)\n# #         sent1_unk_token = [self.unk_replace(word, self.word_dict) for word in sent1_token]\n# #         sent2_unk_token = [self.unk_replace(word, self.word_dict) for word in sent2_token]\n#         sent1_embedding = get_sentence_embedding_en(sent1_token)\n#         sent2_embedding = get_sentence_embedding_es(sent2_token)\n#         normalized_cos_scores = sts_score(get_sts_scores(sent1_embedding, sent2_embedding))\n#         return normalized_cos_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     \"\"\"Sample input sentences to try:\n#     input1: Where is Swaroop going\n#     input2: Swaroop is leaving to college now\n#     \"\"\"\n#     input1 = input(\"Enter your first sentence:\\t\")\n#     input2 = input(\"Enter your second sentence:\\t\")\n#     print(\"Generating the similarity score......\")\n# #     sts = STS_Score()\n#     print(\"The semantic similarity score is\", generate_similarity_score(input1, input2))\n#     print(\"The explainability can be generated through sts_explainability module \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import of Dataset and Cleaning","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# English\ntrain_df_en = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-en-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\nval_df_en = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-en-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\ntest_df_en = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-en-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\n\n# Spanish\ntrain_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\nval_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\ntest_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_en.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nfrom sklearn.linear_model import LinearRegression\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Statistics\nprint(\"Train data size\", len(train_df_en))\nprint(\"Val data size\", len(val_df_en))\nprint(\"Test data size\", len(test_df_en))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nps = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Replace numbers with num\n    text = re.sub(r'\\d+', '', text)\n    # Lower case\n    text= text.lower()\n    sent_token = text.split()\n    # Lemmatize\n#     sent_token = [lemmatizer.lemmatize(word) for word in sent_token]\n    # Stemming\n#     sent_token = [ps.stem(word) for word in sent_token]\n    return sent_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_en['sent1'] = train_df_en['sent1'].apply(lambda x: preprocess_text(x))\ntrain_df_en['sent2'] = train_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\ntrain_df_es['sent1'] = train_df_es['sent1'].apply(lambda x: preprocess_text(x))\ntrain_df_es['sent2'] = train_df_es['sent2'].apply(lambda x: preprocess_text(x))\n\nval_df_en['sent1'] = val_df_en['sent1'].apply(lambda x: preprocess_text(x))\nval_df_en['sent2'] = val_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\nval_df_es['sent1'] = val_df_es['sent1'].apply(lambda x: preprocess_text(x))\nval_df_es['sent2'] = val_df_es['sent2'].apply(lambda x: preprocess_text(x))\n\ntest_df_en['sent1'] = test_df_en['sent1'].apply(lambda x: preprocess_text(x))\ntest_df_en['sent2'] = test_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\ntest_df_es['sent1'] = test_df_es['sent1'].apply(lambda x: preprocess_text(x))\ntest_df_es['sent2'] = test_df_es['sent2'].apply(lambda x: preprocess_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sent_en = list(train_df_en['sent1']) + list(train_df_en['sent2'])\ntotal_sent_es= list(train_df_es['sent1']) + list(train_df_es['sent2'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( total_sent_en[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_dict_en = {}\nfor word_tokens in total_sent_en:\n    for word in word_tokens:\n        if word in word_dict_en:\n            word_dict_en[word] += 1\n        else:\n            word_dict_en[word] = 1\n            \nvocab_length_en = len(word_dict_en)\n\nword_dict_es = {}\nfor word_tokens in total_sent_es:\n    for word in word_tokens:\n        if word in word_dict_es:\n            word_dict_es[word] += 1\n        else:\n            word_dict_es[word] = 1\n            \nvocab_length_es = len(word_dict_es)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vocab_length_en, vocab_length_es)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uni_count_en = 0\n# for value in word_dict_en.values():\n#     if value == 1:\n#         uni_count_en += 1\n# print(uni_count_en)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uni_count_es = 0\n# for value in word_dict_es.values():\n#     if value == 1:\n#         uni_count_es += 1\n# print(uni_count_es)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sorted_counter(word_counter):\n    return {k: v for k, v in sorted(word_counter.items(), key=lambda item: item[1], reverse=False)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_counter_en = get_sorted_counter(word_dict_en)\nsorted_counter_es = get_sorted_counter(word_dict_es)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_dict_en = {}\nfor sent in total_sent_en:\n    if len(sent) in sent_dict_en:\n        sent_dict_en[len(sent)] += 1\n    else:\n        sent_dict_en[len(sent)] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_dict_es = {}\nfor sent in total_sent_es:\n    if len(sent) in sent_dict_es:\n        sent_dict_es[len(sent)] += 1\n    else:\n        sent_dict_es[len(sent)] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(8, 6))\nax.bar(list(sent_dict_en.keys()), list(sent_dict_en.values()), width=0.5)\nplt.title(\"sentence length in English training data\")\nplt.xlabel(\"sentence length\")\nplt.ylabel(\"number of sentences\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(8, 6))\nax.bar(list(sent_dict_es.keys()), list(sent_dict_es.values()), width=0.5)\nplt.title(\"sentence length in Spanish training data\")\nplt.xlabel(\"sentence length\")\nplt.ylabel(\"number of sentences\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def unk_replace(word, word_dict):\n#     if word not in word_dict:\n#         return \"unk\"\n#     else:\n#         if word_dict[word] < 2:\n#             return \"unk\"\n#     return word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df_en['sent1'] = train_df_en['sent1'].apply(lambda x: [unk_replace(word, word_dict_en) for word in x])\n# train_df_en['sent2'] = train_df_en['sent2'].apply(lambda x: [unk_replace(word, word_dict_en) for word in x])\n\n# train_df_es['sent1'] = train_df_es['sent1'].apply(lambda x: [unk_replace(word, word_dict_es) for word in x])\n# train_df_es['sent2'] = train_df_es['sent2'].apply(lambda x: [unk_replace(word, word_dict_es) for word in x])\n\n# val_df_en['sent1'] = val_df_en['sent1'].apply(lambda x: [unk_replace(word, word_dict_en) for word in x])\n# val_df_en['sent2'] = val_df_en['sent2'].apply(lambda x: [unk_replace(word, word_dict_en) for word in x])\n\n# val_df_es['sent1'] = val_df_es['sent1'].apply(lambda x: [unk_replace(word, word_dict_es) for word in x])\n# val_df_es['sent2'] = val_df_es['sent2'].apply(lambda x: [unk_replace(word, word_dict_es) for word in x])\n\n# test_df_en['sent1'] = test_df_en['sent1'].apply(lambda x: [unk_replace(word, word_dict_en) for word in x])\n# test_df_en['sent2'] = test_df_en['sent2'].apply(lambda x: [unk_replace(word, word_dict_en) for word in x])\n\n# test_df_es['sent1'] = test_df_es['sent1'].apply(lambda x: [unk_replace(word, word_dict_es) for word in x])\n# test_df_es['sent2'] = test_df_es['sent2'].apply(lambda x: [unk_replace(word, word_dict_es) for word in x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL","metadata":{}},{"cell_type":"code","source":"# English\n# train_df_en = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-en-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\n# val_df_en = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-en-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\n# test_df_en = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-en-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\n\n# # Spanish\n# train_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\n# val_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\n# test_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df_en['sent1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_en['sent1'] = train_df_en['sent1'].astype(str).apply(eval)\ntrain_df_en['sent2'] = train_df_en['sent2'].astype(str).apply(eval)\n\ntrain_df_es['sent1'] = train_df_es['sent1'].astype(str).apply(eval)\ntrain_df_es['sent2'] = train_df_es['sent2'].astype(str).apply(eval)\n\nval_df_en['sent1'] = val_df_en['sent1'].astype(str).apply(eval)\nval_df_en['sent2'] = val_df_en['sent2'].astype(str).apply(eval)\n\nval_df_es['sent1'] = val_df_es['sent1'].astype(str).apply(eval)\nval_df_es['sent2'] = val_df_es['sent2'].astype(str).apply(eval)\n\ntest_df_en['sent1'] = test_df_en['sent1'].astype(str).apply(eval)\ntest_df_en['sent2'] = test_df_en['sent2'].astype(str).apply(eval)\n\ntest_df_es['sent1'] = test_df_es['sent1'].astype(str).apply(eval)\ntest_df_es['sent2'] = test_df_es['sent2'].astype(str).apply(eval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pickle5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle5 as pickle\nwith open(\"/kaggle/input/word-dict/word_dict.pickle\", 'rb') as handle:\n      word_dict = pickle.load(handle)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = train_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = train_df_es[\"sent2\"] \n\nscore = train_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = train_df_en[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = train_df_es[\"sent1\"] \n\nscore = train_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_train = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = val_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = val_df_es[\"sent2\"] \n\nscore = val_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = val_df_es[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = val_df_es[\"sent1\"] \n\nscore = val_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_val = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = test_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = test_df_es[\"sent2\"] \n\nscore = test_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = test_df_es[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = test_df_es[\"sent1\"] \n\nscore = test_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_test = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(final_train))\n# print(len(final_val))\n# print(len(final_test))\n# final_train.head()\n# final_val.head()\n# final_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\n\n# Define the model architecture\nclass RNNSentenceEncoder_1(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, bidirectional=True):\n        super(RNNSentenceEncoder_1, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        \n        # Initialize the embedding layer with FastText word vectors\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(ft_en_vectors.get_output_matrix()))\n\n        # Freeze the embedding layer\n        self.embedding.requires_grad = False\n        \n        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, bidirectional=bidirectional)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.rnn(x)\n        x = self.fc(x[:, -1, :])\n        x = self.relu(x)\n        return x\n\n\nclass RNNSentenceEncoder_2(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, bidirectional=True, transform=None):\n        super(RNNSentenceEncoder_2, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        self.transform = transform\n        \n        # Initialize the embedding layer with FastText word vectors\n        if self.transform is not None:\n            ft_es_vectors_transformed = []\n            for vec in ft_es_vectors.get_output_matrix():\n                ft_es_vectors_transformed.append(np.matmul(vec, self.transform.T))\n            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(ft_es_vectors_transformed))\n        else:\n            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(ft_es_vectors.get_output_matrix()))\n\n        # Freeze the embedding layer\n        self.embedding.requires_grad = False\n        \n        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, bidirectional=bidirectional)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        if self.transform is not None:\n            x_transformed = []\n            for sentence in x:\n                sentence_transformed = []\n                for word in sentence:\n                    word_transformed = np.matmul(ft_es_vectors.get_word_vector(word), self.transform.T)\n                    sentence_transformed.append(word_transformed)\n                x_transformed.append(torch.tensor(sentence_transformed))\n            x = torch.stack(x_transformed)\n        else:\n            x = torch.tensor(x)\n        x = self.embedding(x)\n        x, _ = self.rnn(x)\n        x = self.fc(x[:, -1, :])\n        x = self.relu(x)\n        return x\n\n    \n    \ndef sent_indicies(sent1):\n    vocab = {}\n    for i, (word1, word2) in enumerate(sent1):\n        if word1 not in vocab:\n            vocab[word1] = len(vocab)\n        if word2 not in vocab:\n            vocab[word2] = len(vocab)\n\n    # Convert the list to a list of numerical indices\n    lst_indices = []\n    for word1, word2 in sent1:\n        lst_indices.append((vocab[word1], vocab[word2]))\n    return lst_indices\n\nclass SimilarityModel(nn.Module):\n    def __init__(self, sentence_encoder1, sentence_encoder2):\n        super(SimilarityModel, self).__init__()\n        self.encoder_1 = sentence_encoder_1\n        self.encoder_2 = sentence_encoder_2\n        self.mlp = nn.Sequential(\n            nn.Linear(2 * hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n    def forward(self, sentences1, sentences2):\n        \n        #print(sent_indicies(sentences1))\n        #print(sentences1)\n        \n        #encoded1 = self.encoder(torch.tensor(sent_indicies(sentences1)))\n        #encoded2 = self.encoder(torch.tensor(sent_indicies(sentences2)))\n        #encoded1 = self.encoder(torch.tensor(sentences1))\n        #encoded2 = self.encoder(torch.tensor(sentences2))\n        encoded1 = self.encoder_1(sentences1)\n        encoded2 = self.encoder_2(sentences2)\n        concatenated = torch.cat([encoded1, encoded2], dim=1)\n        output = self.mlp(concatenated)\n        return output\n\n# Define the dataset\nclass MyDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.vocab = list(word_dict)\n        self.vocab2 = list(word_dict_es)\n        self.vocab.append('unk')\n        self.vocab.append('<pad>')\n        #self.vocab = ['<OOV>', 'apple', 'banana', 'orange', 'cat', 'dog', 'grape', 'hello', 'world', '<pad>']\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def get_indexes(self, sentence, max_len=100):\n        indexed_sentence = []\n        for word in sentence:\n            if word in self.vocab:\n                indexed_sentence.append(self.vocab.index(word))\n            else:\n                indexed_sentence.append(self.vocab.index('unk'))\n        diff = abs(max_len - len(indexed_sentence))\n        if diff > 0:\n            for i in range(diff):\n                indexed_sentence.append(self.vocab.index('<pad>'))\n        return indexed_sentence\n    \n    def get_indexes2(self, sentence, max_len=100):\n        indexed_sentence = []\n        for word in sentence:\n            if word in self.vocab2:\n                indexed_sentence.append(self.vocab2.index(word))\n            else:\n                indexed_sentence.append(len(self.vocab2)+1)\n        diff = abs(max_len - len(indexed_sentence))\n        if diff > 0:\n            for i in range(diff):\n                indexed_sentence.append(len(self.vocab2))\n        return indexed_sentence\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        sentence1 = self.get_indexes(row['sent1'])\n        sentence2 = self.get_indexes2(row['sent2'])\n        similarity_score = row['score']\n        return torch.tensor(sentence1), torch.tensor(sentence2), torch.tensor(similarity_score, dtype=torch.float32)\n\n# Define the training function\ndef train(model, num_epochs, train_loader, val_loader, optimizer, criterion, device):\n    train_losses = []\n    val_losses = []\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for batch_idx, (sentences1, sentences2, similarity_scores) in enumerate(train_loader):\n            # move data to device\n            sentences1, sentences2, similarity_scores = sentences1.to(device), sentences2.to(device), similarity_scores.to(device)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = model(sentences1, sentences2)\n            loss = criterion(outputs, similarity_scores.view(-1, 1))\n            loss.backward()\n            optimizer.step()\n\n            # calculate statistics\n            train_loss += loss.item()\n            #train_correct += torch.sum(torch.abs(outputs - similarity_scores.view(-1, 1)) <= 0.5)\n\n#             if batch_idx % 100 == 99:\n#                 print(f'Train Batch {batch_idx+1}/{len(train_loader)} Loss: {train_loss/(batch_idx+1):.4f}')\n            print(\"sent\")\n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n\n         # Evaluate the model on the validation set\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch_idx, (sentences1, sentences2, similarity_scores) in enumerate(val_loader):\n                val_output = model(sentences1, sentences2)\n                loss = criterion(val_output, similarity_scores.view(-1, 1))\n                val_loss += loss.item()\n                #val_loss += loss_fn(val_output.squeeze(), scores_batch).item() * len(embeddings1_batch)\n            val_loss /= len(val_loader)\n            val_losses.append(val_loss)\n\n            #train_accuracy = train_correct.double() / len(train_loader.dataset)\n        print('Epoch {} - Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, train_loss, val_loss))\n    return train_losses, val_losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the hyperparameters\nvocab_size = 10000\nembedding_dim = 300\nhidden_dim = 128\nnum_layers = 2\nbidirectional = True\nlearning_rate = 0.01\nbatch_size = 100\nnum_epochs = 10\n\n# Initialize the dataset and dataloader\ndataset = MyDataset(final_train)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\nval_dataset = MyDataset(final_val)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize the model and optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nsentence_encoder_1 = RNNSentenceEncoder_1(vocab_size, embedding_dim, hidden_dim, num_layers, bidirectional)\nsentence_encoder_2 = RNNSentenceEncoder_2(vocab_size, embedding_dim, hidden_dim, num_layers, bidirectional)\n\nmodel = SimilarityModel(sentence_encoder_1,sentence_encoder_2).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Initialize the loss function\ncriterion = nn.MSELoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses = train(model, num_epochs, dataloader, val_dataloader, optimizer, criterion, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}