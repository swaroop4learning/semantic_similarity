{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport fasttext\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Figure Settings\nimport ipywidgets as widgets\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Helper functions\ndef cosine_similarity(vec_a, vec_b):\n  \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n  return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n\n\ndef getSimilarity(word1, word2):\n  v1 = ft_en_vectors.get_word_vector(word1)\n  v2 = ft_en_vectors.get_word_vector(word2)\n  return cosine_similarity(v1, v2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# For DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  \"\"\"\n  Function that controls randomness.\n  NumPy and random modules must be imported.\n\n  Args:\n    seed : Integer\n      A non-negative integer that defines the random state. Default is `None`.\n    seed_torch : Boolean\n      If `True` sets the random seed for pytorch tensors, so pytorch module\n      must be imported. Default is `True`.\n\n  Returns:\n    Nothing.\n  \"\"\"\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  \"\"\"\n  DataLoader will reseed workers following randomness in\n  multi-process data loading algorithm.\n\n  Args:\n    worker_id: integer\n      ID of subprocess to seed. 0 means that\n      the data will be loaded in the main process\n      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n\n  Returns:\n    Nothing\n  \"\"\"\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Set device (GPU or CPU). Execute `set_device()`\n\n# Inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  \"\"\"\n  Set the device. CUDA if available, CPU otherwise\n\n  Args:\n    None\n\n  Returns:\n    Nothing\n  \"\"\"\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = set_device()\nSEED = 2021\nset_seed(seed=SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext.util\n# fasttext.util.download_model('en', if_exists='ignore')  # English\nft_en_vectors = fasttext.load_model('/kaggle/input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext.util\n# fasttext.util.download_model('es', if_exists='ignore')  # English\nft_es_vectors = fasttext.load_model('/kaggle/input/fasttext-spanish-300/cc.es.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see similarity without making bilingual embedding","metadata":{}},{"cell_type":"code","source":"# hello = ft_en_vectors.get_word_vector('hello')\n# hi = ft_en_vectors.get_word_vector('hi')\n# bonjour = ft_es_vectors.get_word_vector('bonjour')\n\n# print(f\"Cosine Similarity between HI and HELLO: {cosine_similarity(hello, hi)}\")\n# print(f\"Cosine Similarity between BONJOUR and HELLO: {cosine_similarity(hello, bonjour)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat = ft_en_vectors.get_word_vector('cat')\n# chatte = ft_es_vectors.get_word_vector('chatte')\n# chat = ft_es_vectors.get_word_vector('chat')\n\n# print(f\"Cosine Similarity between cat and chatte: {cosine_similarity(cat, chatte)}\")\n# print(f\"Cosine Similarity between cat and chat: {cosine_similarity(cat, chat)}\")\n# print(f\"Cosine Similarity between chatte and chat: {cosine_similarity(chatte, chat)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_words = set(ft_en_vectors.words)\nes_words = set(ft_es_vectors.words)\noverlap = list(en_words & es_words)\nbilingual_dictionary = [(entry, entry) for entry in overlap]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(bilingual_dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_training_matrices(source_dictionary, target_dictionary,\n                           bilingual_dictionary):\n  source_matrix = []\n  target_matrix = []\n  for (source, target) in tqdm(bilingual_dictionary):\n    # if source in source_dictionary.words and target in target_dictionary.words:\n    source_matrix.append(source_dictionary.get_word_vector(source))\n    target_matrix.append(target_dictionary.get_word_vector(target))\n  # return training matrices\n  return np.array(source_matrix), np.array(target_matrix)\n\n\n# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\ndef normalized(a, axis=-1, order=2):\n  \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n  l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n  l2[l2==0] = 1\n  return a / np.expand_dims(l2, axis)\n\n\ndef learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n  \"\"\"\n  Source and target matrices are numpy arrays, shape\n  (dictionary_length, embedding_dimension). These contain paired\n  word vectors from the bilingual dictionary.\n  \"\"\"\n  # optionally normalize the training vectors\n  if normalize_vectors:\n    source_matrix = normalized(source_matrix)\n    target_matrix = normalized(target_matrix)\n  # perform the SVD\n  product = np.matmul(source_matrix.transpose(), target_matrix)\n  U, s, V = np.linalg.svd(product)\n  # return orthogonal transformation which aligns source language to the target\n  return np.matmul(U, V)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source_training_matrix, target_training_matrix = make_training_matrices(ft_en_vectors, ft_es_vectors, bilingual_dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = learn_transformation(source_training_matrix, target_training_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Letâ€™s run the same examples as above, but this time, whenever we use French words, the matrix multiplies the embedding by the transpose of the transform matrix. That works a lot better!","metadata":{}},{"cell_type":"code","source":"# hello = ft_en_vectors.get_word_vector('hello')\n# hi = ft_en_vectors.get_word_vector('hi')\n# bonjour = np.matmul(ft_es_vectors.get_word_vector('bonjour'), transform.T)\n\n# print(f\"Cosine Similarity between HI and HELLO: {cosine_similarity(hello, hi)}\")\n# print(f\"Cosine Similarity between BONJOUR and HELLO: {cosine_similarity(hello, bonjour)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat = ft_en_vectors.get_word_vector('cat')\n# chatte = np.matmul(ft_es_vectors.get_word_vector('chatte'), transform.T)\n# chat = np.matmul(ft_es_vectors.get_word_vector('chat'), transform.T)\n\n# print(f\"Cosine Similarity between cat and chatte: {cosine_similarity(cat, chatte)}\")\n# print(f\"Cosine Similarity between cat and chat: {cosine_similarity(cat, chat)}\")\n# print(f\"Cosine Similarity between chatte and chat: {cosine_similarity(chatte, chat)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Embeddings have now being obtained","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom gensim.models import KeyedVectors\nimport nltk\n# nltk.download()\nimport nltk\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# English\ntrain_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_train_df.csv')\nval_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_val_df.csv')\ntest_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_test_df.csv')\n\n# Spanish\ntrain_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\nval_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\ntest_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nfrom sklearn.linear_model import LinearRegression\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Replace numbers with num\n    text = re.sub(r'\\d+', '', text)\n    # Lower case\n    text= text.lower()\n    sent_token = text.split()\n    # Lemmatize\n#     sent_token = [lemmatizer.lemmatize(word) for word in sent_token]\n    # Stemming\n#     sent_token = [ps.stem(word) for word in sent_token]\n    return sent_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df_en['sent1'] = train_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# train_df_en['sent2'] = train_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\ntrain_df_es['sent1'] = train_df_es['sent1'].apply(lambda x: preprocess_text(x))\ntrain_df_es['sent2'] = train_df_es['sent2'].apply(lambda x: preprocess_text(x))\n\n# val_df_en['sent1'] = val_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# val_df_en['sent2'] = val_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\nval_df_es['sent1'] = val_df_es['sent1'].apply(lambda x: preprocess_text(x))\nval_df_es['sent2'] = val_df_es['sent2'].apply(lambda x: preprocess_text(x))\n\n# test_df_en['sent1'] = test_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# test_df_en['sent2'] = test_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\ntest_df_es['sent1'] = test_df_es['sent1'].apply(lambda x: preprocess_text(x))\ntest_df_es['sent2'] = test_df_es['sent2'].apply(lambda x: preprocess_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sent_en = list(train_df_en['sent1']) + list(train_df_en['sent2'])\ntotal_sent_es= list(train_df_es['sent1']) + list(train_df_es['sent2'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_dict_en = {}\nfor word_tokens in total_sent_en:\n    for word in word_tokens:\n        if word in word_dict_en:\n            word_dict_en[word] += 1\n        else:\n            word_dict_en[word] = 1\n            \nvocab_length_en = len(word_dict_en)\n\nword_dict_es = {}\nfor word_tokens in total_sent_es:\n    for word in word_tokens:\n        if word in word_dict_es:\n            word_dict_es[word] += 1\n        else:\n            word_dict_es[word] = 1\n            \nvocab_length_es = len(word_dict_es)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sorted_counter(word_counter):\n    return {k: v for k, v in sorted(word_counter.items(), key=lambda item: item[1], reverse=False)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_counter_en = get_sorted_counter(word_dict_en)\nsorted_counter_es = get_sorted_counter(word_dict_es)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_en['sent1'] = train_df_en['sent1'].astype(str).apply(eval)\ntrain_df_en['sent2'] = train_df_en['sent2'].astype(str).apply(eval)\n\ntrain_df_es['sent1'] = train_df_es['sent1'].astype(str).apply(eval)\ntrain_df_es['sent2'] = train_df_es['sent2'].astype(str).apply(eval)\n\nval_df_en['sent1'] = val_df_en['sent1'].astype(str).apply(eval)\nval_df_en['sent2'] = val_df_en['sent2'].astype(str).apply(eval)\n\nval_df_es['sent1'] = val_df_es['sent1'].astype(str).apply(eval)\nval_df_es['sent2'] = val_df_es['sent2'].astype(str).apply(eval)\n\ntest_df_en['sent1'] = test_df_en['sent1'].astype(str).apply(eval)\ntest_df_en['sent2'] = test_df_en['sent2'].astype(str).apply(eval)\n\ntest_df_es['sent1'] = test_df_es['sent1'].astype(str).apply(eval)\ntest_df_es['sent2'] = test_df_es['sent2'].astype(str).apply(eval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = train_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = train_df_es[\"sent2\"] \n\nscore = train_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = train_df_en[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = train_df_es[\"sent1\"] \n\nscore = train_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_train = new_df_one\n# printing the new dataframe\n# final_train.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = val_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = val_df_es[\"sent2\"] \n\nscore = val_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = val_df_es[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = val_df_es[\"sent1\"] \n\nscore = val_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_val = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = test_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = test_df_es[\"sent2\"] \n\nscore = test_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = test_df_es[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = test_df_es[\"sent1\"] \n\nscore = test_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_test = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sentences1 = list(final_train['sent1'])\ntrain_sentences2 = list(final_train['sent2'])\ntrain_similarity_scores = list(final_train['score'])\n\nval_sentences1 = list(final_val['sent1'])\nval_sentences2 = list(final_val['sent2'])\nval_similarity_scores = list(final_val['score'])\n\ntest_sentences1 = list(final_test['sent1'])\ntest_sentences2 = list(final_test['sent2'])\ntest_similarity_scores = list(final_test['score'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_sentences1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sentence_embedding_en(sentence, max_length=30):\n        words = sentence\n        # filter out words that are not present in the model's vocabulary\n        words = [word for word in words if word in ft_en_vectors.words ]\n        # generate word embeddings for each word\n        embeddings = [ft_en_vectors.get_word_vector(word) for word in words]\n        #mean strategy\n        embedding = np.mean(embeddings, axis=0)\n        return embedding\n    \ndef get_sentence_embedding_es(sentence, max_length=30):\n        words = sentence\n        # filter out words that are not present in the model's vocabulary\n        words = [word for word in words if word in ft_es_vectors.words ]\n        # generate word embeddings for each word\n        embeddings = [np.matmul(ft_es_vectors.get_word_vector(word), transform.T) for word in words]\n        #mean strategy\n        embedding = np.mean(embeddings, axis=0)\n        return embedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print( val_sentences2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate sentence embeddings\ntrain_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in train_sentences1])\nprint(\"1\")\ntrain_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in train_sentences2])\nprint(\"1\")\n\nval_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in val_sentences1])\nprint(\"1\")\nval_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in val_sentences2])\nprint(\"1\")\n\n\ntest_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in test_sentences1])\nprint(\"1\")\n\ntest_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in test_sentences2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_X2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = np.concatenate([train_X1, train_X2], axis=1)\nval_X = np.concatenate([val_X1, val_X2], axis=1)\ntest_X = np.concatenate([test_X1, test_X2], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sts_score(sim_score):\n    sts_score = (sim_score+1) * 2.5\n    return sts_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial\nfrom scipy.stats import pearsonr\n\ndef get_sts_scores(emb1_lt, emb2_lt):\n    y_pred = []\n    for i in range(len(emb1_lt)):\n        sim_score = 1 - spatial.distance.cosine(emb1_lt[i], emb2_lt[i])\n        y_pred.append(sts_score(sim_score))\n    return y_pred\n    \ndef pearson_corr(y_true, y_pred):\n    \"\"\"\n    Calculate Pearson correlation coefficient between two arrays.\n    \"\"\"\n    corr, _ = pearsonr(y_true, y_pred)\n    return corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalised Cosine Similarity","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import torch\n# import fasttext.util\n# from torch import nn\n# from torch.utils.data import DataLoader\n\n# # Load FastText embeddings\n# ft_es_vectors = fasttext.load_model('/kaggle/input/fasttext-spanish-300/cc.es.300.bin')\n# ft_en_vectors = fasttext.load_model('/kaggle/input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin')\n\n# # Get the word vectors and corresponding words for both languages\n# words_es = ft_es_vectors.get_words()\n# vectors_es = np.array([ft_es_vectors.get_word_vector(word) for word in words_es])\n\n# words_en = ft_en_vectors.get_words()\n# vectors_en = np.array([ft_en_vectors.get_word_vector(word) for word in words_en])\n\n# # Convert the word vectors to PyTorch tensors\n# vectors_es_tensor = torch.from_numpy(vectors_es).float()\n# vectors_en_tensor = torch.from_numpy(vectors_en).float()\n\n# # Define a simple feed-forward neural network for mapping\n# class MappingNetwork(nn.Module):\n#     def __init__(self, input_dim, output_dim):\n#         super(MappingNetwork, self).__init__()\n#         self.fc = nn.Linear(input_dim, output_dim)\n\n#     def forward(self, x):\n#         return self.fc(x)\n\n# # Create the mapping network\n# input_dim = vectors_es_tensor.shape[1]\n# output_dim = vectors_en_tensor.shape[1]\n# mapping_network = MappingNetwork(input_dim, output_dim)\n\n# # Define the loss function and optimizer\n# loss_fn = nn.MSELoss()\n# optimizer = torch.optim.Adam(mapping_network.parameters(), lr=0.001)\n\n# # Train the mapping network\n# dataset = DataLoader(list(zip(vectors_es_tensor, vectors_en_tensor)), batch_size=10000, shuffle=True)\n\n# num_epochs = 2\n# total_steps = len(dataset) * num_epochs\n\n# for epoch in range(num_epochs):\n#     for step, batch in enumerate(dataset):\n#         src_vectors, tgt_vectors = batch\n#         optimizer.zero_grad()\n#         mapped_vectors = mapping_network(src_vectors)\n#         loss = loss_fn(mapped_vectors, tgt_vectors)\n#         loss.backward()\n#         optimizer.step()\n\n#         # Calculate progress in percentage\n#         progress = (epoch * len(dataset) + step + 1) / total_steps * 100\n\n#         # Print epoch and progress\n#     print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(dataset)}], Progress: {progress:.2f}%\")\n# print(\"!\")\n# # Map the source language word vectors to the target language space\n# # mapped_vectors_es = mapping_network(vectors_es_tensor).detach().numpy()\n\n# # Define batch size for processing\n# batch_size = 1000\n\n# # Calculate the total number of batches\n# total_batches = int(np.ceil(len(vectors_es_tensor) / batch_size))\n\n# # Create an empty array to store the mapped vectors\n# mapped_vectors_es = []\n\n# # Process the data in batches\n# for i in range(total_batches):\n#     start_idx = i * batch_size\n#     end_idx = (i + 1) * batch_size\n\n#     # Get a batch of source vectors\n#     src_vectors_batch = vectors_es_tensor[start_idx:end_idx]\n\n#     # Map the batch of source vectors to the target language space\n#     mapped_vectors_batch = mapping_network(src_vectors_batch).detach().numpy()\n\n#     # Append the mapped vectors to the results array\n#     mapped_vectors_es.append(mapped_vectors_batch)\n\n# # Concatenate the mapped vectors from all batches\n# mapped_vectors_es = np.concatenate(mapped_vectors_es, axis=0)\n\n\n# print(\"!\")\n# # Calculate cosine similarity between mapped source vectors and target vectors\n# cos_sim_scores = np.dot(mapped_vectors_es, vectors_en.T) / (\n#         np.linalg.norm(mapped_vectors_es, axis=1).reshape(-1, 1) * np.linalg.norm(vectors_en, axis=1))\n# print(\"!\")\n# # \n\n# import numpy as np\n\n# # Example English and Spanish sentences\n# sentence_en = \"I like cats\"\n# sentence_es = \"Me gustan los gatos\"\n# print(\"!\")\n# # Tokenize the sentences into words\n# tokens_en = sentence_en.lower().split()\n# tokens_es = sentence_es.lower().split()\n# print(\"!\")\n# # Calculate sentence embeddings by averaging word vectors\n# embedding_en = np.mean([mapped_vectors_es[words_es.index(token)] for token in tokens_en], axis=0)\n# embedding_es = np.mean([vectors_es[words_es.index(token)] for token in tokens_es], axis=0)\n# print(\"!\")\n# # Calculate cosine similarity between sentence embeddings\n# cos_sim = np.dot(embedding_en, embedding_es) / (np.linalg.norm(embedding_en) * np.linalg.norm(embedding_es))\n\n# print(\"Cosine Similarity:\", cos_sim)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n\n# # Example English and Spanish sentences\n# sentence_en = \"I like cats\"\n# sentence_es = \"Me gustan los gatos\"\n\n# # Tokenize the sentences into words\n# tokens_en = sentence_en.lower().split()\n# tokens_es = sentence_es.lower().split()\n\n# # Calculate sentence embeddings by averaging word vectors\n# embedding_en = np.mean([mapped_vectors_es[words_es.index(token)] for token in tokens_en], axis=0)\n# embedding_es = np.mean([vectors_es[words_es.index(token)] for token in tokens_es], axis=0)\n\n# # Calculate cosine similarity between sentence embeddings\n# cos_sim = np.dot(embedding_en, embedding_es) / (np.linalg.norm(embedding_en) * np.linalg.norm(embedding_es))\n\n# print(\"Cosine Similarity:\", cos_sim)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example of computing similarity between two words\n# word_idx_es = list(words_es).index('manzana')\n# word_idx_en = list(words_en).index('apple')\n# similarity_score = cos_sim_scores[word_idx_es][word_idx_en]\n# print(\"Similarity score between 'manzana' and 'apple':\", similarity_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y_pred = get_sts_scores(train_X1, train_X2)\nval_y_pred = get_sts_scores(val_X1, val_X2)\ntest_y_pred = get_sts_scores(test_X1, test_X2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(train_similarity_scores, train_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(val_similarity_scores, val_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(test_similarity_scores, test_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Residual Analysis","metadata":{}},{"cell_type":"code","source":"# row_max_len = lambda row: max(len(row['sent1']), len(row['sent2']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df['sent_len'] = train_df.apply(row_max_len, axis=1)\n# val_df['sent_len'] = val_df.apply(row_max_len, axis=1)\n# test_df['sent_len'] = test_df.apply(row_max_len, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df['diff'] = abs(train_df['score'] - train_y_pred)\n# val_df['diff'] = abs(val_df['score'] - val_y_pred)\n# test_df['diff'] = abs(test_df['score'] - test_y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_df = train_df.groupby(['sent_len']).mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_df['sent_length'] = grouped_df.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# fig, ax = plt.subplots(figsize=(8, 6))\n# ax.scatter(list(grouped_df.index), list(grouped_df['diff']))\n# plt.title(\"average error trend in training data\")\n# plt.xlabel(\"sentence length\")\n# plt.ylabel(\"average difference\")\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_val = val_df.groupby(['sent_len']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(8, 6))\n# ax.scatter(list(grouped_val.index), list(grouped_val['diff']))\n# plt.title(\"average error trend in validation data\")\n# plt.xlabel(\"sentence length\")\n# plt.ylabel(\"average difference\")\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_test = test_df.groupby(['sent_len']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(8, 6))\n# ax.scatter(list(grouped_test.index), list(grouped_test['diff']))\n# plt.title(\"average error trend in test data\")\n# plt.xlabel(\"sentence length\")\n# plt.ylabel(\"average difference\")\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# train a linear regression model\nreg = LinearRegression().fit(train_X, train_s1imilarity_scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_y_pred = reg.predict(val_X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(val_similarity_scores, val_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_y_pred = reg.predict(test_X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(test_similarity_scores, test_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the data into PyTorch tensors\ntrain_embeddings1 = torch.tensor(train_X1, dtype=torch.float)\ntrain_embeddings2 = torch.tensor(train_X2, dtype=torch.float)\nscores = torch.tensor(train_similarity_scores, dtype=torch.float)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_embeddings1 = torch.tensor(val_X1, dtype=torch.float)\nval_embeddings2 = torch.tensor(val_X2, dtype=torch.float)\nval_scores = torch.tensor(val_similarity_scores, dtype=torch.float)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embeddings1 = torch.tensor(test_X1, dtype=torch.float)\ntest_embeddings2 = torch.tensor(test_X2, dtype=torch.float)\ntest_scores = torch.tensor(test_similarity_scores, dtype=torch.float)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the hyperparameters\ninput_dim = 300 # The dimension of the sentence embeddings\nhidden_dim = 150\nlr = 0.01\nnum_epochs = 10\n#batch_size = 10","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class BiLSTMRegression(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, num_layers):\n#         super().__init__()\n#         self.input_dim = input_dim\n#         self.hidden_dim = hidden_dim\n#         self.num_layers = num_layers\n#         self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n#         self.fc = nn.Linear(hidden_dim*2, 1)\n\n#     def forward(self, x1, x2):\n#         x = torch.cat((x1, x2), dim=1)\n#         x = x.view(len(x), 1, -1)\n#         h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n#         c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n#         out, _ = self.bilstm(x, (h0, c0))\n#         out = self.fc(out[:, -1, :])\n#         return out\n    \nclass BiLSTMRegression(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.dropout_prob = dropout_prob\n        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc = nn.Linear(hidden_dim*2, 1)\n\n    def forward(self, x1, x2):\n        x = torch.cat((x1, x2), dim=1)\n        x = x.view(len(x), 1, -1)\n        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n        out, _ = self.bilstm(x, (h0, c0))\n        out = self.dropout(out) # Apply dropout\n        out = self.fc(out[:, -1, :])\n        return out\n\n    \n\nclass GRURegression(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim*2, 1)\n\n    def forward(self, x1, x2):\n        x = torch.cat((x1, x2), dim=1)\n        x = x.view(len(x), 1, -1)\n        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n        out, _ = self.gru(x, h0)\n        out = self.fc(out[:, -1, :])\n        return out\n\n\nclass BiLSTMAttention(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, attention_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.attention_dim = attention_dim\n        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n        self.attention = SelfAttention(hidden_dim*2, attention_dim, 1)\n        #self.attention = nn.Linear(hidden_dim*2, attention_dim)\n        #self.softmax = nn.Softmax(dim=1)\n        self.fc = nn.Linear(hidden_dim*2, 1)\n\n    def forward(self, x1, x2):\n        x = torch.cat((x1, x2), dim=1)\n        x = x.view(len(x), 1, -1)\n        h0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers*2, len(x), self.hidden_dim).to(x.device)\n        out, _ = self.bilstm(x, (h0, c0))\n        #att_weights = self.softmax(self.attention(out))\n        att_weights = self.attention(out)\n        out = torch.sum(out * att_weights, dim=1)\n        out = self.fc(out)\n        return out\n\nclass SelfAttention(nn.Module):\n    \"\"\"\n    Implementation of the attention block\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SelfAttention, self).__init__()\n        # TODO implement\n\n        # constructing linear layers with weights analogous to Ws1 and Ws2\n        self.layer1 = nn.Linear(input_size, hidden_size, bias=False)\n        self.layer2 = nn.Linear(hidden_size, output_size, bias=False)\n        self.softmax = nn.Softmax(dim=1)\n\n    ## the forward function would receive lstm's all hidden states as input\n    def forward(self, attention_input):\n        # TODO implement\n\n        # implementing the attention mechanism\n        output = self.layer1(attention_input)\n        output = torch.tanh(output)\n        output = self.layer2(output)\n        #output = F.softmax(output.transpose(1,2), dim=2)\n        output = self.softmax(output)\n\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = GRURegression(input_dim*2, hidden_dim, num_layers=2)\nmodel = BiLSTMRegression(input_dim*2, hidden_dim, num_layers=2, dropout_prob = 0.3)\n#model = BiLSTMAttention(input_dim*2, hidden_dim, num_layers=2, attention_dim=600)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n\n# Define the loss function\nloss_fn = nn.MSELoss()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.utils.data as data\n\n# Define a custom dataset class\nclass SentenceSimilarityDataset(data.Dataset):\n    def __init__(self, embeddings1, embeddings2, scores):\n        self.embeddings1 = embeddings1\n        self.embeddings2 = embeddings2\n        self.scores = scores\n\n    def __len__(self):\n        return len(self.embeddings1)\n\n    def __getitem__(self, index):\n        return self.embeddings1[index], self.embeddings2[index], self.scores[index]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dataset\ntrain_dataset = SentenceSimilarityDataset(train_embeddings1, train_embeddings2, scores)\nval_dataset = SentenceSimilarityDataset(val_embeddings1, val_embeddings2, val_scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the batch size\nbatch_size = 10\n\n# Create the DataLoader\ntrain_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, num_epochs, train_dataloader, val_dataloader):\n    train_losses = []\n    val_losses = []\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for batch in train_dataloader:\n            optimizer.zero_grad()\n            embeddings1_batch, embeddings2_batch, scores_batch = batch\n            output = model(embeddings1_batch, embeddings2_batch)\n            loss = loss_fn(output.squeeze(), scores_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * len(embeddings1_batch)\n        train_loss /= len(train_dataloader)\n        train_losses.append(train_loss)\n\n    # Evaluate the model on the validation set\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch in val_dataloader:\n                embeddings1_batch, embeddings2_batch, scores_batch = batch\n                val_output = model(embeddings1_batch, embeddings2_batch)\n                val_loss += loss_fn(val_output.squeeze(), scores_batch).item() * len(embeddings1_batch)\n            val_loss /= len(val_dataloader)\n            val_losses.append(val_loss)\n\n        print('Epoch {} - Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, train_loss, val_loss))\n    return train_losses, val_losses","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses = train(model, optimizer, num_epochs, train_dataloader, val_dataloader)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BiGRU Analysis","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Define the data\nepochs = range(1, 16)\n#train_losses = train_loss\n#val_losses = val_loss\n\n# Plot the data\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\n\n# Add labels and title\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train vs Validation Loss for BiGRU')\n\n# Add legend\nplt.legend()\n\n# Show the plot\nplt.show()\nprint(\"\\t More Data needed\\t\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, emb1, emb2):\n    model.eval()\n    with torch.no_grad():\n        test_output = model(emb1, emb2)\n    y_pred_test = test_output.squeeze().tolist()\n    return y_pred_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = evaluate(model, train_embeddings1, train_embeddings2)\ncorr = pearson_corr(train_similarity_scores, y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = evaluate(model, val_embeddings1, val_embeddings2)\ncorr = pearson_corr(val_similarity_scores, y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = evaluate(model, test_embeddings1, test_embeddings2)\ncorr = pearson_corr(test_similarity_scores, y_pred_test)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_losses), len(val_losses)","metadata":{},"execution_count":null,"outputs":[]}]}