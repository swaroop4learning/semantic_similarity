{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az0rjP-zupa5",
        "outputId": "c1ba9b81-710d-4d01-c2aa-a697b0e9d040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.4 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from spacy.lang.es import Spanish\n",
        "nlp = Spanish()\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "KhdGF9mcd2SD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = load_dataset(\"stsb_multi_mt\", name=\"es\", split=\"dev\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAoTbDUmusM0",
        "outputId": "15f4ea6e-0ff6-4384-9787-b3a17ebe1f8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset stsb_multi_mt (/root/.cache/huggingface/datasets/stsb_multi_mt/es/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train))\n",
        "print(type(train[0]))\n",
        "print(type(train['sentence1']))\n",
        "print(type(train['similarity_score']))\n",
        "print(train)\n",
        "print(train[0])\n",
        "print(train[1238])\n",
        "print(train[0][\"sentence1\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r70s7ldiuupm",
        "outputId": "8c39c272-4c30-4453-e6af-ecdf9afc772e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "<class 'dict'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Dataset({\n",
            "    features: ['sentence1', 'sentence2', 'similarity_score'],\n",
            "    num_rows: 1500\n",
            "})\n",
            "{'sentence1': 'Un hombre con un casco está bailando.', 'sentence2': 'Un hombre con un casco está bailando.', 'similarity_score': 5.0}\n",
            "{'sentence1': 'Los equipos trabajaron para instalar una nueva alcantarilla y preparar la autopista para que los automovilistas pudieran usar los carriles hacia el este para viajar, ya que las nubes de tormenta amenazaban con arrojar más lluvia.', 'sentence2': 'Los equipos trabajaron para instalar una nueva alcantarilla y repavimentar la autopista para que los automovilistas pudieran usar los carriles hacia el este para viajar.', 'similarity_score': 3.3329999446868896}\n",
            "Un hombre con un casco está bailando.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = []\n",
        "sentence2 = []\n",
        "sim_scores = []\n",
        "\n",
        "for length in range (len(train)):\n",
        "  sentence1.append(train[length][\"sentence1\"].lower())\n",
        "  sentence2.append(train[length][\"sentence2\"].lower())\n",
        "  sim_scores.append(train[length][\"similarity_score\"])\n",
        "\n",
        "for sentence in sentence1:\n",
        "      # Puntuation \n",
        "    sentence = re.sub(r\"[^\\w\\s']+\" ,' ', sentence)\n",
        "    # sentence = re.sub(r\"(?<=[^\\d\\s])\\.(?=\\s|$)\", \"\", sentence)\n",
        "    # sentence = re.sub(r\"(?<=\\w)\\.(?=\\w)\", \"\", sentence)\n",
        "    # sentence = re.sub(re\".\",\"\", sentence)\n",
        "\n",
        "    #Removing extra spaces\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
        "\n",
        "for sentence in sentence2:\n",
        "      sentence = re.sub(r\"[^\\w\\s']+\" ,' ', sentence)\n",
        "      sentence = re.sub(r\"\\s+\", \" \", sentence)\n"
      ],
      "metadata": {
        "id": "16pV6u3ouyWD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences1 = []\n",
        "for sentence in sentence1:\n",
        "    doc = nlp(sentence)\n",
        "    tokenized_sentences1.append([token.text for token in doc])\n",
        "\n",
        "# print(tokenized_sentences1)"
      ],
      "metadata": {
        "id": "EEbRLLxDu3VD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences2 = []\n",
        "for sentence in sentence2:\n",
        "    doc = nlp(sentence)\n",
        "    tokenized_sentences2.append([token.text for token in doc])\n",
        "\n",
        "# print(tokenized_sentences2)"
      ],
      "metadata": {
        "id": "JDgiZhEL_nON"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUqQO3sWwEW-",
        "outputId": "cbb1bee9-ea7f-49a3-e790-8b7f74a2ddfb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/NLP/PreTrained_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIPynGVqwd3w",
        "outputId": "1f5077eb-a2d1-48c2-85bd-d8034dc1d97f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/NLP/PreTrained_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz"
      ],
      "metadata": {
        "id": "mh1eDEvLwruM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gunzip SBW-vectors-300-min5.bin.gz"
      ],
      "metadata": {
        "id": "_YIh3gDox5oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeyedVectors.load_word2vec_format('SBW-vectors-300-min5.bin', binary=True)\n",
        "# SBW-vectors-300-min5"
      ],
      "metadata": {
        "id": "vHEjn3qFzbF3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings1 = []\n",
        "\n",
        "for sentence in tokenized_sentences1:\n",
        "    sentence_embedding = []\n",
        "    for word in sentence:\n",
        "        if word in model:\n",
        "            sentence_embedding.append(model[word])\n",
        "    if sentence_embedding:\n",
        "        sentence_embedding = np.mean(sentence_embedding, axis=0)\n",
        "    else:\n",
        "        sentence_embedding = np.zeros(model.vector_size)\n",
        "    sentence_embeddings1.append(sentence_embedding)\n",
        "# print(sentence_embeddings[0])\n",
        "print((sentence_embeddings1[0].shape))"
      ],
      "metadata": {
        "id": "Bc0XC9sQ99D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20355d0b-8691-4f4c-d5f7-bc7a70dae2fd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings2 = []\n",
        "\n",
        "for sentence in tokenized_sentences2:\n",
        "    sentence_embedding = []\n",
        "    for word in sentence:\n",
        "        if word in model:\n",
        "            sentence_embedding.append(model[word])\n",
        "    if sentence_embedding:\n",
        "        sentence_embedding = np.mean(sentence_embedding, axis=0)\n",
        "    else:\n",
        "        sentence_embedding = np.zeros(model.vector_size)\n",
        "    sentence_embeddings2.append(sentence_embedding)\n",
        "# print(sentence_embeddings[0])\n",
        "print((sentence_embeddings2[0].shape))"
      ],
      "metadata": {
        "id": "S3XEzIVC_sCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad33f9e8-baf5-4b64-f695-3d9bba292cf9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# sentence_embeddings1 and sentence_embeddings2 are the two lists of sentence embeddings\n",
        "# of the same length\n",
        "assert len(sentence_embeddings1) == len(sentence_embeddings2)\n",
        "\n",
        "# Compute the cosine similarity between corresponding indexed sentences\n",
        "cos_sim = []\n",
        "for emb1, emb2 in zip(sentence_embeddings1, sentence_embeddings2):\n",
        "    similarity = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
        "    cos_sim.append(similarity)\n"
      ],
      "metadata": {
        "id": "tch7a4J8AQTI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize sim_scores using Min-Max scaling\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "sim_scores_norm = scaler.fit_transform(np.array(sim_scores).reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "# cos_sim and similarity_score are the two lists of similarity scores\n",
        "# of the same length\n",
        "assert len(cos_sim) == len(sim_scores)\n",
        "\n",
        "# Calculate the Pearson correlation coefficient and its p-value\n",
        "corr, p_value = pearsonr(cos_sim, sim_scores_norm)\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {corr:.3f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmRxoBWsCx28",
        "outputId": "f78fb09b-f8c0-4fe3-e086-2ca996b8c6a6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation coefficient: 0.633\n",
            "P-value: 0.000\n"
          ]
        }
      ]
    }
  ]
}