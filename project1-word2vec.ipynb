{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport fasttext\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:35:52.074612Z","iopub.execute_input":"2023-05-08T19:35:52.075346Z","iopub.status.idle":"2023-05-08T19:35:52.221457Z","shell.execute_reply.started":"2023-05-08T19:35:52.075280Z","shell.execute_reply":"2023-05-08T19:35:52.219821Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# @title Figure Settings\nimport ipywidgets as widgets\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:35:52.224144Z","iopub.execute_input":"2023-05-08T19:35:52.224590Z","iopub.status.idle":"2023-05-08T19:35:52.633860Z","shell.execute_reply.started":"2023-05-08T19:35:52.224547Z","shell.execute_reply":"2023-05-08T19:35:52.632760Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# @title Helper functions\ndef cosine_similarity(vec_a, vec_b):\n  \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n  return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n\n\ndef getSimilarity(word1, word2):\n  v1 = ft_en_vectors.get_word_vector(word1)\n  v2 = ft_en_vectors.get_word_vector(word2)\n  return cosine_similarity(v1, v2)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:35:52.635602Z","iopub.execute_input":"2023-05-08T19:35:52.636738Z","iopub.status.idle":"2023-05-08T19:35:52.644253Z","shell.execute_reply.started":"2023-05-08T19:35:52.636687Z","shell.execute_reply":"2023-05-08T19:35:52.642533Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# For DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  \"\"\"\n  Function that controls randomness.\n  NumPy and random modules must be imported.\n\n  Args:\n    seed : Integer\n      A non-negative integer that defines the random state. Default is `None`.\n    seed_torch : Boolean\n      If `True` sets the random seed for pytorch tensors, so pytorch module\n      must be imported. Default is `True`.\n\n  Returns:\n    Nothing.\n  \"\"\"\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  \"\"\"\n  DataLoader will reseed workers following randomness in\n  multi-process data loading algorithm.\n\n  Args:\n    worker_id: integer\n      ID of subprocess to seed. 0 means that\n      the data will be loaded in the main process\n      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n\n  Returns:\n    Nothing\n  \"\"\"\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:35:52.647530Z","iopub.execute_input":"2023-05-08T19:35:52.648963Z","iopub.status.idle":"2023-05-08T19:35:55.010171Z","shell.execute_reply.started":"2023-05-08T19:35:52.648881Z","shell.execute_reply":"2023-05-08T19:35:55.008858Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# @title Set device (GPU or CPU). Execute `set_device()`\n\n# Inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  \"\"\"\n  Set the device. CUDA if available, CPU otherwise\n\n  Args:\n    None\n\n  Returns:\n    Nothing\n  \"\"\"\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:35:55.011889Z","iopub.execute_input":"2023-05-08T19:35:55.012582Z","iopub.status.idle":"2023-05-08T19:35:55.020242Z","shell.execute_reply.started":"2023-05-08T19:35:55.012527Z","shell.execute_reply":"2023-05-08T19:35:55.018889Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"DEVICE = set_device()\nSEED = 2021\nset_seed(seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:35:55.021705Z","iopub.execute_input":"2023-05-08T19:35:55.022206Z","iopub.status.idle":"2023-05-08T19:35:55.037936Z","shell.execute_reply.started":"2023-05-08T19:35:55.022156Z","shell.execute_reply":"2023-05-08T19:35:55.036812Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \nRandom seed 2021 has been set.\n","output_type":"stream"}]},{"cell_type":"code","source":"import fasttext.util\n# fasttext.util.download_model('en', if_exists='ignore')  # English\nft_en_vectors = fasttext.load_model('/kaggle/input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext.util\n# fasttext.util.download_model('es', if_exists='ignore')  # English\nft_es_vectors = fasttext.load_model('/kaggle/input/fasttext-spanish-300/cc.es.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see similarity without making bilingual embedding","metadata":{}},{"cell_type":"code","source":"hello = ft_en_vectors.get_word_vector('hello')\nhi = ft_en_vectors.get_word_vector('hi')\nbonjour = ft_es_vectors.get_word_vector('bonjour')\n\nprint(f\"Cosine Similarity between HI and HELLO: {cosine_similarity(hello, hi)}\")\nprint(f\"Cosine Similarity between BONJOUR and HELLO: {cosine_similarity(hello, bonjour)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = ft_en_vectors.get_word_vector('cat')\nchatte = ft_es_vectors.get_word_vector('chatte')\nchat = ft_es_vectors.get_word_vector('chat')\n\nprint(f\"Cosine Similarity between cat and chatte: {cosine_similarity(cat, chatte)}\")\nprint(f\"Cosine Similarity between cat and chat: {cosine_similarity(cat, chat)}\")\nprint(f\"Cosine Similarity between chatte and chat: {cosine_similarity(chatte, chat)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_words = set(ft_en_vectors.words)\nes_words = set(ft_es_vectors.words)\noverlap = list(en_words & es_words)\nbilingual_dictionary = [(entry, entry) for entry in overlap]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(bilingual_dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_training_matrices(source_dictionary, target_dictionary,\n                           bilingual_dictionary):\n  source_matrix = []\n  target_matrix = []\n  for (source, target) in tqdm(bilingual_dictionary):\n    # if source in source_dictionary.words and target in target_dictionary.words:\n    source_matrix.append(source_dictionary.get_word_vector(source))\n    target_matrix.append(target_dictionary.get_word_vector(target))\n  # return training matrices\n  return np.array(source_matrix), np.array(target_matrix)\n\n\n# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\ndef normalized(a, axis=-1, order=2):\n  \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n  l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n  l2[l2==0] = 1\n  return a / np.expand_dims(l2, axis)\n\n\ndef learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n  \"\"\"\n  Source and target matrices are numpy arrays, shape\n  (dictionary_length, embedding_dimension). These contain paired\n  word vectors from the bilingual dictionary.\n  \"\"\"\n  # optionally normalize the training vectors\n  if normalize_vectors:\n    source_matrix = normalized(source_matrix)\n    target_matrix = normalized(target_matrix)\n  # perform the SVD\n  product = np.matmul(source_matrix.transpose(), target_matrix)\n  U, s, V = np.linalg.svd(product)\n  # return orthogonal transformation which aligns source language to the target\n  return np.matmul(U, V)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source_training_matrix, target_training_matrix = make_training_matrices(ft_en_vectors, ft_es_vectors, bilingual_dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = learn_transformation(source_training_matrix, target_training_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Letâ€™s run the same examples as above, but this time, whenever we use French words, the matrix multiplies the embedding by the transpose of the transform matrix. That works a lot better!","metadata":{}},{"cell_type":"code","source":"hello = ft_en_vectors.get_word_vector('hello')\nhi = ft_en_vectors.get_word_vector('hi')\nbonjour = np.matmul(ft_es_vectors.get_word_vector('bonjour'), transform.T)\n\nprint(f\"Cosine Similarity between HI and HELLO: {cosine_similarity(hello, hi)}\")\nprint(f\"Cosine Similarity between BONJOUR and HELLO: {cosine_similarity(hello, bonjour)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = ft_en_vectors.get_word_vector('cat')\nchatte = np.matmul(ft_es_vectors.get_word_vector('chatte'), transform.T)\nchat = np.matmul(ft_es_vectors.get_word_vector('chat'), transform.T)\n\nprint(f\"Cosine Similarity between cat and chatte: {cosine_similarity(cat, chatte)}\")\nprint(f\"Cosine Similarity between cat and chat: {cosine_similarity(cat, chat)}\")\nprint(f\"Cosine Similarity between chatte and chat: {cosine_similarity(chatte, chat)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Embeddings have now being obtained","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom gensim.models import KeyedVectors\nimport nltk\n# nltk.download()\nimport nltk\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# English\ntrain_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_train_df.csv')\nval_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_val_df.csv')\ntest_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_test_df.csv')\n\n# Spanish\ntrain_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\nval_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\ntest_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nfrom sklearn.linear_model import LinearRegression\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Replace numbers with num\n    text = re.sub(r'\\d+', '', text)\n    # Lower case\n    text= text.lower()\n    sent_token = text.split()\n    # Lemmatize\n#     sent_token = [lemmatizer.lemmatize(word) for word in sent_token]\n    # Stemming\n#     sent_token = [ps.stem(word) for word in sent_token]\n    return sent_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df_en['sent1'] = train_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# train_df_en['sent2'] = train_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\ntrain_df_es['sent1'] = train_df_es['sent1'].apply(lambda x: preprocess_text(x))\ntrain_df_es['sent2'] = train_df_es['sent2'].apply(lambda x: preprocess_text(x))\n\n# val_df_en['sent1'] = val_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# val_df_en['sent2'] = val_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\nval_df_es['sent1'] = val_df_es['sent1'].apply(lambda x: preprocess_text(x))\nval_df_es['sent2'] = val_df_es['sent2'].apply(lambda x: preprocess_text(x))\n\n# test_df_en['sent1'] = test_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# test_df_en['sent2'] = test_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\ntest_df_es['sent1'] = test_df_es['sent1'].apply(lambda x: preprocess_text(x))\ntest_df_es['sent2'] = test_df_es['sent2'].apply(lambda x: preprocess_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sent_en = list(train_df_en['sent1']) + list(train_df_en['sent2'])\ntotal_sent_es= list(train_df_es['sent1']) + list(train_df_es['sent2'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_dict_en = {}\nfor word_tokens in total_sent_en:\n    for word in word_tokens:\n        if word in word_dict_en:\n            word_dict_en[word] += 1\n        else:\n            word_dict_en[word] = 1\n            \nvocab_length_en = len(word_dict_en)\n\nword_dict_es = {}\nfor word_tokens in total_sent_es:\n    for word in word_tokens:\n        if word in word_dict_es:\n            word_dict_es[word] += 1\n        else:\n            word_dict_es[word] = 1\n            \nvocab_length_es = len(word_dict_es)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sorted_counter(word_counter):\n    return {k: v for k, v in sorted(word_counter.items(), key=lambda item: item[1], reverse=False)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_counter_en = get_sorted_counter(word_dict_en)\nsorted_counter_es = get_sorted_counter(word_dict_es)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_en['sent1'] = train_df_en['sent1'].astype(str).apply(eval)\ntrain_df_en['sent2'] = train_df_en['sent2'].astype(str).apply(eval)\n\ntrain_df_es['sent1'] = train_df_es['sent1'].astype(str).apply(eval)\ntrain_df_es['sent2'] = train_df_es['sent2'].astype(str).apply(eval)\n\nval_df_en['sent1'] = val_df_en['sent1'].astype(str).apply(eval)\nval_df_en['sent2'] = val_df_en['sent2'].astype(str).apply(eval)\n\nval_df_es['sent1'] = val_df_es['sent1'].astype(str).apply(eval)\nval_df_es['sent2'] = val_df_es['sent2'].astype(str).apply(eval)\n\ntest_df_en['sent1'] = test_df_en['sent1'].astype(str).apply(eval)\ntest_df_en['sent2'] = test_df_en['sent2'].astype(str).apply(eval)\n\ntest_df_es['sent1'] = test_df_es['sent1'].astype(str).apply(eval)\ntest_df_es['sent2'] = test_df_es['sent2'].astype(str).apply(eval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = train_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = train_df_es[\"sent2\"] \n\nscore = train_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = train_df_en[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = train_df_es[\"sent1\"] \n\nscore = train_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_train = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = val_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = val_df_es[\"sent2\"] \n\nscore = val_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = val_df_es[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = val_df_es[\"sent1\"] \n\nscore = val_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_val = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = test_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = test_df_es[\"sent2\"] \n\nscore = test_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = test_df_es[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = test_df_es[\"sent1\"] \n\nscore = test_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_test = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sentences1 = list(final_train['sent1'])\ntrain_sentences2 = list(final_train['sent2'])\ntrain_similarity_scores = list(final_train['score'])\n\nval_sentences1 = list(final_val['sent1'])\nval_sentences2 = list(final_val['sent2'])\nval_similarity_scores = list(final_val['score'])\n\ntest_sentences1 = list(final_test['sent1'])\ntest_sentences2 = list(final_test['sent2'])\ntest_similarity_scores = list(final_test['score'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_sentences1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sentence_embedding_en(sentence, max_length=30):\n        words = sentence\n        # filter out words that are not present in the model's vocabulary\n        words = [word for word in words if word in ft_en_vectors.words ]\n        # generate word embeddings for each word\n        embeddings = [ft_en_vectors.get_word_vector(word) for word in words]\n        #mean strategy\n        embedding = np.mean(embeddings, axis=0)\n        return embedding\n    \ndef get_sentence_embedding_es(sentence, max_length=30):\n        words = sentence\n        # filter out words that are not present in the model's vocabulary\n        words = [word for word in words if word in ft_es_vectors.words ]\n        # generate word embeddings for each word\n        embeddings = [np.matmul(ft_es_vectors.get_word_vector(word), transform.T) for word in words]\n        #mean strategy\n        embedding = np.mean(embeddings, axis=0)\n        return embedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print( val_sentences2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # generate sentence embeddings\n# train_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in train_sentences1])\n# print(\"1\")\n# train_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in train_sentences2])\n# print(\"1\")\n\n# val_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in val_sentences1])\n# print(\"1\")\n# val_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in val_sentences2])\n# print(\"1\")\n\n\n# test_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in test_sentences1])\n# print(\"1\")\n\n# test_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in test_sentences2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_X2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_X = np.concatenate([train_X1, train_X2], axis=1)\n# val_X = np.concatenate([val_X1, val_X2], axis=1)\n# test_X = np.concatenate([test_X1, test_X2], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sts_score(sim_score):\n    sts_score = (sim_score+1) * 2.5\n    return sts_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial\nfrom scipy.stats import pearsonr\n\ndef get_sts_scores(emb1_lt, emb2_lt):\n    y_pred = []\n    for i in range(len(emb1_lt)):\n        sim_score = 1 - spatial.distance.cosine(emb1_lt[i], emb2_lt[i])\n        y_pred.append(sts_score(sim_score))\n    return y_pred\n    \ndef pearson_corr(y_true, y_pred):\n    \"\"\"\n    Calculate Pearson correlation coefficient between two arrays.\n    \"\"\"\n    corr, _ = pearsonr(y_true, y_pred)\n    return corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalised Cosine Similarity","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport fasttext.util\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# Load FastText embeddings\nft_es_vectors = fasttext.load_model('/kaggle/input/fasttext-spanish-300/cc.es.300.bin')\nft_en_vectors = fasttext.load_model('/kaggle/input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin')\n\n# Get the word vectors and corresponding words for both languages\nwords_es = ft_es_vectors.get_words()\nvectors_es = np.array([ft_es_vectors.get_word_vector(word) for word in words_es])\n\nwords_en = ft_en_vectors.get_words()\nvectors_en = np.array([ft_en_vectors.get_word_vector(word) for word in words_en])\n\n# Convert the word vectors to PyTorch tensors\nvectors_es_tensor = torch.from_numpy(vectors_es).float()\nvectors_en_tensor = torch.from_numpy(vectors_en).float()\n\n# Define a simple feed-forward neural network for mapping\nclass MappingNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(MappingNetwork, self).__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.fc(x)\n\n# Create the mapping network\ninput_dim = vectors_es_tensor.shape[1]\noutput_dim = vectors_en_tensor.shape[1]\nmapping_network = MappingNetwork(input_dim, output_dim)\n\n# Define the loss function and optimizer\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(mapping_network.parameters(), lr=0.001)\n\n# Train the mapping network\ndataset = DataLoader(list(zip(vectors_es_tensor, vectors_en_tensor)), batch_size=32, shuffle=True)\n\nnum_epochs = 10\ntotal_steps = len(dataset) * num_epochs\n\nfor epoch in range(num_epochs):\n    for step, batch in enumerate(dataset):\n        src_vectors, tgt_vectors = batch\n        optimizer.zero_grad()\n        mapped_vectors = mapping_network(src_vectors)\n        loss = loss_fn(mapped_vectors, tgt_vectors)\n        loss.backward()\n        optimizer.step()\n\n        # Calculate progress in percentage\n        progress = (epoch * len(dataset) + step + 1) / total_steps * 100\n\n        # Print epoch and progress\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(dataset)}], Progress: {progress:.2f}%\")\n\n# Map the source language word vectors to the target language space\nmapped_vectors_es = mapping_network(vectors_es_tensor).detach().numpy()\n\n# Calculate cosine similarity between mapped source vectors and target vectors\ncos_sim_scores = np.dot(mapped_vectors_es, vectors_en.T) / (\n        np.linalg.norm(mapped_vectors_es, axis=1).reshape(-1, 1) * np.linalg.norm(vectors_en, axis=1))\n\n# \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Example of computing similarity between two words\nword_idx_es = list(words_es).index('manzana')\nword_idx_en = list(words_en).index('apple')\nsimilarity_score = cos_sim_scores[word_idx_es][word_idx_en]\nprint(\"Similarity score between 'manzana' and 'apple':\", similarity_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y_pred = get_sts_scores(train_X1, train_X2)\nval_y_pred = get_sts_scores(val_X1, val_X2)\ntest_y_pred = get_sts_scores(test_X1, test_X2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(train_similarity_scores, train_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(val_similarity_scores, val_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(test_similarity_scores, test_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Residual Analysis","metadata":{}},{"cell_type":"code","source":"row_max_len = lambda row: max(len(row['sent1']), len(row['sent2']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['sent_len'] = train_df.apply(row_max_len, axis=1)\nval_df['sent_len'] = val_df.apply(row_max_len, axis=1)\ntest_df['sent_len'] = test_df.apply(row_max_len, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['diff'] = abs(train_df['score'] - train_y_pred)\nval_df['diff'] = abs(val_df['score'] - val_y_pred)\ntest_df['diff'] = abs(test_df['score'] - test_y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_df = train_df.groupby(['sent_len']).mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_df['sent_length'] = grouped_df.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(list(grouped_df.index), list(grouped_df['diff']))\nplt.title(\"average error trend in training data\")\nplt.xlabel(\"sentence length\")\nplt.ylabel(\"average difference\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_val = val_df.groupby(['sent_len']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(list(grouped_val.index), list(grouped_val['diff']))\nplt.title(\"average error trend in validation data\")\nplt.xlabel(\"sentence length\")\nplt.ylabel(\"average difference\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_test = test_df.groupby(['sent_len']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(list(grouped_test.index), list(grouped_test['diff']))\nplt.title(\"average error trend in test data\")\nplt.xlabel(\"sentence length\")\nplt.ylabel(\"average difference\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}